"About the data: Exported on Jan 08, 2025. Criteria: '(LLM) AND (HLS) AND (FPGA)' in full data; Publication Type is Preprint or Proceeding.  Note: For hyper-authorship publications with more than 100 authors the export does not include author names and affiliations. © 2025 Digital Science &amp; Research Solutions Inc. All rights reserved. Parts of this work may also be protected by copyright of content providers and other third parties, which together with all rights of Digital Science, user agrees not to violate. Redistribution / external use of this work (or parts thereof) is prohibited without prior written approval. Please contact info@dimensions.ai for further information.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rank,Publication ID,DOI,PMID,PMCID,Title,Abstract,Acknowledgements,Funding,Source title,Anthology title,Book editors,MeSH terms,Publication date,PubYear,Publication date (online),Publication date (print),Volume,Issue,Pagination,Open Access,Publication Type,Authors,Authors (Raw Affiliation),Corresponding Authors,Authors Affiliations,Times cited,Recent citations,RCR,FCR,Source Linkout,Dimensions URL,Fields of Research (ANZSRC 2020),Sustainable Development Goals
1521,pub.1174720501,10.48550/arxiv.2408.06810,,,HLSPilot: LLM-based High-Level Synthesis,"Large language models (LLMs) have catalyzed an upsurge in automatic code
generation, garnering significant attention for register transfer level (RTL)
code generation. Despite the potential of RTL code generation with natural
language, it remains error-prone and limited to relatively small modules
because of the substantial semantic gap between natural language expressions
and hardware design intent. In response to the limitations, we propose a
methodology that reduces the semantic gaps by utilizing C/C++ for generating
hardware designs via High-Level Synthesis (HLS) tools. Basically, we build a
set of C-to-HLS optimization strategies catering to various code patterns, such
as nested loops and local arrays. Then, we apply these strategies to sequential
C/C++ code through in-context learning, which provides the LLMs with exemplary
C/C++ to HLS prompts. With this approach, HLS designs can be generated
effectively. Since LLMs still face problems in determining the optimized pragma
parameters precisely, we have a design space exploration (DSE) tool integrated
for pragma parameter tuning. Furthermore, we also employ profiling tools to
pinpoint the performance bottlenecks within a program and selectively convert
bottleneck components to HLS code for hardware acceleration. By combining the
LLM-based profiling, C/C++ to HLS translation, and DSE, we have established
HLSPilot, the first LLM-enabled high-level synthesis framework, which can fully
automate the high-level application acceleration on hybrid CPU-FPGA
architectures. According to our experiments on real-world application
benchmarks, HLSPilot achieve comparable performance in general and can even
outperform manually crafted counterparts, thereby underscoring the substantial
promise of LLM-assisted hardware designs.",,,arXiv,,,,2024-08-13,2024,,,,,,All OA; Green,Preprint,"Xiong, Chenwei; Liu, Cheng; Li, Huawei; Li, Xiaowei","Xiong, Chenwei (); Liu, Cheng (); Li, Huawei (); Li, Xiaowei ()",,"Xiong, Chenwei (); Liu, Cheng (); Li, Huawei (); Li, Xiaowei ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1174720501,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware",
1506,pub.1181432896,10.48550/arxiv.2410.13079,,,RapidStream IR: Infrastructure for FPGA High-Level Physical Synthesis,"The increasing complexity of large-scale FPGA accelerators poses significant
challenges in achieving high performance while maintaining design productivity.
High-level synthesis (HLS) has been adopted as a solution, but the mismatch
between the high-level description and the physical layout often leads to
suboptimal operating frequency. Although existing proposals for high-level
physical synthesis, which use coarse-grained design partitioning,
floorplanning, and pipelining to improve frequency, have gained traction, they
lack a framework enabling (1) pipelining of real-world designs at arbitrary
hierarchical levels, (2) integration of HLS blocks, vendor IPs, and handcrafted
RTL designs, (3) portability to emerging new target FPGA devices, and (4)
extensibility for the easy implementation of new design optimization tools.
  We present RapidStream IR, a practical high-level physical synthesis (HLPS)
infrastructure for representing the composition of complex FPGA designs and
exploring physical optimizations. Our approach introduces a flexible
intermediate representation (IR) that captures interconnection protocols at
arbitrary hierarchical levels, coarse-grained pipelining, and spatial
information, enabling the creation of reusable passes for design frequency
optimizations. RapidStream IR improves the frequency of a broad set of
mixed-source designs by 7% to 62%, including large language models and genomics
accelerators, and is portable to user-customizable new FPGA platforms. We
further demonstrate its extensibility through case studies, showcasing the
ability to facilitate future research.",,,arXiv,,,,2024-10-16,2024,,,,,,All OA; Green,Preprint,"Lau, Jason; Xiao, Yuanlong; Xie, Yutong; Chi, Yuze; Song, Linghao; Xiang, Shaojie; Lo, Michael; Zhang, Zhiru; Cong, Jason; Guo, Licheng","Lau, Jason (); Xiao, Yuanlong (); Xie, Yutong (); Chi, Yuze (); Song, Linghao (); Xiang, Shaojie (); Lo, Michael (); Zhang, Zhiru (); Cong, Jason (); Guo, Licheng ()",,"Lau, Jason (); Xiao, Yuanlong (); Xie, Yutong (); Chi, Yuze (); Song, Linghao (); Xiang, Shaojie (); Lo, Michael (); Zhang, Zhiru (); Cong, Jason (); Guo, Licheng ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1181432896,"33 Built Environment and Design; 40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware",
1494,pub.1175797009,10.48550/arxiv.2409.11424,,,LlamaF: An Efficient Llama2 Architecture Accelerator on Embedded FPGAs,"Large language models (LLMs) have demonstrated remarkable abilities in
natural language processing. However, their deployment on resource-constrained
embedded devices remains difficult due to memory and computational demands. In
this paper, we present an FPGA-based accelerator designed to improve LLM
inference performance on embedded FPGAs. We employ post-training quantization
to reduce model size and optimize for off-chip memory bandwidth. Our design
features asynchronous computation and a fully pipelined accelerator for
matrix-vector multiplication. Experiments of the TinyLlama 1.1B model on a
Xilinx ZCU102 platform show a 14.3-15.8x speedup and a 6.1x power efficiency
improvement over running exclusively on ZCU102 processing system (PS).",,,arXiv,,,,2024-09-12,2024,,,,,,All OA; Green,Preprint,"Xu, Han; Li, Yutong; Ji, Shihao","Xu, Han (); Li, Yutong (); Ji, Shihao ()",,"Xu, Han (); Li, Yutong (); Ji, Shihao ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1175797009,33 Built Environment and Design; 3301 Architecture; 40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,7 Affordable and Clean Energy
1493,pub.1184021537,10.1109/wf-iot62078.2024.10811385,,,LlamaF: An Efficient Llama2 Architecture Accelerator on Embedded FPGAs,"Large language models (LLMs) have demonstrated remarkable abilities in natural language processing. However, their deployment on resource-constrained embedded devices remains difficult due to memory and computational demands. In this paper, we present an FPGA-based accelerator designed to improve LLM inference performance on embedded FPGAs. We employ post-training quantization to reduce model size and optimize for off-chip memory bandwidth. Our design features asynchronous computation and a fully pipelined accelerator for matrix-vector multiplication. Experiments of the TinyLlama 1.1B model on a Xilinx ZCU102 platform show a 14.3-15.8x speedup and a 6.1x power efficiency improvement over running exclusively on ZCU102 processing system (PS).","We would like to thank the anonymous reviewers for their comments and suggestions, which helped improve the quality of this paper. We would also gratefully acknowledge the support of VMware Inc. for its university research fund to this research.","We would like to thank the anonymous reviewers for their comments and suggestions, which helped improve the quality of this paper. We would also gratefully acknowledge the support of VMware Inc. for its university research fund to this research.",,2024 IEEE 10th World Forum on Internet of Things (WF-IoT),,,2024-11-13,2024,,2024-11-13,00,,1-7,All OA; Green,Proceeding,"Xu, Han; Li, Yutong; Ji, Shihao","Xu, Han (University of Illinois at Urbana-Champaign Urbana, Illinois); Li, Yutong (University of Illinois at Urbana-Champaign Champaign, Illinois); Ji, Shihao (Georgia State University, Atlanta, Georgia)","Xu, Han (University of Illinois Urbana-Champaign)","Xu, Han (University of Illinois Urbana-Champaign); Li, Yutong (University of Illinois Urbana-Champaign); Ji, Shihao (Georgia State University)",0,0,,,http://arxiv.org/pdf/2409.11424,https://app.dimensions.ai/details/publication/pub.1184021537,33 Built Environment and Design; 3301 Architecture; 40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,7 Affordable and Clean Energy
1493,pub.1170130348,10.1109/asp-dac58780.2024.10473893,,,Invited Paper: Software/Hardware Co-design for LLM and Its Application for Design Verification,"The widespread adoption of Large Language Models (LLMs) is impeded by their demanding compute and memory resources. The first task of this paper is to explore optimization strategies to expedite LLMs, including quantization, pruning, and operation-level optimizations. One unique direction is to optimize LLM inference through novel software/hardware co-design methods. Given the accelerated LLMs, the second task of this paper is to study LLMs' performance in the usage scenario of circuit design and verification. Specifically, we place a particular emphasis on functional verification. Through automated prompt engineering, we harness the capabilities of the established LLM, GPT-4, to generate High-Level Synthesis (HLS) designs with predefined errors based on 11 open-source synthesizable HLS benchmark suites. This dataset is a comprehensive collection of over 1000 function-level designs, and each of which is afflicted with up to 45 distinct combinations of defects injected into the source code. This dataset, named Chrysalis, expands upon what's available in current HLS error models, offering a rich resource for training to improve how LLMs debug code. The dataset can be accessed at: https://github.com/UIUC-ChenLab/Chrysalis-HLS.",,,,2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC),,,2024-01-22,2024,,,,,435-441,Closed,Proceeding,"Wan, Lily Jiaxin; Huang, Yingbing; Li, Yuhong; Ye, Hanchen; Wang, Jinghua; Zhang, Xiaofan; Chen, Deming","Wan, Lily Jiaxin (University of Illinois Urbana-Champaign); Huang, Yingbing (University of Illinois Urbana-Champaign); Li, Yuhong (University of Illinois Urbana-Champaign); Ye, Hanchen (University of Illinois Urbana-Champaign); Wang, Jinghua (University of Illinois Urbana-Champaign); Zhang, Xiaofan (Google); Chen, Deming (University of Illinois Urbana-Champaign)",,"Wan, Lily Jiaxin (University of Illinois Urbana-Champaign); Huang, Yingbing (University of Illinois Urbana-Champaign); Li, Yuhong (University of Illinois Urbana-Champaign); Ye, Hanchen (University of Illinois Urbana-Champaign); Wang, Jinghua (University of Illinois Urbana-Champaign); Zhang, Xiaofan (Google (United States)); Chen, Deming (University of Illinois Urbana-Champaign)",5,5,,,,https://app.dimensions.ai/details/publication/pub.1170130348,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",
1489,pub.1182091709,10.1145/3649329.3663517,,,"Invited: New Solutions on LLM Acceleration, Optimization, and Application","Large Language Models (LLMs) have revolutionized a wide range of applications with their strong human-like understanding and creativity. Due to the continuously growing model size and complexity, LLM training and deployment have shown significant challenges, which often results in extremely high computational and storage costs and energy consumption. In this paper, we discuss the recent advancements and research directions on (1) LLM algorithm-level acceleration, (2) LLM-hardware co-design for improved system efficiency, (3) LLM-to-accelerator compilation for customized LLM accelerators, and (4) LLM-aided design for HLS (High-Level Synthesis) functional verification. For each aspect, we present the background study, our proposed solutions, and future directions. An extended version of this work can be found at: https://arxiv.org/abs/2406.10903.",,,,Proceedings of the 61st ACM/IEEE Design Automation Conference,,,2024-06-23,2024,2024-11-07,2024-06-23,,,1-4,Closed,Proceeding,"Huang, Yingbing; Wan, Lily Jiaxin; Ye, Hanchen; Jha, Manvi; Wang, Jinghua; Li, Yuhong; Zhang, Xiaofan; Chen, Deming","Huang, Yingbing (University of Illinois Urbana-Champaign, Urbana, IL, United States); Wan, Lily Jiaxin (University of Illinois Urbana-Champaign, Urbana, IL, United States); Ye, Hanchen (University of Illinois Urbana-Champaign, Urbana, IL, United States); Jha, Manvi (University of Illinois Urbana-Champaign, Urbana, IL, United States); Wang, Jinghua (University of Illinois Urbana-Champaign, Urbana, IL, United States); Li, Yuhong (University of Illinois Urbana-Champaign, Urbana, IL, United States); Zhang, Xiaofan (Google, San Jose, CA, United States); Chen, Deming (University of Illinois Urbana-Champaign, Champaign, IL, United States)","Chen, Deming (University of Illinois Urbana-Champaign)","Huang, Yingbing (University of Illinois Urbana-Champaign); Wan, Lily Jiaxin (University of Illinois Urbana-Champaign); Ye, Hanchen (University of Illinois Urbana-Champaign); Jha, Manvi (University of Illinois Urbana-Champaign); Wang, Jinghua (University of Illinois Urbana-Champaign); Li, Yuhong (University of Illinois Urbana-Champaign); Zhang, Xiaofan (Google (United States)); Chen, Deming (University of Illinois Urbana-Champaign)",3,3,,,,https://app.dimensions.ai/details/publication/pub.1182091709,33 Built Environment and Design; 3303 Design; 40 Engineering,7 Affordable and Clean Energy
1480,pub.1167360414,10.48550/arxiv.2312.15159,,,Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference,"Recent advancements in large language models (LLMs) boasting billions of
parameters have generated a significant demand for efficient deployment in
inference workloads. The majority of existing approaches rely on temporal
architectures that reuse hardware units for different network layers and
operators. However, these methods often encounter challenges in achieving low
latency due to considerable memory access overhead. This paper investigates the
feasibility and potential of model-specific spatial acceleration for LLM
inference on FPGAs. Our approach involves the specialization of distinct
hardware units for specific operators or layers, facilitating direct
communication between them through a dataflow architecture while minimizing
off-chip memory accesses. We introduce a comprehensive analytical model for
estimating the performance of a spatial LLM accelerator, taking into account
the on-chip compute and memory resources available on an FPGA. Through our
analysis, we can determine the scenarios in which FPGA-based spatial
acceleration can outperform its GPU-based counterpart. To enable more
productive implementations of an LLM model on FPGAs, we further provide a
library of high-level synthesis (HLS) kernels that are composable and reusable.
This library will be made available as open-source. To validate the
effectiveness of both our analytical model and HLS library, we have implemented
BERT and GPT2 on an AMD Alveo U280 FPGA device. Experimental results
demonstrate our approach can achieve up to 13.4x speedup when compared to
previous FPGA-based accelerators for the BERT model. For GPT generative
inference, we attain a 2.2x speedup compared to DFX, an FPGA overlay, in the
prefill stage, while achieving a 1.9x speedup and a 5.7x improvement in energy
efficiency compared to the NVIDIA A100 GPU in the decode stage.",,,arXiv,,,,2023-12-22,2023,,,,,,All OA; Green,Preprint,"Chen, Hongzheng; Zhang, Jiahao; Du, Yixiao; Xiang, Shaojie; Yue, Zichao; Zhang, Niansong; Cai, Yaohui; Zhang, Zhiru","Chen, Hongzheng (); Zhang, Jiahao (); Du, Yixiao (); Xiang, Shaojie (); Yue, Zichao (); Zhang, Niansong (); Cai, Yaohui (); Zhang, Zhiru ()",,"Chen, Hongzheng (); Zhang, Jiahao (); Du, Yixiao (); Xiang, Shaojie (); Yue, Zichao (); Zhang, Niansong (); Cai, Yaohui (); Zhang, Zhiru ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1167360414,"33 Built Environment and Design; 40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",7 Affordable and Clean Energy
1478,pub.1176209492,10.1109/orss62274.2024.10697938,,,Evaluating Large Language Models for High-Level Synthesis,"Large language models (LLMs) are comprehensive tools, that are capable of modeling natural languages, such as human reasoning, and code structures, and even more recently, using expansive collective databases are even now able to create generated information based on a given user prompts. Given the extensive capabilities of LLMs and their diverse knowledge in many fields, this paper explores the use of LLMs for hardware design. Specifically improving LLM’s capability for code editing and generation of C++ code targeting high level synthesis (HLS) for FPGA design, as well as creating generated code based on a programmable prompt. We built a frame work to interact with LLM tools. Results indicate that there is success in creating an external framework for zero-shot code editing, indicating that the approach is valid with minor model performance. Future work looks to improve the framework’s accuracy for the current model and expand usage to other models as well as code generation.",,,,2024 IEEE Opportunity Research Scholars Symposium (ORSS),,,2024-07-15,2024,,2024-07-15,00,,49-52,Closed,Proceeding,"Rizvi, Ali; Simon, Nia; Tocho, Janet; Yongaci, Asil; Abi-Karam, Stefan; Hao, Cong","Rizvi, Ali (); Simon, Nia (); Tocho, Janet (); Yongaci, Asil (); Abi-Karam, Stefan (); Hao, Cong ()","Rizvi, Ali ()","Rizvi, Ali (); Simon, Nia (); Tocho, Janet (); Yongaci, Asil (); Abi-Karam, Stefan (); Hao, Cong ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1176209492,33 Built Environment and Design; 3303 Design; 46 Information and Computing Sciences,
1476,pub.1171302147,10.48550/arxiv.2405.00738,,,HLSTransform: Energy-Efficient Llama 2 Inference on FPGAs Via High Level Synthesis,"Graphics Processing Units (GPUs) have become the leading hardware accelerator
for deep learning applications and are used widely in training and inference of
transformers; transformers have achieved state-of-the-art performance in many
areas of machine learning and are especially used in most modern Large Language
Models (LLMs). However, GPUs require large amounts of energy, which poses
environmental concerns, demands high operational costs, and causes GPUs to be
unsuitable for edge computing. We develop an accelerator for transformers,
namely, Llama 2, an open-source state-of-the-art LLM, using high level
synthesis (HLS) on Field Programmable Gate Arrays (FPGAs). HLS allows us to
rapidly prototype FPGA designs without writing code at the register-transfer
level (RTL). We name our method HLSTransform, and the FPGA designs we
synthesize with HLS achieve up to a 12.75x reduction and 8.25x reduction in
energy used per token on the Xilinx Virtex UltraScale+ VU9P FPGA compared to an
Intel Xeon Broadwell E5-2686 v4 CPU and NVIDIA RTX 3090 GPU respectively, while
increasing inference speeds by up to 2.46x compared to CPU and maintaining
0.53x the speed of an RTX 3090 GPU despite the GPU's 4 times higher base clock
rate. With the lack of existing open-source FPGA accelerators for transformers,
we open-source our code and document our steps for synthesis. We hope this work
will serve as a step in democratizing the use of FPGAs in transformer inference
and inspire research into energy-efficient inference methods as a whole. The
code can be found on https://github.com/HLSTransform/submission.",,,arXiv,,,,2024-04-29,2024,,,,,,All OA; Green,Preprint,"He, Andy; Key, Darren; Bulling, Mason; Chang, Andrew; Shapiro, Skyler; Lee, Everett","He, Andy (); Key, Darren (); Bulling, Mason (); Chang, Andrew (); Shapiro, Skyler (); Lee, Everett ()",,"He, Andy (); Key, Darren (); Bulling, Mason (); Chang, Andrew (); Shapiro, Skyler (); Lee, Everett ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1171302147,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware",7 Affordable and Clean Energy
1473,pub.1182097646,10.1145/3649329.3663500,,,Late Breaking Results: Language-level QoR modeling for High-Level Synthesis,"This paper proposes a language-level modeling approach for HighLevel Synthesis based on the state-of-the-art Transformer architecture. Our approach estimates the performance and required resources of HLS applications directly from the source code when different synthesis directives, in terms of HLS #pragmas, are applied. Results show that the proposed architecture achieves 96.02% accuracy for predicting the feasibility class of applications and an average of 0.95 and 0.91 R2 scores for predicting the actual performance and required resources, respectively.",,,,Proceedings of the 61st ACM/IEEE Design Automation Conference,,,2024-06-23,2024,2024-11-07,2024-06-23,,,1-2,Closed,Proceeding,"Masouros, Dimosthenis; Ferikoglou, Aggelos; Zervakis, Georgios; Xydis, Sotirios; Soudris, Dimitrios","Masouros, Dimosthenis (National Technical University of Athens, Athens, Athens, Greece); Ferikoglou, Aggelos (National Technical University of Athens, Athens, Attica, Greece); Zervakis, Georgios (University of Patras, Patras, Achaia, Greece); Xydis, Sotirios (National Technical University of Athens, Athens, Attica, Greece); Soudris, Dimitrios (National Technical University of Athens, Athens, Attica, Greece)","Ferikoglou, Aggelos (National Technical University of Athens)","Masouros, Dimosthenis (National Technical University of Athens); Ferikoglou, Aggelos (National Technical University of Athens); Zervakis, Georgios (University of Patras); Xydis, Sotirios (National Technical University of Athens); Soudris, Dimitrios (National Technical University of Athens)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1182097646,40 Engineering; 4008 Electrical Engineering,
1473,pub.1172101532,10.48550/arxiv.2405.16072,,,SynthAI: A Multi Agent Generative AI Framework for Automated Modular HLS Design Generation,"In this paper, we introduce SynthAI, a new method for the automated creation
of High-Level Synthesis (HLS) designs. SynthAI integrates ReAct agents,
Chain-of-Thought (CoT) prompting, web search technologies, and the
Retrieval-Augmented Generation (RAG) framework within a structured decision
graph. This innovative approach enables the systematic decomposition of complex
hardware design tasks into multiple stages and smaller, manageable modules. As
a result, SynthAI produces synthesizable designs that closely adhere to
user-specified design objectives and functional requirements. We further
validate the capabilities of SynthAI through several case studies, highlighting
its proficiency in generating complex, multi-module logic designs from a single
initial prompt. The SynthAI code is provided via the following repo:
\url{https://github.com/sarashs/FPGA_AGI}",,,arXiv,,,,2024-05-25,2024,,,,,,All OA; Green,Preprint,"Sheikholeslam, Seyed Arash; Ivanov, Andre","Sheikholeslam, Seyed Arash (); Ivanov, Andre ()",,"Sheikholeslam, Seyed Arash (); Ivanov, Andre ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1172101532,33 Built Environment and Design; 3303 Design; 46 Information and Computing Sciences; 4602 Artificial Intelligence,
1467,pub.1176220951,10.1109/lad62341.2024.10691860,,,An Iteratively-refined Dataset for High-Level Synthesis Functional Verification through LLM-Aided Bug Injection,"This paper explores the application of Large Language Models (LLMs) in the domain of High-Level Synthesis (HLS) for hardware design and verification, focusing on functional verification challenges. The scarcity of open-source HLS codebases, especially those containing bugs, poses a significant challenge, as LLMs require extensive datasets for efficient fine-tuning and evaluation. To tackle this, we introduce an innovative bug injection methodology working with a new dataset that we curated from a wide range of open-source HLS benchmark suites. This dataset features over 1,500 designs, with both the version injected with bugs and the corresponding bug-free version. Our bug injection method synergizes In-Context Learning (ICL) with Retrieval Augmented Generation (RAG), and Chain of Thought (CoT). This approach significantly boosts the dataset’s overall validity rate for single-bug injections. We demonstrate our solution quality using GPT-4 Turbo for injecting either logic bugs or non-ideal pragmas (compiler directives) into HLS designs. For logic bugs, we achieve an 84.8% ratio for valid injection attempts. Furthermore, our approach maintains an 88.0% dataset validity rate (the valid bug injection rate). In addition, we also evaluate the quality of HLS pragma injections (focusing on non-ideal pragmas), and achieve a 74.0% attempt and an 87.9% valid injection ratio. Compared to brute-force prompting, our strategy yields a 20.4% and a 54.0% validity improvement for the bug and non-ideal pragma injection, respectively. The Chrysalis dataset is accessible at https://github.com/UIUC-ChenLab/Chrysalis-HLS .","This work was supported in part by the Semiconductor Research Corporation (SRC) under the grant number 2023-CT-3175, and by the AMD Center of Excellence at UIUC. We would also like to extend our appreciation to Xiaofan Zhang from Google for the valuable discussions and insights provided.","This work was supported in part by the Semiconductor Research Corporation (SRC) under the grant number 2023-CT-3175, and by the AMD Center of Excellence at UIUC.",,2024 IEEE LLM Aided Design Workshop (LAD),,,2024-06-29,2024,,2024-06-29,00,,1-6,Closed,Proceeding,"Wan, Lily Jiaxin; Ye, Hanchen; Wang, Jinghua; Jha, Manvi; Chen, Deming","Wan, Lily Jiaxin (University of Illinois Urbana-Champaign); Ye, Hanchen (University of Illinois Urbana-Champaign); Wang, Jinghua (University of Illinois Urbana-Champaign); Jha, Manvi (University of Illinois Urbana-Champaign); Chen, Deming (University of Illinois Urbana-Champaign)","Wan, Lily Jiaxin (University of Illinois Urbana-Champaign)","Wan, Lily Jiaxin (University of Illinois Urbana-Champaign); Ye, Hanchen (University of Illinois Urbana-Champaign); Wang, Jinghua (University of Illinois Urbana-Champaign); Jha, Manvi (University of Illinois Urbana-Champaign); Chen, Deming (University of Illinois Urbana-Champaign)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1176220951,33 Built Environment and Design; 40 Engineering; 4008 Electrical Engineering,
1455,pub.1172946157,10.48550/arxiv.2406.10903,,,"New Solutions on LLM Acceleration, Optimization, and Application","Large Language Models (LLMs) have become extremely potent instruments with
exceptional capacities for comprehending and producing human-like text in a
wide range of applications. However, the increasing size and complexity of LLMs
present significant challenges in both training and deployment, leading to
substantial computational and storage costs as well as heightened energy
consumption. In this paper, we provide a review of recent advancements and
research directions aimed at addressing these challenges and enhancing the
efficiency of LLM-based systems. We begin by discussing algorithm-level
acceleration techniques focused on optimizing LLM inference speed and resource
utilization. We also explore LLM-hardware co-design strategies with a vision to
improve system efficiency by tailoring hardware architectures to LLM
requirements. Further, we delve into LLM-to-accelerator compilation approaches,
which involve customizing hardware accelerators for efficient LLM deployment.
Finally, as a case study to leverage LLMs for assisting circuit design, we
examine LLM-aided design methodologies for an important task: High-Level
Synthesis (HLS) functional verification, by creating a new dataset that
contains a large number of buggy and bug-free codes, which can be essential for
training LLMs to specialize on HLS verification and debugging. For each aspect
mentioned above, we begin with a detailed background study, followed by the
presentation of several novel solutions proposed to overcome specific
challenges. We then outline future research directions to drive further
advancements. Through these efforts, we aim to pave the way for more efficient
and scalable deployment of LLMs across a diverse range of applications.",,,arXiv,,,,2024-06-16,2024,,,,,,All OA; Green,Preprint,"Huang, Yingbing; Wan, Lily Jiaxin; Ye, Hanchen; Jha, Manvi; Wang, Jinghua; Li, Yuhong; Zhang, Xiaofan; Chen, Deming","Huang, Yingbing (); Wan, Lily Jiaxin (); Ye, Hanchen (); Jha, Manvi (); Wang, Jinghua (); Li, Yuhong (); Zhang, Xiaofan (); Chen, Deming ()",,"Huang, Yingbing (); Wan, Lily Jiaxin (); Ye, Hanchen (); Jha, Manvi (); Wang, Jinghua (); Li, Yuhong (); Zhang, Xiaofan (); Chen, Deming ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1172946157,"40 Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",7 Affordable and Clean Energy
1446,pub.1170335282,10.1145/3626202.3637600,,,A Comprehensive Evaluation of FPGA-Based Spatial Acceleration of LLMs,"Recent advancements in large language models (LLMs) have generated significant demands for efficient deployment in inference workloads. Most existing approaches rely on temporal architectures that reuse hardware units for different network layers and operators. However, these methods often encounter challenges in achieving low latency due to considerable memory access overhead. This paper investigates the feasibility and potential of model-specific spatial acceleration for LLM inference on FPGAs. Our approach involves the specialization of distinct hardware units for specific operators, facilitating direct communication between them through a dataflow architecture while minimizing off-chip memory accesses. We introduce a comprehensive analytical model for estimating the performance of a spatial LLM accelerator, considering the available hardware resources on an FPGA. Through our analysis, we can determine the scenarios in which FPGA-based spatial acceleration can outperform its GPU-based counterpart. To enable more productive implementations of an LLM model on FPGAs, we further provide a library of high-level synthesis (HLS) kernels that are composable and reusable. To validate the effectiveness of both our analytical model and HLS library, we have implemented BERT and GPT2 on an AMD Alveo U280 FPGA. Experimental results demonstrate our approach can achieve up to 16.1x speedup when compared to previous FPGA-based accelerators for the BERT model. For GPT generative inference, we attain a 2.2x speedup compared to DFX, an FPGA overlay, in the prefill stage, while achieving a 1.9x speedup and a 5.7x improvement in energy efficiency compared to the NVIDIA A100 GPU in the decode stage.",,,,Proceedings of the 2024 ACM/SIGDA International Symposium on Field Programmable Gate Arrays,,,2024-04-02,2024,2024-04-02,2024-04,,,185-185,Closed,Proceeding,"Chen, Hongzheng; Zhang, Jiahao; Du, Yixiao; Xiang, Shaojie; Yue, Zichao; Zhang, Niansong; Cai, Yaohui; Zhang, Zhiru","Chen, Hongzheng (Cornell University, Ithaca, USA); Zhang, Jiahao (Tsinghua University, Beijing, China); Du, Yixiao (Cornell University, Ithaca, USA); Xiang, Shaojie (Cornell University, Ithaca, USA); Yue, Zichao (Cornell University, Ithaca, USA); Zhang, Niansong (Cornell University, Ithaca, USA); Cai, Yaohui (Cornell University, Ithaca, USA); Zhang, Zhiru (Cornell University, Ithaca, USA)",,"Chen, Hongzheng (Cornell University); Zhang, Jiahao (Tsinghua University); Du, Yixiao (Cornell University); Xiang, Shaojie (Cornell University); Yue, Zichao (Cornell University); Zhang, Niansong (Cornell University); Cai, Yaohui (Cornell University); Zhang, Zhiru (Cornell University)",4,4,,,,https://app.dimensions.ai/details/publication/pub.1170335282,"33 Built Environment and Design; 3301 Architecture; 40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",7 Affordable and Clean Energy
1441,pub.1166542036,10.1109/iccad57390.2023.10323953,,,GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models,"The remarkable capabilities and intricate nature of Artificial Intelligence (AI) have dramatically escalated the imperative for specialized AI accelerators. Nonetheless, designing these accelerators for various AI workloads remains both labor- and time-intensive. While existing design exploration and automation tools can partially alleviate the need for extensive human involvement, they still demand substantial hardware expertise, posing a barrier to non-experts and stifling AI accelerator development. Motivated by the astonishing potential of large language models (LLMs) for generating high-quality content in response to human language instructions, we embark on this work to examine the possibility of harnessing LLMs to automate AI accelerator design. Through this endeavor, we develop GPT4AIGChip, a framework intended to democratize AI accelerator design by leveraging human natural languages instead of domain-specific languages. Specifically, we first perform an in-depth investigation into LLMs' limitations and capabilities for AI accelerator design, thus aiding our understanding of our current position and garnering insights into LLM-powered automated AI accelerator design. Furthermore, drawing inspiration from the above insights, we develop a framework called GPT4AIGChip, which features an automated demo-augmented prompt-generation pipeline utilizing in-context learning to guide LLMs towards creating high-quality AI accelerator design. To our knowledge, this work is the first to demonstrate an effective pipeline for LLM-powered automated AI accelerator generation. Accordingly, we anticipate that our insights and framework can serve as a catalyst for innovations in next-generation LLM-powered design automation tools.","The work is supported by the National Science Foundation (NSF) through the CCRI program (Award number: 2016727), an NSF CAREER award (Award number: 2048183), and CoCoSys, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.","The work is supported by the National Science Foundation (NSF) through the CCRI program (Award number: 2016727), an NSF CAREER award (Award number: 2048183), and CoCoSys, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.",,2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD),,,2023-11-02,2023,,2023-11-02,00,,1-9,All OA; Green,Proceeding,"Fu, Yonggan; Zhang, Yongan; Yu, Zhongzhi; Li, Sixu; Ye, Zhifan; Li, Chaojian; Wan, Cheng; Lin, Yingyan Celine","Fu, Yonggan (School of Computer Science, Georgia Institute of Technology); Zhang, Yongan (School of Computer Science, Georgia Institute of Technology); Yu, Zhongzhi (School of Computer Science, Georgia Institute of Technology); Li, Sixu (School of Computer Science, Georgia Institute of Technology); Ye, Zhifan (School of Computer Science, Georgia Institute of Technology); Li, Chaojian (School of Computer Science, Georgia Institute of Technology); Wan, Cheng (School of Computer Science, Georgia Institute of Technology); Lin, Yingyan Celine (School of Computer Science, Georgia Institute of Technology)","Fu, Yonggan (Georgia Institute of Technology); Zhang, Yongan (Georgia Institute of Technology); Yu, Zhongzhi (Georgia Institute of Technology)","Fu, Yonggan (Georgia Institute of Technology); Zhang, Yongan (Georgia Institute of Technology); Yu, Zhongzhi (Georgia Institute of Technology); Li, Sixu (Georgia Institute of Technology); Ye, Zhifan (Georgia Institute of Technology); Li, Chaojian (Georgia Institute of Technology); Wan, Cheng (Georgia Institute of Technology); Lin, Yingyan Celine (Georgia Institute of Technology)",27,27,,26.94,https://arxiv.org/pdf/2309.10730,https://app.dimensions.ai/details/publication/pub.1166542036,46 Information and Computing Sciences; 4608 Human-Centred Computing,
1439,pub.1164204468,10.48550/arxiv.2309.10730,,,GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models,"The remarkable capabilities and intricate nature of Artificial Intelligence
(AI) have dramatically escalated the imperative for specialized AI
accelerators. Nonetheless, designing these accelerators for various AI
workloads remains both labor- and time-intensive. While existing design
exploration and automation tools can partially alleviate the need for extensive
human involvement, they still demand substantial hardware expertise, posing a
barrier to non-experts and stifling AI accelerator development. Motivated by
the astonishing potential of large language models (LLMs) for generating
high-quality content in response to human language instructions, we embark on
this work to examine the possibility of harnessing LLMs to automate AI
accelerator design. Through this endeavor, we develop GPT4AIGChip, a framework
intended to democratize AI accelerator design by leveraging human natural
languages instead of domain-specific languages. Specifically, we first perform
an in-depth investigation into LLMs' limitations and capabilities for AI
accelerator design, thus aiding our understanding of our current position and
garnering insights into LLM-powered automated AI accelerator design.
Furthermore, drawing inspiration from the above insights, we develop a
framework called GPT4AIGChip, which features an automated demo-augmented
prompt-generation pipeline utilizing in-context learning to guide LLMs towards
creating high-quality AI accelerator design. To our knowledge, this work is the
first to demonstrate an effective pipeline for LLM-powered automated AI
accelerator generation. Accordingly, we anticipate that our insights and
framework can serve as a catalyst for innovations in next-generation
LLM-powered design automation tools.",,,arXiv,,,,2023-09-19,2023,,,,,,All OA; Green,Preprint,"Fu, Yonggan; Zhang, Yongan; Yu, Zhongzhi; Li, Sixu; Ye, Zhifan; Li, Chaojian; Wan, Cheng; Lin, Yingyan","Fu, Yonggan (); Zhang, Yongan (); Yu, Zhongzhi (); Li, Sixu (); Ye, Zhifan (); Li, Chaojian (); Wan, Cheng (); Lin, Yingyan ()",,"Fu, Yonggan (); Zhang, Yongan (); Yu, Zhongzhi (); Li, Sixu (); Ye, Zhifan (); Li, Chaojian (); Wan, Cheng (); Lin, Yingyan ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1164204468,46 Information and Computing Sciences; 4608 Human-Centred Computing,
1436,pub.1174904085,10.48550/arxiv.2408.10428,,,Are LLMs Any Good for High-Level Synthesis?,"The increasing complexity and demand for faster, energy-efficient hardware
designs necessitate innovative High-Level Synthesis (HLS) methodologies. This
paper explores the potential of Large Language Models (LLMs) to streamline or
replace the HLS process, leveraging their ability to understand natural
language specifications and refactor code. We survey the current research and
conduct experiments comparing Verilog designs generated by a standard HLS tool
(Vitis HLS) with those produced by LLMs translating C code or natural language
specifications. Our evaluation focuses on quantifying the impact on
performance, power, and resource utilization, providing an assessment of the
efficiency of LLM-based approaches. This study aims to illuminate the role of
LLMs in HLS, identifying promising directions for optimized hardware design in
applications such as AI acceleration, embedded systems, and high-performance
computing.",,,arXiv,,,,2024-08-19,2024,,,,,,All OA; Green,Preprint,"Liao, Yuchao; Adegbija, Tosiron; Lysecky, Roman","Liao, Yuchao (); Adegbija, Tosiron (); Lysecky, Roman ()",,"Liao, Yuchao (); Adegbija, Tosiron (); Lysecky, Roman ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1174904085,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware",7 Affordable and Clean Energy
1433,pub.1174388707,10.48550/arxiv.2408.00462,,,Designing Efficient LLM Accelerators for Edge Devices,"The increase in open-source availability of Large Language Models (LLMs) has
enabled users to deploy them on more and more resource-constrained edge devices
to reduce reliance on network connections and provide more privacy. However,
the high computation and memory demands of LLMs make their execution on
resource-constrained edge devices challenging and inefficient. To address this
issue, designing new and efficient edge accelerators for LLM inference is
crucial. FPGA-based accelerators are ideal for LLM acceleration due to their
reconfigurability, as they enable model-specific optimizations and higher
performance per watt. However, creating and integrating FPGA-based accelerators
for LLMs (particularly on edge devices) has proven challenging, mainly due to
the limited hardware design flows for LLMs in existing FPGA platforms.
  To tackle this issue, in this paper we first propose a new design platform,
named SECDA-LLM, that utilizes the SECDA methodology to streamline the process
of designing, integrating, and deploying efficient FPGA-based LLM accelerators
for the llama.cpp inference framework. We then demonstrate, through a case
study, the potential benefits of SECDA-LLM by creating a new MatMul accelerator
that supports block floating point quantized operations for LLMs. Our initial
accelerator design, deployed on the PYNQ-Z1 board, reduces latency 1.7 seconds
per token or ~2 seconds per word) by 11x over the dual-core Arm NEON-based CPU
execution for the TinyLlama model.",,,arXiv,,,,2024-08-01,2024,,,,,,All OA; Green,Preprint,"Haris, Jude; Saha, Rappy; Hu, Wenhao; Cano, José","Haris, Jude (); Saha, Rappy (); Hu, Wenhao (); Cano, José ()",,"Haris, Jude (); Saha, Rappy (); Hu, Wenhao (); Cano, José ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1174388707,33 Built Environment and Design; 3301 Architecture; 46 Information and Computing Sciences,7 Affordable and Clean Energy
1419,pub.1171095269,10.48550/arxiv.2404.16158,,,The Feasibility of Implementing Large-Scale Transformers on Multi-FPGA Platforms,"FPGAs are rarely mentioned when discussing the implementation of large
machine learning applications, such as Large Language Models (LLMs), in the
data center. There has been much evidence showing that single FPGAs can be
competitive with GPUs in performance for some computations, especially for low
latency, and often much more efficient when power is considered. This suggests
that there is merit to exploring the use of multiple FPGAs for large machine
learning applications. The challenge with using multiple FPGAs is that there is
no commonly-accepted flow for developing and deploying multi-FPGA applications,
i.e., there are no tools to describe a large application, map it to multiple
FPGAs and then deploy the application on a multi-FPGA platform. In this paper,
we explore the feasibility of implementing large transformers using multiple
FPGAs by developing a scalable multi-FPGA platform and some tools to map large
applications to the platform. We validate our approach by designing an
efficient multi-FPGA version of the I-BERT transformer and implement one
encoder using six FPGAs as a working proof-of-concept to show that our platform
and tools work. Based on our proof-of-concept prototype and the estimations of
performance using the latest FPGAs compared to GPUs, we conclude that there can
be a place for FPGAs in the world of large machine learning applications. We
demonstrate a promising first step that shows that with the right
infrastructure and tools it is reasonable to continue to explore the possible
benefits of using FPGAs for applications such as LLMs.",,,arXiv,,,,2024-04-24,2024,,,,,,All OA; Green,Preprint,"Gao, Yu; Vega, Juan Camilo; Chow, Paul","Gao, Yu (); Vega, Juan Camilo (); Chow, Paul ()",,"Gao, Yu (); Vega, Juan Camilo (); Chow, Paul ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1171095269,46 Information and Computing Sciences; 4602 Artificial Intelligence,
1403,pub.1169619528,10.48550/arxiv.2403.06664,,,Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System,"The recent huge advance of Large Language Models (LLMs) is mainly driven by
the increase in the number of parameters. This has led to substantial memory
capacity requirements, necessitating the use of dozens of GPUs just to meet the
capacity. One popular solution to this is storage-offloaded training, which
uses host memory and storage as an extended memory hierarchy. However, this
obviously comes at the cost of storage bandwidth bottleneck because storage
devices have orders of magnitude lower bandwidth compared to that of GPU device
memories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck
of storage-offloaded LLM training using near-storage processing devices on a
real system. The main component of Smart-Infinity is SmartUpdate, which
performs parameter updates on custom near-storage accelerators. We identify
that moving parameter updates to the storage side removes most of the storage
traffic. In addition, we propose an efficient data transfer handler structure
to address the system integration issues for Smart-Infinity. The handler allows
overlapping data transfers with fixed memory consumption by reusing the device
buffer. Lastly, we propose accelerator-assisted gradient
compression/decompression to enhance the scalability of Smart-Infinity. When
scaling to multiple near-storage processing devices, the write traffic on the
shared channel becomes the bottleneck. To alleviate this, we compress the
gradients on the GPU and decompress them on the accelerators. It provides
further acceleration from reduced traffic. As a result, Smart-Infinity achieves
a significant speedup compared to the baseline. Notably, Smart-Infinity is a
ready-to-use approach that is fully integrated into PyTorch on a real system.
We will open-source Smart-Infinity to facilitate its use.",,,arXiv,,,,2024-03-11,2024,,,,,,All OA; Green,Preprint,"Jang, Hongsun; Song, Jaeyong; Jung, Jaewon; Park, Jaeyoung; Kim, Youngsok; Lee, Jinho","Jang, Hongsun (); Song, Jaeyong (); Jung, Jaewon (); Park, Jaeyoung (); Kim, Youngsok (); Lee, Jinho ()",,"Jang, Hongsun (); Song, Jaeyong (); Jung, Jaewon (); Park, Jaeyoung (); Kim, Youngsok (); Lee, Jinho ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1169619528,40 Engineering; 46 Information and Computing Sciences; 4605 Data Management and Data Science,7 Affordable and Clean Energy
1398,pub.1181226538,10.48550/arxiv.2410.07356,,,Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models,"High-level synthesis (HLS) allows hardware designers to create hardware
designs with high-level programming languages like C/C++/OpenCL, which greatly
improves hardware design productivity. However, existing HLS flows require
programmers' hardware design expertise and rely on programmers' manual code
transformations and directive annotations to guide compiler optimizations.
Optimizing HLS designs requires non-trivial HLS expertise and tedious iterative
process in HLS code optimization. Automating HLS code optimizations has become
a burning need. Recently, large language models (LLMs) trained on massive code
and programming tasks have demonstrated remarkable proficiency in comprehending
code, showing the ability to handle domain-specific programming queries
directly without labor-intensive fine-tuning. In this work, we propose a novel
retrieval-augmented LLM-based approach to effectively optimize high-level
synthesis (HLS) programs. Our proposed method leverages few-shot learning,
enabling large language models to adopt domain-specific knowledge through
natural language prompts. We propose a unique framework, Retrieve Augmented
Large Language Model Aided Design (RALAD), designed to enhance LLMs'
performance in HLS code optimization tasks. RALAD employs advanced embedding
techniques and top-\emph{k} search algorithms to dynamically source relevant
knowledge from extensive databases, thereby providing contextually appropriate
responses to complex programming queries. Our implementation of RALAD on two
specialized domains, utilizing comparatively smaller language models, achieves
an impressive 80\% success rate in compilation tasks and outperforms general
LLMs by 3.7 -- 19$\times$ in latency improvement.",,,arXiv,,,,2024-10-09,2024,,,,,,All OA; Green,Preprint,"Xu, Haocheng; Hu, Haotian; Huang, Sitao","Xu, Haocheng (); Hu, Haotian (); Huang, Sitao ()",,"Xu, Haocheng (); Hu, Haotian (); Huang, Sitao ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1181226538,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences; 4602 Artificial Intelligence",
1397,pub.1176209645,10.1109/lad62341.2024.10691855,,,Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models,"High-level synthesis (HLS) allows hardware designers to create hardware designs with high-level programming languages like C/C++/OpenCL, which greatly improves hardware design productivity. However, existing HLS flows require programmers’ hardware design expertise and rely on programmers’ manual code transformations and directive annotations to guide compiler optimizations. Optimizing HLS designs requires nontrivial HLS expertise and tedious iterative process in HLS code optimization. Automating HLS code optimizations has become a burning need. Recently, large language models (LLMs) trained on massive code and programming tasks have demonstrated remarkable proficiency in comprehending code, showing the ability to handle domain-specific programming queries directly without labor-intensive fine-tuning. In this work, we propose a novel retrieval-augmented LLM-based approach to effectively optimize high-level synthesis (HLS) programs. Our proposed method leverages few-shot learning, enabling large language models to adopt domain-specific knowledge through natural language prompts. We propose a unique framework, Retrieve Augmented Large Language Model Aided Design (RALAD), designed to enhance LLMs’ performance in HLS code optimization tasks. RALAD employs advanced embedding techniques and top- k search algorithms to dynamically source relevant knowledge from extensive databases, thereby providing contextually appropriate responses to complex programming queries. Our implementation of RALAD on two specialized domains, utilizing comparatively smaller language models, achieves an impressive 80% success rate in compilation tasks and outperforms general LLMs by 3.7 – 19× in latency improvement.",,,,2024 IEEE LLM Aided Design Workshop (LAD),,,2024-06-29,2024,,2024-06-29,00,,1-5,Closed,Proceeding,"Xu, Haocheng; Hu, Haotian; Huang, Sitao","Xu, Haocheng (Dept. of EECS, University of California, Irvine, Irvine, USA); Hu, Haotian (School of the Gifted Young, University of Science and Technology of China, Hefei, China); Huang, Sitao (Dept. of EECS, University of California, Irvine, Irvine, USA)","Xu, Haocheng (University of California, Irvine)","Xu, Haocheng (University of California, Irvine); Hu, Haotian (University of Science and Technology of China); Huang, Sitao (University of California, Irvine)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1176209645,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences; 4602 Artificial Intelligence",
1391,pub.1175346583,10.1145/3670474.3685953,,,Automated C/C++ Program Repair for High-Level Synthesis via Large Language Models,"In High-Level Synthesis (HLS), converting a regular C/C++ program into its HLS-compatible counterpart (HLS-C) still requires tremendous manual effort. Various program scripts have been introduced to automate this process. But the resulting codes usually contain many issues that should be manually repaired by developers. Since Large Language Models (LLMs) have the ability to automate code generation, they can also be used for automated program repair in HLS. However, due to the limited training of LLMs considering hardware and software simultaneously, hallucinations may occur during program repair using LLMs, leading to compilation failures. Besides, using LLMs for iterative repair also incurs a high cost. To address these challenges, we propose an LLM-driven program repair framework that takes regular C/C++ code as input and automatically generates its corresponding HLS-C code for synthesis while minimizing human repair effort. To mitigate the hallucinations in LLMs and enhance the prompt quality, a Retrieval-Augmented Generation (RAG) paradigm is introduced to guide the LLMs toward correct repair. In addition, we use LLMs to create a static bit width optimization program to identify the optimized bit widths for variables. Moreover, LLM-driven HLS optimization strategies are introduced to add/tune pragmas in HLS-C programs for circuit optimization. Experimental results demonstrate that the proposed LLM-driven automated framework can achieve much higher repair pass rates in 24 real-world applications compared with the traditional scripts and the direct application of LLMs for program repair. The codes are open-sourced at this link: https://github.com/code-source1/catapult.",,,,Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD,,,2024-09-09,2024,2024-09-09,2024-09-09,,,1-9,All OA; Green,Proceeding,"Xu, Kangwei; Zhang, Grace Li; Yin, Xunzhao; Zhuo, Cheng; Schlichtmann, Ulf; Li, Bing","Xu, Kangwei (Chair of Electronic Design Automation, Technical University of Munich (TUM), Munich, Germany); Zhang, Grace Li (Hardware for Artificial Intelligence Group, Technical University of Darmstadt, Darmstadt, Germany); Yin, Xunzhao (College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China); Zhuo, Cheng (College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China); Schlichtmann, Ulf (Chair of Electronic Design Automation, Technical University of Munich (TUM), Munich, Germany); Li, Bing (Research Group of Digital Integrated Systems, University of Siegen, Siegen, Germany)",,"Xu, Kangwei (Technical University of Munich); Zhang, Grace Li (TU Darmstadt); Yin, Xunzhao (Zhejiang University); Zhuo, Cheng (Zhejiang University); Schlichtmann, Ulf (Technical University of Munich); Li, Bing (University of Siegen)",0,0,,,http://arxiv.org/pdf/2407.03889,https://app.dimensions.ai/details/publication/pub.1175346583,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences; 4612 Software Engineering",
1385,pub.1173610669,10.48550/arxiv.2407.03889,,,Automated C/C++ Program Repair for High-Level Synthesis via Large Language Models,"In High-Level Synthesis (HLS), converting a regular C/C++ program into its
HLS-compatible counterpart (HLS-C) still requires tremendous manual effort.
Various program scripts have been introduced to automate this process. But the
resulting codes usually contain many issues that should be manually repaired by
developers. Since Large Language Models (LLMs) have the ability to automate
code generation, they can also be used for automated program repair in HLS.
However, due to the limited training of LLMs considering hardware and software
simultaneously, hallucinations may occur during program repair using LLMs,
leading to compilation failures. Besides, using LLMs for iterative repair also
incurs a high cost. To address these challenges, we propose an LLM-driven
program repair framework that takes regular C/C++ code as input and
automatically generates its corresponding HLS-C code for synthesis while
minimizing human repair effort. To mitigate the hallucinations in LLMs and
enhance the prompt quality, a Retrieval-Augmented Generation (RAG) paradigm is
introduced to guide the LLMs toward correct repair. In addition, we use LLMs to
create a static bit width optimization program to identify the optimized bit
widths for variables. Moreover, LLM-driven HLS optimization strategies are
introduced to add/tune pragmas in HLS-C programs for circuit optimization.
Experimental results demonstrate that the proposed LLM-driven automated
framework can achieve much higher repair pass rates in 24 real-world
applications compared with the traditional scripts and the direct application
of LLMs for program repair.",,,arXiv,,,,2024-07-04,2024,,,,,,All OA; Green,Preprint,"Xu, Kangwei; Zhang, Grace Li; Yin, Xunzhao; Zhuo, Cheng; Schlichtmann, Ulf; Li, Bing","Xu, Kangwei (); Zhang, Grace Li (); Yin, Xunzhao (); Zhuo, Cheng (); Schlichtmann, Ulf (); Li, Bing ()",,"Xu, Kangwei (); Zhang, Grace Li (); Yin, Xunzhao (); Zhuo, Cheng (); Schlichtmann, Ulf (); Li, Bing ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1173610669,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences; 4612 Software Engineering",
1379,pub.1170400076,10.1109/hpca57654.2024.00034,,,Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System,"The recent huge advance of Large Language Models (LLMs) is mainly driven by the increase in the number of parameters. This has led to substantial memory capacity requirements, necessitating the use of dozens of GPUs just to meet the capacity. One popular solution to this is storage-offloaded training, which uses host memory and storage as an extended memory hierarchy. However, this obviously comes at the cost of storage bandwidth bottleneck because storage devices have orders of magnitude lower bandwidth compared to that of GPU device memories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck of storage-offloaded LLM training using near-storage processing devices on a real system. The main component of Smart-Infinity is SmartUpdate, which performs parameter updates on custom near-storage accelerators. We identify that moving parameter updates to the storage side removes most of the storage traffic. In addition, we propose an efficient data transfer handler structure to address the system integration issues for Smart-Infinity. The handler allows overlapping data transfers with fixed memory consumption by reusing the device buffer. Lastly, we propose accelerator-assisted gradient compression/decompression to enhance the scalability of Smart-Infinity. When scaling to multiple near-storage processing devices, the write traffic on the shared channel becomes the bottleneck. To alleviate this, we compress the gradients on the GPU and decompress them on the accelerators. It provides further acceleration from reduced traffic. As a result, Smart-Infinity achieves a significant speedup compared to the baseline. Notably, SmartInfinity is a ready-to-use approach that is fully integrated into PyTorch on a real system. The implementation of Smart-Infinity is available at https://github.com/AIS-SNU/smart-infinity.","This work was supported by Samsung Electronics Co., Ltd (IO221213-04119-01), the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (2022R1C1C1011307, 2022R1C1C1008131), and Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) under the artificial intelligence semiconductor support program to nurture the best talents (IITP2023-RS-2023-00256081) grant funded by the Korea government (MSIT). The experimental environment was provided by Samsung Memory Research Center (SMRC). Hongsun Jang, Jaewon Jung, Youngsok Kim, and Jinho Lee were partly supported by the BK21 FOUR (Fostering Outstanding Universities for Research) funded by the Ministry of Education (MOE) and the National Research Foundation (NRF) of Korea.","This work was supported by Samsung Electronics Co., Ltd (IO221213-04119-01), the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (2022R1C1C1011307, 2022R1C1C1008131), and Institute of Information & communications Technology Planning & Evaluation (IITP) under the artificial intelligence semiconductor support program to nurture the best talents (IITP2023-RS-2023-00256081) grant funded by the Korea government (MSIT). The experimental environment was provided by Samsung Memory Research Center (SMRC). Hongsun Jang, Jaewon Jung, Youngsok Kim, and Jinho Lee were partly supported by the BK21 FOUR (Fostering Outstanding Universities for Research) funded by the Ministry of Education (MOE) and the National Research Foundation (NRF) of Korea.",,2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA),,,2024-03-06,2024,,2024-03-06,00,,345-360,All OA; Green,Proceeding,"Jang, Hongsun; Song, Jaeyong; Jung, Jaewon; Park, Jaeyoung; Kim, Youngsok; Lee, Jinho","Jang, Hongsun (Department of Electrical and Computer Engineering, Seoul National University); Song, Jaeyong (Department of Electrical and Computer Engineering, Seoul National University); Jung, Jaewon (Department of Electrical and Computer Engineering, Seoul National University); Park, Jaeyoung (Department of Electrical and Computer Engineering, University of Texas at Austin); Kim, Youngsok (Department of Computer Science, Yonsei University); Lee, Jinho (Department of Electrical and Computer Engineering, Seoul National University)","Lee, Jinho (Seoul National University)","Jang, Hongsun (Seoul National University); Song, Jaeyong (Seoul National University); Jung, Jaewon (Seoul National University); Park, Jaeyoung (Department of Electrical and Computer Engineering, University of Texas at Austin); Kim, Youngsok (Yonsei University); Lee, Jinho (Seoul National University)",3,3,,,https://arxiv.org/pdf/2403.06664,https://app.dimensions.ai/details/publication/pub.1170400076,40 Engineering; 46 Information and Computing Sciences; 4605 Data Management and Data Science,7 Affordable and Clean Energy
1375,pub.1164952289,10.48550/arxiv.2310.09949,,,Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models,"A Retrieval-Augmented Language Model (RALM) augments a generative language
model by retrieving context-specific knowledge from an external database. This
strategy facilitates impressive text generation quality even with smaller
models, thus reducing orders of magnitude of computational demands. However,
RALMs introduce unique system design challenges due to (a) the diverse workload
characteristics between LM inference and retrieval and (b) the various system
requirements and bottlenecks for different RALM configurations such as model
sizes, database sizes, and retrieval frequencies. We propose Chameleon, a
heterogeneous accelerator system that integrates both LM and retrieval
accelerators in a disaggregated architecture. The heterogeneity ensures
efficient acceleration of both LM inference and retrieval, while the
accelerator disaggregation enables the system to independently scale both types
of accelerators to fulfill diverse RALM requirements. Our Chameleon prototype
implements retrieval accelerators on FPGAs and assigns LM inference to GPUs,
with a CPU server orchestrating these accelerators over the network. Compared
to CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72x
speedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleon
exhibits up to 2.16x reduction in latency and 3.18x speedup in throughput
compared to the hybrid CPU-GPU architecture. These promising results pave the
way for bringing accelerator heterogeneity and disaggregation into future RALM
systems.",,,arXiv,,,,2023-10-15,2023,,,,,,All OA; Green,Preprint,"Jiang, Wenqi; Zeller, Marco; Waleffe, Roger; Hoefler, Torsten; Alonso, Gustavo","Jiang, Wenqi (); Zeller, Marco (); Waleffe, Roger (); Hoefler, Torsten (); Alonso, Gustavo ()",,"Jiang, Wenqi (); Zeller, Marco (); Waleffe, Roger (); Hoefler, Torsten (); Alonso, Gustavo ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1164952289,33 Built Environment and Design; 3301 Architecture; 46 Information and Computing Sciences,7 Affordable and Clean Energy
1374,pub.1182105233,10.1109/mlcad62225.2024.10740262,,,Automated C/C++ Program Repair for High-Level Synthesis via Large Language Models,"In High-Level Synthesis (HLS), converting a regular C/C++ program into its HLS-compatible counterpart (HLS-C) still requires tremendous manual effort. Various program scripts have been introduced to automate this process. But the resulting codes usually contain many issues that should be manually repaired by developers. Since Large Language Models (LLMs) have the ability to automate code generation, they can also be used for automated program repair in HLS. However, due to the limited training of LLMs considering hardware and software simultaneously, hallucinations may occur during program repair using LLMs, leading to compilation failures. Besides, using LLMs for iterative repair also incurs a high cost. To address these challenges, we propose an LLM-driven program repair framework that takes regular $\mathrm{C} / \mathrm{C}++$ code as input and automatically generates its corresponding HLS-C code for synthesis while minimizing human repair effort. To mitigate the hallucinations in LLMs and enhance the prompt quality, a Retrieval-Augmented Generation (RAG) paradigm is introduced to guide the LLMs toward correct repair. In addition, we use LLMs to create a static bit width optimization program to identify the optimized bit widths for variables. Moreover, LLM-driven HLS optimization strategies are introduced to add/tune pragmas in HLS-C programs for circuit optimization. Experimental results demonstrate that the proposed LLM-driven automated framework can achieve much higher repair pass rates in 24 real-world applications compared with the traditional scripts and the direct application of LLMs for program repair. The codes are open-sourced at this link: https://github.com/code-source1/catapult.","This work is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 504518248 and supported by TUM International Graduate School of Science and Engineering (IGSSE).","This work is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 504518248 and supported by TUM International Graduate School of Science and Engineering (IGSSE).",,2024 ACM/IEEE 6th Symposium on Machine Learning for CAD (MLCAD),,,2024-01-11,2024,,2024-01-11,00,,1-9,All OA; Green,Proceeding,"Xu, Kangwei; Zhang, Grace Li; Yin, Xunzhao; Zhuo, Cheng; Schlichtmann, Ulf; Li, Bing","Xu, Kangwei (Chair of Electronic Design Automation, Technical University of Munich (TUM), Munich, Germany); Zhang, Grace Li (Hardware for Artificial Intelligence Group, Technical University of Darmstadt, Darmstadt, Germany); Yin, Xunzhao (College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China); Zhuo, Cheng (College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China); Schlichtmann, Ulf (Chair of Electronic Design Automation, Technical University of Munich (TUM), Munich, Germany); Li, Bing (Research Group of Digital Integrated Systems, University of Siegen, Siegen, Germany)","Xu, Kangwei (Technical University of Munich)","Xu, Kangwei (Technical University of Munich); Zhang, Grace Li (TU Darmstadt); Yin, Xunzhao (Zhejiang University); Zhuo, Cheng (Zhejiang University); Schlichtmann, Ulf (Technical University of Munich); Li, Bing (University of Siegen)",0,0,,,http://arxiv.org/pdf/2407.03889,https://app.dimensions.ai/details/publication/pub.1182105233,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences; 4612 Software Engineering",
1369,pub.1174540655,10.48550/arxiv.2408.02793,,,Evaluating Large Language Models for Automatic Register Transfer Logic Generation via High-Level Synthesis,"The ever-growing popularity of large language models (LLMs) has resulted in
their increasing adoption for hardware design and verification. Prior research
has attempted to assess the capability of LLMs to automate digital hardware
design by producing superior-quality Register Transfer Logic (RTL)
descriptions, particularly in Verilog. However, these tests have revealed that
Verilog code production using LLMs at current state-of-the-art lack sufficient
functional correctness to be practically viable, compared to automatic
generation of programs in general-purpose programming languages such as C, C++,
Python, etc. With this as the key insight, in this paper we assess the
performance of a two-stage software pipeline for automated Verilog RTL
generation: LLM based automatic generation of annotated C++ code suitable for
high-level synthesis (HLS), followed by HLS to generate Verilog RTL. We have
benchmarked the performance of our proposed scheme using the open-source
VerilogEval dataset, for four different industry-scale LLMs, and the Vitis HLS
tool. Our experimental results demonstrate that our two-step technique
substantially outperforms previous proposed techniques of direct Verilog RTL
generation by LLMs in terms of average functional correctness rates, reaching
score of 0.86 in pass@1 metric.",,,arXiv,,,,2024-08-05,2024,,,,,,All OA; Green,Preprint,"Swaroopa, Sneha; Mukherjee, Rijoy; Debnath, Anushka; Chakraborty, Rajat Subhra","Swaroopa, Sneha (); Mukherjee, Rijoy (); Debnath, Anushka (); Chakraborty, Rajat Subhra ()",,"Swaroopa, Sneha (); Mukherjee, Rijoy (); Debnath, Anushka (); Chakraborty, Rajat Subhra ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1174540655,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",
1366,pub.1168042270,10.48550/arxiv.2401.09890,,,A Survey on Hardware Accelerators for Large Language Models,"Large Language Models (LLMs) have emerged as powerful tools for natural
language processing tasks, revolutionizing the field with their ability to
understand and generate human-like text. As the demand for more sophisticated
LLMs continues to grow, there is a pressing need to address the computational
challenges associated with their scale and complexity. This paper presents a
comprehensive survey on hardware accelerators designed to enhance the
performance and energy efficiency of Large Language Models. By examining a
diverse range of accelerators, including GPUs, FPGAs, and custom-designed
architectures, we explore the landscape of hardware solutions tailored to meet
the unique computational demands of LLMs. The survey encompasses an in-depth
analysis of architecture, performance metrics, and energy efficiency
considerations, providing valuable insights for researchers, engineers, and
decision-makers aiming to optimize the deployment of LLMs in real-world
applications.",,,arXiv,,,,2024-01-18,2024,,,,,,All OA; Green,Preprint,"Kachris, Christoforos","Kachris, Christoforos ()",,"Kachris, Christoforos ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1168042270,33 Built Environment and Design; 3301 Architecture; 46 Information and Computing Sciences,7 Affordable and Clean Energy
1359,pub.1182091476,10.1109/mlcad62225.2024.10740232,,,MLCAD 2024 Cover Page,,,,,2024 ACM/IEEE 6th Symposium on Machine Learning for CAD (MLCAD),,,2024-11-06,2024,2024-11-06,,,,c1-c321,Closed,Proceeding,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1182091476,,
1348,pub.1173082302,10.1145/3625223.3649280,,,HDLGen-ChatGPT Case Study: RISC-V Processor VHDL and Verilog Model - Testbench and EDA Project Generation,"This paper presents the open source HDLGen-ChatGPT application, working in tandem with ChatGPT-3.5, the free online large language model (LLM) chat interface. The tools enable fast digital systems design and test specification capture, and automatic generation of both VHDL and Verilog models, and testbenches, and AMD Vivado and Intel Quartus Electronic Design Automation (EDA) projects. EDA tools check the generated HDL syntax, simulate and synthesise HDL models, and follow the steps to FPGA hardware prototyping. The tools exploit a formal, top-down design and test specification documentation process, domain knowledge, and the flexibility of LLMs, for HDL code generation. Results are included for a hierarchical RV32I RISC-V processor design. Process steps are illustrated for the RISC-V 32 × 32-bit register bank component. The process typically requires only minimal manual HDL capture or editing, and often none at all. URLs to tutorial videos for the complete RISC-V design are provided on the GitHub project repository. The paper evaluates the results and provides HDLGen-ChatGPT and ChatGPT usage recommendations. The tools can be applied in digital systems training programmes, with reduced emphasis on the assessment of HDL model and testbench capture and generation, while maintaining strong emphasis on the assessment of system design, test planning and documentation, HDL simulation verification/debug, and analysis of synthesised netlists.",,,,Proceedings of the 34th International Workshop on Rapid System Prototyping,,,2023-09-21,2023,2024-06-21,2023-09-21,,,1-7,Closed,Proceeding,"Morgan, Fearghal; Byrne, John Patrick; Bupathi, Abishek; George, Roshan; Elahi, Adnan; Callaly, Frank; Kelly, Seán; O'Loughlin, Declan","Morgan, Fearghal (University of Galway, Galway, Ireland); Byrne, John Patrick (University of Galway, Galway, Ireland); Bupathi, Abishek (University of Galway, Galway, Ireland); George, Roshan (University of Galway, Galway, Ireland); Elahi, Adnan (University of Galway, Galway, Ireland); Callaly, Frank (University of Galway, Galway, Ireland); Kelly, Seán (University of Galway, Galway, Ireland); O'Loughlin, Declan (Trinity College Dublin, Dublin, Ireland)","Morgan, Fearghal (University of Galway)","Morgan, Fearghal (University of Galway); Byrne, John Patrick (University of Galway); Bupathi, Abishek (University of Galway); George, Roshan (University of Galway); Elahi, Adnan (University of Galway); Callaly, Frank (University of Galway); Kelly, Seán (University of Galway); O'Loughlin, Declan (Trinity College Dublin)",1,1,,0.91,,https://app.dimensions.ai/details/publication/pub.1173082302,33 Built Environment and Design; 40 Engineering; 4008 Electrical Engineering; 4010 Engineering Practice and Education; 46 Information and Computing Sciences,
1339,pub.1181619736,10.48550/arxiv.2410.18582,,,LLM-Aided Efficient Hardware Design Automation,"With the rapidly increasing complexity of modern chips, hardware engineers
are required to invest more effort in tasks such as circuit design,
verification, and physical implementation. These workflows often involve
continuous modifications, which are labor-intensive and prone to errors.
Therefore, there is an increasing need for more efficient and cost-effective
Electronic Design Automation (EDA) solutions to accelerate new hardware
development. Recently, large language models (LLMs) have made significant
advancements in contextual understanding, logical reasoning, and response
generation. Since hardware designs and intermediate scripts can be expressed in
text format, it is reasonable to explore whether integrating LLMs into EDA
could simplify and fully automate the entire workflow. Accordingly, this paper
discusses such possibilities in several aspects, covering hardware description
language (HDL) generation, code debugging, design verification, and physical
implementation. Two case studies, along with their future outlook, are
introduced to highlight the capabilities of LLMs in code repair and testbench
generation. Finally, future directions and challenges are highlighted to
further explore the potential of LLMs in shaping the next-generation EDA",,,arXiv,,,,2024-10-24,2024,,,,,,All OA; Green,Preprint,"Xu, Kangwei; Qiu, Ruidi; Zhao, Zhuorui; Zhang, Grace Li; Schlichtmann, Ulf; Li, Bing","Xu, Kangwei (); Qiu, Ruidi (); Zhao, Zhuorui (); Zhang, Grace Li (); Schlichtmann, Ulf (); Li, Bing ()",,"Xu, Kangwei (); Qiu, Ruidi (); Zhao, Zhuorui (); Zhang, Grace Li (); Schlichtmann, Ulf (); Li, Bing ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1181619736,40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,
1338,pub.1170523943,10.48550/arxiv.2404.04815,,,Allo: A Programming Model for Composable Accelerator Design,"Special-purpose hardware accelerators are increasingly pivotal for sustaining
performance improvements in emerging applications, especially as the benefits
of technology scaling continue to diminish. However, designers currently lack
effective tools and methodologies to construct complex, high-performance
accelerator architectures in a productive manner. Existing high-level synthesis
(HLS) tools often require intrusive source-level changes to attain satisfactory
quality of results. Despite the introduction of several new accelerator design
languages (ADLs) aiming to enhance or replace HLS, their advantages are more
evident in relatively simple applications with a single kernel. Existing ADLs
prove less effective for realistic hierarchical designs with multiple kernels,
even if the design hierarchy is flattened.
  In this paper, we introduce Allo, a composable programming model for
efficient spatial accelerator design. Allo decouples hardware customizations,
including compute, memory, communication, and data type from algorithm
specification, and encapsulates them as a set of customization primitives. Allo
preserves the hierarchical structure of an input program by combining
customizations from different functions in a bottom-up, type-safe manner. This
approach facilitates holistic optimizations that span across function
boundaries. We conduct comprehensive experiments on commonly-used HLS
benchmarks and several realistic deep learning models. Our evaluation shows
that Allo can outperform state-of-the-art HLS tools and ADLs on all test cases
in the PolyBench. For the GPT2 model, the inference latency of the Allo
generated accelerator is 1.7x faster than the NVIDIA A100 GPU with 5.4x higher
energy efficiency, demonstrating the capability of Allo to handle large-scale
designs.",,,arXiv,,,,2024-04-07,2024,,,,,,All OA; Green,Preprint,"Chen, Hongzheng; Zhang, Niansong; Xiang, Shaojie; Zeng, Zhichen; Dai, Mengjia; Zhang, Zhiru","Chen, Hongzheng (); Zhang, Niansong (); Xiang, Shaojie (); Zeng, Zhichen (); Dai, Mengjia (); Zhang, Zhiru ()",,"Chen, Hongzheng (); Zhang, Niansong (); Xiang, Shaojie (); Zeng, Zhichen (); Dai, Mengjia (); Zhang, Zhiru ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1170523943,"33 Built Environment and Design; 40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware",7 Affordable and Clean Energy
1337,pub.1039274834,10.1145/996566.996606,,,An area estimation methodology for FPGA based designs at systemc-level,This paper presents a parametric area estimation methodology at SystemC level for FPGA-based designs. The approach is conceived to reduce the effort to adapt the area estimators to the evolutions of the EDA design environments. It consists in identifying the subset of measures that can be derived form the system level description and that are also relevant at VHDL-RT level. Estimators' parameters are then automatically derived from a set of benchmarks.,,,,Proceedings of the 41st annual Design Automation Conference,,,2004-06-07,2004,2004-06-07,2004-06-07,,,129-132,All OA; Green,Proceeding,"Brandolese, Carlo; Fornaciari, William; Salice, Fabio","Brandolese, Carlo (Politecnico di Milano, Milano, Italy); Fornaciari, William (Politecnico di Milano, Milano, Italy); Salice, Fabio (Politecnico di Milano, Milano, Italy)",,"Brandolese, Carlo (Politecnico di Milano); Fornaciari, William (Politecnico di Milano); Salice, Fabio (Politecnico di Milano)",44,1,,11.63,http://www.cs.york.ac.uk/rts/docs/SIGDA-Compendium-1994-2004/papers/2004/dac04/pdffiles/p129.pdf,https://app.dimensions.ai/details/publication/pub.1039274834,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware",
1317,pub.1170797374,10.48550/arxiv.2404.10875,,,SA-DS: A Dataset for Large Language Model-Driven AI Accelerator Design Generation,"In the ever-evolving landscape of Deep Neural Networks (DNN) hardware
acceleration, unlocking the true potential of systolic array accelerators has
long been hindered by the daunting challenges of expertise and time investment.
Large Language Models (LLMs) offer a promising solution for automating code
generation which is key to unlocking unprecedented efficiency and performance
in various domains, including hardware descriptive code. The generative power
of LLMs can enable the effective utilization of preexisting designs and
dedicated hardware generators. However, the successful application of LLMs to
hardware accelerator design is contingent upon the availability of specialized
datasets tailored for this purpose. To bridge this gap, we introduce the
Systolic Array-based Accelerator Data Set (SA-DS). SA-DS comprises a diverse
collection of spatial array designs following the standardized Berkeley's
Gemmini accelerator generator template, enabling design reuse, adaptation, and
customization. SA-DS is intended to spark LLM-centered research on DNN hardware
accelerator architecture. We envision that SA-DS provides a framework that will
shape the course of DNN hardware acceleration research for generations to come.
SA-DS is open-sourced under the permissive MIT license at
https://github.com/ACADLab/SA-DS.git}{https://github.com/ACADLab/SA-DS.",,,arXiv,,,,2024-04-16,2024,,,,,,All OA; Green,Preprint,"Vungarala, Deepak; Nazzal, Mahmoud; Morsali, Mehrdad; Zhang, Chao; Ghosh, Arnob; Khreishah, Abdallah; Angizi, Shaahin","Vungarala, Deepak (); Nazzal, Mahmoud (); Morsali, Mehrdad (); Zhang, Chao (); Ghosh, Arnob (); Khreishah, Abdallah (); Angizi, Shaahin ()",,"Vungarala, Deepak (); Nazzal, Mahmoud (); Morsali, Mehrdad (); Zhang, Chao (); Ghosh, Arnob (); Khreishah, Abdallah (); Angizi, Shaahin ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1170797374,"40 Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",7 Affordable and Clean Energy
1314,pub.1172839857,10.48550/arxiv.2406.09233,,,C2HLSC: Can LLMs Bridge the Software-to-Hardware Design Gap?,"High Level Synthesis (HLS) tools offer rapid hardware design from C code, but
their compatibility is limited by code constructs. This paper investigates
Large Language Models (LLMs) for refactoring C code into HLS-compatible
formats. We present several case studies by using an LLM to rewrite C code for
NIST 800-22 randomness tests, a QuickSort algorithm and AES-128 into
HLS-synthesizable c. The LLM iteratively transforms the C code guided by user
prompts, implementing functions like streaming data and hardware-specific
signals. This evaluation demonstrates the LLM's potential to assist hardware
design refactoring regular C code into HLS synthesizable C code.",,,arXiv,,,,2024-06-13,2024,,,,,,All OA; Green,Preprint,"Collini, Luca; Garg, Siddharth; Karri, Ramesh","Collini, Luca (); Garg, Siddharth (); Karri, Ramesh ()",,"Collini, Luca (); Garg, Siddharth (); Karri, Ramesh ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1172839857,40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,
1306,pub.1182097721,10.1109/mlcad62225.2024.10740204,,,Cross-Modality Program Representation Learning for Electronic Design Automation with High-Level Synthesis,"In recent years, domain-specific accelerators (DSAs) have gained popularity for applications such as deep learning and autonomous driving. To facilitate DSA designs, programmers use high-level synthesis (HLS) to compile a high-level description written in C/C++ into a design with low-level hardware description languages that eventually synthesize DSAs on circuits. However, creating a high-quality HLS design still demands significant domain knowledge, particularly in microarchitecture decisions expressed as pragmas. Thus, it is desirable to automate such decisions with the help of machine learning for predicting the quality of HLS designs, requiring a deeper understanding of the program that consists of original code and pragmas. Naturally, these programs can be considered as sequence data. In addition, these programs can be compiled and converted into a control data flow graph (CDFG). But existing works either fail to leverage both modalities or combine the two in shallow or coarse ways. We propose PROGSG, a model that allows interaction between the source code sequence modality and the graph modality in a deep and fine-grained way. To alleviate the scarcity of labeled designs, a pre-training method is proposed based on a suite of compiler’s data flow analysis tasks. Experimental results show that PROGSG reduces the RMSE of design performance predictions by up to 22%, and identifies designs with an average of 1.10× and 1.26× (up to 8.17× and 13.31×) performance improvement in design space exploration (DSE) task compared to HARP and AutoDSE, respectively. CCS Concepts • Hardware → High-level and register-transfer level synthesis; • Computing methodologies → Neural networks.","This work was partially supported by NSF grants 2211557, 1937599, 2119643, and 2303037, SRC JUMP 2.0 PRISM Center, NASA, Okawa Foundation, Amazon Research, Cisco, Picsart, Snapchat, and the CDSC industrial partners (https://cdsc.ucla.edu/partners/). The authors would also like to thank Maria Brbic (EPFL) for early discussions on integrating GNN and LLM models, AMD/Xilinx for HACC equipment donation, and Marci Baun for editing the paper. J. Cong has a financial interest in AMD.","This work was partially supported by NSF grants 2211557, 1937599, 2119643, and 2303037, SRC JUMP 2.0 PRISM Center, NASA, Okawa Foundation, Amazon Research, Cisco, Picsart, Snapchat, and the CDSC industrial partners (https://cdsc.ucla.edu/partners/). The authors would also like to thank Maria Brbic (EPFL) for early discussions on integrating GNN and LLM models, AMD/Xilinx for HACC equipment donation, and Marci Baun for editing the paper. J. Cong has a financial interest in AMD.",,2024 ACM/IEEE 6th Symposium on Machine Learning for CAD (MLCAD),,,2024-01-11,2024,,2024-01-11,00,,1-12,All OA; Green,Proceeding,"Qin, Zongyue; Bai, Yunsheng; Sohrabizadeh, Atefeh; Ding, Zijian; Hu, Ziniu; Sun, Yizhou; Cong, Jason","Qin, Zongyue (University of California, Los Angeles, USA); Bai, Yunsheng (University of California, Los Angeles, USA); Sohrabizadeh, Atefeh (University of California, Los Angeles, USA); Ding, Zijian (University of California, Los Angeles, USA); Hu, Ziniu (University of California, Los Angeles, USA); Sun, Yizhou (University of California, Los Angeles, USA); Cong, Jason (University of California, Los Angeles, USA)","Qin, Zongyue (University of California, Los Angeles)","Qin, Zongyue (University of California, Los Angeles); Bai, Yunsheng (University of California, Los Angeles); Sohrabizadeh, Atefeh (University of California, Los Angeles); Ding, Zijian (University of California, Los Angeles); Hu, Ziniu (University of California, Los Angeles); Sun, Yizhou (University of California, Los Angeles); Cong, Jason (University of California, Los Angeles)",0,0,,,https://arxiv.org/pdf/2406.09606,https://app.dimensions.ai/details/publication/pub.1182097721,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",
1305,pub.1168184503,10.48550/arxiv.2401.12224,,,LLM4EDA: Emerging Progress in Large Language Models for Electronic Design Automation,"Driven by Moore's Law, the complexity and scale of modern chip design are
increasing rapidly. Electronic Design Automation (EDA) has been widely applied
to address the challenges encountered in the full chip design process. However,
the evolution of very large-scale integrated circuits has made chip design
time-consuming and resource-intensive, requiring substantial prior expert
knowledge. Additionally, intermediate human control activities are crucial for
seeking optimal solutions. In system design stage, circuits are usually
represented with Hardware Description Language (HDL) as a textual format.
Recently, Large Language Models (LLMs) have demonstrated their capability in
context understanding, logic reasoning and answer generation. Since circuit can
be represented with HDL in a textual format, it is reasonable to question
whether LLMs can be leveraged in the EDA field to achieve fully automated chip
design and generate circuits with improved power, performance, and area (PPA).
In this paper, we present a systematic study on the application of LLMs in the
EDA field, categorizing it into the following cases: 1) assistant chatbot, 2)
HDL and script generation, and 3) HDL verification and analysis. Additionally,
we highlight the future research direction, focusing on applying LLMs in logic
synthesis, physical design, multi-modal feature extraction and alignment of
circuits. We collect relevant papers up-to-date in this field via the following
link: https://github.com/Thinklab-SJTU/Awesome-LLM4EDA.",,,arXiv,,,,2023-12-28,2023,,,,,,All OA; Green,Preprint,"Zhong, Ruizhe; Du, Xingbo; Kai, Shixiong; Tang, Zhentao; Xu, Siyuan; Zhen, Hui-Ling; Hao, Jianye; Xu, Qiang; Yuan, Mingxuan; Yan, Junchi","Zhong, Ruizhe (); Du, Xingbo (); Kai, Shixiong (); Tang, Zhentao (); Xu, Siyuan (); Zhen, Hui-Ling (); Hao, Jianye (); Xu, Qiang (); Yuan, Mingxuan (); Yan, Junchi ()",,"Zhong, Ruizhe (); Du, Xingbo (); Kai, Shixiong (); Tang, Zhentao (); Xu, Siyuan (); Zhen, Hui-Ling (); Hao, Jianye (); Xu, Qiang (); Yuan, Mingxuan (); Yan, Junchi ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1168184503,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware",
1302,pub.1176218049,10.1109/lad62341.2024.10691856,,,C2HLSC: Can LLMs Bridge the Software-to-Hardware Design Gap?,"High Level Synthesis (HLS) tools offer rapid hardware design from C code, but their compatibility is limited by code constructs. This paper investigates Large Language Models (LLMs) for refactoring C code into HLS-compatible formats. We present several case studies by using an LLM to rewrite C code for NIST 800-22 randomness tests, a QuickSort algorithm and AES-128 into HLS-synthesizable c. The LLM iteratively transforms the C code guided by user prompts, implementing functions like streaming data and hardware-specific signals. This evaluation demonstrates the LLM’s potential to assist hardware design refactoring regular C code into HLS synthesizable C code.",,,,2024 IEEE LLM Aided Design Workshop (LAD),,,2024-06-29,2024,,2024-06-29,00,,1-12,All OA; Green,Proceeding,"Collini, Luca; Garg, Siddharth; Karri, Ramesh","Collini, Luca (); Garg, Siddharth (); Karri, Ramesh ()","Collini, Luca ()","Collini, Luca (); Garg, Siddharth (); Karri, Ramesh ()",1,1,,,http://arxiv.org/pdf/2406.09233,https://app.dimensions.ai/details/publication/pub.1176218049,40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,
1280,pub.1182069090,10.1109/socc62300.2024.10737859,,,SOCC 2024 Conference Programme,,,,2015 28th IEEE International System-on-Chip Conference (SOCC),2024 IEEE 37th International System-on-Chip Conference (SOCC),,,2024-11-05,2024,2024-11-05,,,,xv-xxxv,Closed,Proceeding,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1182069090,,
1279,pub.1181098255,10.48550/arxiv.2410.04466,,,Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective,"Large Language Models (LLMs) have demonstrated remarkable capabilities across
various fields, from natural language understanding to text generation.
Compared to non-generative LLMs like BERT and DeBERTa, generative LLMs like GPT
series and Llama series are currently the main focus due to their superior
algorithmic performance. The advancements in generative LLMs are closely
intertwined with the development of hardware capabilities. Various hardware
platforms exhibit distinct hardware characteristics, which can help improve LLM
inference performance. Therefore, this paper comprehensively surveys efficient
generative LLM inference on different hardware platforms. First, we provide an
overview of the algorithm architecture of mainstream generative LLMs and delve
into the inference process. Then, we summarize different optimization methods
for different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide
inference results for generative LLMs. Furthermore, we perform a qualitative
and quantitative comparison of inference performance with batch sizes 1 and 8
on different hardware platforms by considering hardware power consumption,
absolute inference speed (tokens/s), and energy efficiency (tokens/J). We
compare the performance of the same optimization methods across different
hardware platforms, the performance across different hardware platforms, and
the performance of different methods on the same hardware platform. This
provides a systematic and comprehensive summary of existing inference
acceleration work by integrating software optimization methods and hardware
platforms, which can point to the future trends and potential developments of
generative LLMs and hardware technology for edge-side scenarios.",,,arXiv,,,,2024-10-06,2024,,,,,,All OA; Green,Preprint,"Li, Jinhao; Xu, Jiaming; Huang, Shan; Chen, Yonghua; Li, Wen; Liu, Jun; Lian, Yaoxiu; Pan, Jiayi; Ding, Li; Zhou, Hao; Wang, Yu; Dai, Guohao","Li, Jinhao (); Xu, Jiaming (); Huang, Shan (); Chen, Yonghua (); Li, Wen (); Liu, Jun (); Lian, Yaoxiu (); Pan, Jiayi (); Ding, Li (); Zhou, Hao (); Wang, Yu (); Dai, Guohao ()",,"Li, Jinhao (); Xu, Jiaming (); Huang, Shan (); Chen, Yonghua (); Li, Wen (); Liu, Jun (); Lian, Yaoxiu (); Pan, Jiayi (); Ding, Li (); Zhou, Hao (); Wang, Yu (); Dai, Guohao ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1181098255,46 Information and Computing Sciences; 4602 Artificial Intelligence,7 Affordable and Clean Energy
1277,pub.1181200825,10.1109/fpl64840.2024.00053,,,LORA: A Latency-Oriented Recurrent Architecture for GPT Model on Multi-FPGA Platform with Communication Optimization,"Large Language Models (LLMs) have been widely deployed in data centers to provide various services, among which the most representative is the Generative Pre-trained Transformer (GPT). The GPT model has heavy memory and computing overhead, and its inference process has two stages with distinct computing characteristics: Prefill and Decode. Utilizing existing GPUs and FPGA accelerators to construct a platform for deploying GPT in data centers faces the challenges of needing more effective synchronization schemes or structures with higher computational intensity. This paper proposes LORA, a low latency end-to-end GPT acceleration platform utilizing multiple FPGAs. Firstly, we optimize the synchronization timing of the GPT model to reduce the computation and communication overhead. Secondly, we devise some efficient synchronization steps for specific layers of the GPT model that overlap part of the computation and communication delay to improve the latency of our platform. Finally, we deploy recurrent structures on each FPGA to accelerate the different stages of the GPT model. Implemented on the Xilinx Alveo U280 FPGAs, LORA achieves an average $11.1 \times$ speedup over NVIDIA V100 GPUs on the modern GPT-2 model. Compared to the existing multi-FPGA accelerator appliance, LORA shows performance improvements of up to $4 \times$ and $2.7 \times$ in the Prefill and Decode stages.","This work was supported in part by the National Key R&amp;D Program of China under Grants 2022YFB4501600 and 2022YFB4501603, in part by the National Natural Science Foundation of China under Grants 62102383, 61976200, and 62172380, in part by Jiangsu Provincial Natural Science Foundation under Grant BK20210123, in part by Youth Innovation Promotion Association CAS under Grant Y2021121.","This work was supported in part by the National Key R&D Program of China under Grants 2022YFB4501600 and 2022YFB4501603, in part by the National Natural Science Foundation of China under Grants 62102383, 61976200, and 62172380, in part by Jiangsu Provincial Natural Science Foundation under Grant BK20210123, in part by Youth Innovation Promotion Association CAS under Grant Y2021121.",,2024 34th International Conference on Field-Programmable Logic and Applications (FPL),,,2024-01-06,2024,,2024-01-06,00,,332-338,Closed,Proceeding,"Zheng, ZhenDong; Cheng, Qianyu; Wang, Teng; Gong, Lei; Chen, Xianglan; Tang, Cheng; Wang, Chao; Zhou, Xuehai","Zheng, ZhenDong (School of Computer Science and Technology, Suzhou Institute for Advanced Research University of Science and Technology of China); Cheng, Qianyu (School of Computer Science and Technology, Suzhou Institute for Advanced Research University of Science and Technology of China); Wang, Teng (School of Computer Science and Technology, Suzhou Institute for Advanced Research University of Science and Technology of China); Gong, Lei (School of Computer Science and Technology, Suzhou Institute for Advanced Research University of Science and Technology of China); Chen, Xianglan (School of Computer Science and Technology, Suzhou Institute for Advanced Research University of Science and Technology of China); Tang, Cheng (School of Computer Science and Technology, Suzhou Institute for Advanced Research University of Science and Technology of China); Wang, Chao (School of Computer Science and Technology, Suzhou Institute for Advanced Research University of Science and Technology of China); Zhou, Xuehai (School of Computer Science and Technology, Suzhou Institute for Advanced Research University of Science and Technology of China)","Wang, Teng (University of Science and Technology of China); Gong, Lei (University of Science and Technology of China)","Zheng, ZhenDong (University of Science and Technology of China); Cheng, Qianyu (University of Science and Technology of China); Wang, Teng (University of Science and Technology of China); Gong, Lei (University of Science and Technology of China); Chen, Xianglan (University of Science and Technology of China); Tang, Cheng (University of Science and Technology of China); Wang, Chao (University of Science and Technology of China); Zhou, Xuehai (University of Science and Technology of China)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1181200825,40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,
1276,pub.1158185737,10.48550/arxiv.2305.10838,,,ProgSG: Cross-Modality Representation Learning for Programs in Electronic Design Automation,"Recent years have witnessed the growing popularity of domain-specific
accelerators (DSAs), such as Google's TPUs, for accelerating various
applications such as deep learning, search, autonomous driving, etc. To
facilitate DSA designs, high-level synthesis (HLS) is used, which allows a
developer to compile a high-level description in the form of software code in C
and C++ into a design in low-level hardware description languages (such as VHDL
or Verilog) and eventually synthesized into a DSA on an ASIC
(application-specific integrated circuit) or FPGA (field-programmable gate
arrays). However, existing HLS tools still require microarchitecture decisions,
expressed in terms of pragmas (such as directives for parallelization and
pipelining). To enable more people to design DSAs, it is desirable to automate
such decisions with the help of deep learning for predicting the quality of HLS
designs. This requires us a deeper understanding of the program, which is a
combination of original code and pragmas. Naturally, these programs can be
considered as sequence data, for which large language models (LLM) can help. In
addition, these programs can be compiled and converted into a control data flow
graph (CDFG), and the compiler also provides fine-grained alignment between the
code tokens and the CDFG nodes. However, existing works either fail to leverage
both modalities or combine the two in shallow or coarse ways. We propose ProgSG
allowing the source code sequence modality and the graph modalities to interact
with each other in a deep and fine-grained way. To alleviate the scarcity of
labeled designs, a pre-training method is proposed based on a suite of
compiler's data flow analysis tasks. Experimental results on two benchmark
datasets show the superiority of ProgSG over baseline methods that either only
consider one modality or combine the two without utilizing the alignment
information.",,,arXiv,,,,2023-05-18,2023,,,,,,All OA; Green,Preprint,"Bai, Yunsheng; Sohrabizadeh, Atefeh; Qin, Zongyue; Hu, Ziniu; Sun, Yizhou; Cong, Jason","Bai, Yunsheng (); Sohrabizadeh, Atefeh (); Qin, Zongyue (); Hu, Ziniu (); Sun, Yizhou (); Cong, Jason ()",,"Bai, Yunsheng (); Sohrabizadeh, Atefeh (); Qin, Zongyue (); Hu, Ziniu (); Sun, Yizhou (); Cong, Jason ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1158185737,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",
1276,pub.1175346851,10.1145/3670474.3685952,,,Cross-Modality Program Representation Learning for Electronic Design Automation with High-Level Synthesis,"In recent years, domain-specific accelerators (DSAs) have gained popularity for applications such as deep learning and autonomous driving. To facilitate DSA designs, programmers use high-level synthesis (HLS) to compile a high-level description written in C/C++ into a design with low-level hardware description languages that eventually synthesize DSAs on circuits. However, creating a high-quality HLS design still demands significant domain knowledge, particularly in microarchitecture decisions expressed as pragmas. Thus, it is desirable to automate such decisions with the help of machine learning for predicting the quality of HLS designs, requiring a deeper understanding of the program that consists of original code and pragmas. Naturally, these programs can be considered as sequence data. In addition, these programs can be compiled and converted into a control data flow graph (CDFG). But existing works either fail to leverage both modalities or combine the two in shallow or coarse ways. We propose ProgSG, a model that allows interaction between the source code sequence modality and the graph modality in a deep and fine-grained way. To alleviate the scarcity of labeled designs, a pre-training method is proposed based on a suite of compiler's data flow analysis tasks. Experimental results show that ProgSG reduces the RMSE of design performance predictions by up to 22%, and identifies designs with an average of 1.10 and 1.26 (up to 8.17 and 13.31) performance improvement in design space exploration (DSE) task compared to HARP and AutoDSE, respectively.",,,,Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD,,,2024-09-09,2024,2024-09-09,2024-09-09,,,1-12,All OA; Green,Proceeding,"Qin, Zongyue; Bai, Yunsheng; Sohrabizadeh, Atefeh; Ding, Zijian; Hu, Ziniu; Sun, Yizhou; Cong, Jason","Qin, Zongyue (University of California, Los Angeles, USA); Bai, Yunsheng (University of California, Los Angeles, USA); Sohrabizadeh, Atefeh (University of California, Los Angeles, USA); Ding, Zijian (University of California, Los Angeles, USA); Hu, Ziniu (University of California, Los Angeles, USA); Sun, Yizhou (University of California, Los Angeles, USA); Cong, Jason (University of California, Los Angeles, USA)",,"Qin, Zongyue (University of California, Los Angeles); Bai, Yunsheng (University of California, Los Angeles); Sohrabizadeh, Atefeh (University of California, Los Angeles); Ding, Zijian (University of California, Los Angeles); Hu, Ziniu (University of California, Los Angeles); Sun, Yizhou (University of California, Los Angeles); Cong, Jason (University of California, Los Angeles)",0,0,,,https://arxiv.org/pdf/2406.09606,https://app.dimensions.ai/details/publication/pub.1175346851,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",
1268,pub.1158309006,10.48550/arxiv.2305.14019,,,ChipGPT: How far are we from natural language hardware design,"As large language models (LLMs) like ChatGPT exhibited unprecedented machine
intelligence, it also shows great performance in assisting hardware engineers
to realize higher-efficiency logic design via natural language interaction. To
estimate the potential of the hardware design process assisted by LLMs, this
work attempts to demonstrate an automated design environment that explores LLMs
to generate hardware logic designs from natural language specifications. To
realize a more accessible and efficient chip development flow, we present a
scalable four-stage zero-code logic design framework based on LLMs without
retraining or finetuning. At first, the demo, ChipGPT, begins by generating
prompts for the LLM, which then produces initial Verilog programs. Second, an
output manager corrects and optimizes these programs before collecting them
into the final design space. Eventually, ChipGPT will search through this space
to select the optimal design under the target metrics. The evaluation sheds
some light on whether LLMs can generate correct and complete hardware logic
designs described by natural language for some specifications. It is shown that
ChipGPT improves programmability, and controllability, and shows broader design
optimization space compared to prior work and native LLMs alone.",,,arXiv,,,,2023-05-23,2023,,,,,,All OA; Green,Preprint,"Chang, Kaiyan; Wang, Ying; Ren, Haimeng; Wang, Mengdi; Liang, Shengwen; Han, Yinhe; Li, Huawei; Li, Xiaowei","Chang, Kaiyan (); Wang, Ying (); Ren, Haimeng (); Wang, Mengdi (); Liang, Shengwen (); Han, Yinhe (); Li, Huawei (); Li, Xiaowei ()",,"Chang, Kaiyan (); Wang, Ying (); Ren, Haimeng (); Wang, Mengdi (); Liang, Shengwen (); Han, Yinhe (); Li, Huawei (); Li, Xiaowei ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1158309006,33 Built Environment and Design; 3303 Design,
1260,pub.1160754753,10.48550/arxiv.2307.07319,,,The Power of Large Language Models for Wireless Communication System Development: A Case Study on FPGA Platforms,"Large language models (LLMs) have garnered significant attention across
various research disciplines, including the wireless communication community.
There have been several heated discussions on the intersection of LLMs and
wireless technologies. While recent studies have demonstrated the ability of
LLMs to generate hardware description language (HDL) code for simple
computation tasks, developing wireless prototypes and products via HDL poses
far greater challenges because of the more complex computation tasks involved.
In this paper, we aim to address this challenge by investigating the role of
LLMs in FPGA-based hardware development for advanced wireless signal
processing. We begin by exploring LLM-assisted code refactoring, reuse, and
validation, using an open-source software-defined radio (SDR) project as a case
study. Through the case study, we find that an LLM assistant can potentially
yield substantial productivity gains for researchers and developers. We then
examine the feasibility of using LLMs to generate HDL code for advanced
wireless signal processing, using the Fast Fourier Transform (FFT) algorithm as
an example. This task presents two unique challenges: the scheduling of
subtasks within the overall task and the multi-step thinking required to solve
certain arithmetic problem within the task. To address these challenges, we
employ in-context learning (ICL) and Chain-of-Thought (CoT) prompting
techniques, culminating in the successful generation of a 64-point Verilog FFT
module. Our results demonstrate the potential of LLMs for generalization and
imitation, affirming their usefulness in writing HDL code for wireless
communication systems. Overall, this work contributes to understanding the role
of LLMs in wireless communication and motivates further exploration of their
capabilities.",,,arXiv,,,,2023-07-14,2023,,,,,,All OA; Green,Preprint,"Du, Yuyang; Deng, Hongyu; Liew, Soung Chang; Chen, Kexin; Shao, Yulin; Chen, He","Du, Yuyang (); Deng, Hongyu (); Liew, Soung Chang (); Chen, Kexin (); Shao, Yulin (); Chen, He ()",,"Du, Yuyang (); Deng, Hongyu (); Liew, Soung Chang (); Chen, Kexin (); Shao, Yulin (); Chen, He ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1160754753,40 Engineering; 4006 Communications Engineering; 46 Information and Computing Sciences,
1260,pub.1172921079,10.48550/arxiv.2406.09606,,,Cross-Modality Program Representation Learning for Electronic Design Automation with High-Level Synthesis,"In recent years, domain-specific accelerators (DSAs) have gained popularity
for applications such as deep learning and autonomous driving. To facilitate
DSA designs, programmers use high-level synthesis (HLS) to compile a high-level
description written in C/C++ into a design with low-level hardware description
languages that eventually synthesize DSAs on circuits. However, creating a
high-quality HLS design still demands significant domain knowledge,
particularly in microarchitecture decisions expressed as \textit{pragmas}.
Thus, it is desirable to automate such decisions with the help of machine
learning for predicting the quality of HLS designs, requiring a deeper
understanding of the program that consists of original code and pragmas.
Naturally, these programs can be considered as sequence data. In addition,
these programs can be compiled and converted into a control data flow graph
(CDFG). But existing works either fail to leverage both modalities or combine
the two in shallow or coarse ways. We propose ProgSG, a model that allows
interaction between the source code sequence modality and the graph modality in
a deep and fine-grained way. To alleviate the scarcity of labeled designs, a
pre-training method is proposed based on a suite of compiler's data flow
analysis tasks. Experimental results show that ProgSG reduces the RMSE of
design performance predictions by up to $22\%$, and identifies designs with an
average of $1.10\times$ and $1.26\times$ (up to $8.17\times$ and $13.31\times$)
performance improvement in design space exploration (DSE) task compared to HARP
and AutoDSE, respectively.",,,arXiv,,,,2024-06-13,2024,,,,,,All OA; Green,Preprint,"Qin, Zongyue; Bai, Yunsheng; Sohrabizadeh, Atefeh; Ding, Zijian; Hu, Ziniu; Sun, Yizhou; Cong, Jason","Qin, Zongyue (); Bai, Yunsheng (); Sohrabizadeh, Atefeh (); Ding, Zijian (); Hu, Ziniu (); Sun, Yizhou (); Cong, Jason ()",,"Qin, Zongyue (); Bai, Yunsheng (); Sohrabizadeh, Atefeh (); Ding, Zijian (); Hu, Ziniu (); Sun, Yizhou (); Cong, Jason ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1172921079,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",
1250,pub.1172185087,10.48550/arxiv.2405.19213,,,HawkVision: Low-Latency Modeless Edge AI Serving,"The trend of modeless ML inference is increasingly growing in popularity as
it hides the complexity of model inference from users and caters to diverse
user and application accuracy requirements. Previous work mostly focuses on
modeless inference in data centers. To provide low-latency inference, in this
paper, we promote modeless inference at the edge. The edge environment
introduces additional challenges related to low power consumption, limited
device memory, and volatile network environments.
  To address these challenges, we propose HawkVision, which provides
low-latency modeless serving of vision DNNs. HawkVision leverages a two-layer
edge-DC architecture that employs confidence scaling to reduce the number of
model options while meeting diverse accuracy requirements. It also supports
lossy inference under volatile network environments. Our experimental results
show that HawkVision outperforms current serving systems by up to 1.6X in P99
latency for providing modeless service. Our FPGA prototype demonstrates similar
performance at certain accuracy levels with up to a 3.34X reduction in power
consumption.",,,arXiv,,,,2024-05-29,2024,,,,,,All OA; Green,Preprint,"Lao, ChonLam; Gao, Jiaqi; Ananthanarayanan, Ganesh; Akella, Aditya; Yu, Minlan","Lao, ChonLam (); Gao, Jiaqi (); Ananthanarayanan, Ganesh (); Akella, Aditya (); Yu, Minlan ()",,"Lao, ChonLam (); Gao, Jiaqi (); Ananthanarayanan, Ganesh (); Akella, Aditya (); Yu, Minlan ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1172185087,46 Information and Computing Sciences; 4605 Data Management and Data Science,
1250,pub.1166541991,10.1109/iccad57390.2023.10323853,,,Robust GNN-Based Representation Learning for HLS,"The efficient and timely optimization of microarchitecture for a target application is hindered by the long evaluation runtime of a design candidate, creating a serious burden. To tackle this problem, researchers have started using learning algorithms such as graph neural networks (GNNs) to accelerate the process by developing a surrogate of the target tool. However, challenges arise when developing such models for HLS tools due to the program's long dependency range and deeply coupled input program and transformations (i.e., pragmas). To address them, in this paper, we present HARP ($H$ierarchical $A$ugmentation for $R$epresentation with $P$ragma optimization) with a novel hierarchical graph representation of the HLS design by introducing auxiliary nodes to include high-level hierarchical information about the design. Additionally, HARP decouples the representation of the program and its transformations and includes a neural pragma transformer (NPT) approach to facilitate a more systematic treatment of this process. Our proposed graph representation and model architecture of HARP not only enhance the performance of the model and design space exploration based on it but also improve the model's transfer learning capability, enabling easier adaptation to new environments11All materials available at https://github.com/UCLA-VAST/HARP. All materials available at https://github.com/UCLA-VAST/HARP","This work was partially supported by NSF 2211557, NSF 1937599, NSF 2119643, NSF 2303037, NASA, SRC JUMP 2.0 Center, Okawa Foundation, Amazon Research, Cisco, Picsart, Snapchat, and CDSC industrial partners https://cdsc.ucla.edu/partners/). We would also like to thank Marci Baun for editing the paper.","This work was partially supported by NSF 2211557, NSF 1937599, NSF 2119643, NSF 2303037, NASA, SRC JUMP 2.0 Center, Okawa Foundation, Amazon Research, Cisco, Picsart, Snapchat, and CDSC industrial partners https://cdsc.ucla.edu/partners/).",,2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD),,,2023-11-02,2023,,2023-11-02,00,,1-9,Closed,Proceeding,"Sohrabizadeh, Atefeh; Bai, Yunsheng; Sun, Yizhou; Cong, Jason","Sohrabizadeh, Atefeh (Computer Science Department, University of California - Los Angeles, USA); Bai, Yunsheng (Computer Science Department, University of California - Los Angeles, USA); Sun, Yizhou (Computer Science Department, University of California - Los Angeles, USA); Cong, Jason (Computer Science Department, University of California - Los Angeles, USA)",,"Sohrabizadeh, Atefeh (University of California, Los Angeles); Bai, Yunsheng (University of California, Los Angeles); Sun, Yizhou (University of California, Los Angeles); Cong, Jason (University of California, Los Angeles)",9,9,,7.98,,https://app.dimensions.ai/details/publication/pub.1166541991,46 Information and Computing Sciences; 4611 Machine Learning,
1238,pub.1182074105,10.20944/preprints202411.0156.v1,,,"Hardware Design and Verification with Large Language Models: A Literature Survey, Challenges, and Open Issues","Large Language Models (LLMs) are emerging as promising tools in hardware design and verification, with recent advancements suggesting they could fundamentally reshape conventional practices. In this survey, we analyze over 54 research papers to assess the current role of LLMs in enhancing automation, optimization, and innovation within hardware design and verification workflows. Our review highlights LLM applications across synthesis, simulation, and formal verification, emphasizing their potential to streamline development processes while upholding high standards of accuracy and performance. We identify critical challenges, such as scalability, model interpretability, and the alignment of LLMs with domain-specific languages and methodologies. Furthermore, we discuss open issues, including the necessity for tailored model fine-tuning, integration with existing Electronic Design Automation (EDA) tools, and effective handling of complex data structures typical of hardware projects. This survey not only consolidates existing knowledge but also outlines prospective research directions, underscoring the transformative role LLMs could play in the future of hardware design and verification.",,,Preprints.org,,,,2024-11-04,2024,2024-11-04,,,,202411.0156.v1,All OA; Green,Preprint,"Abdollahi, Meisam; Yeganli, S. Faegheh; Baharloo, Mohammad; Baniasadi, Amirali","Abdollahi, Meisam (); Yeganli, S. Faegheh (); Baharloo, Mohammad (); Baniasadi, Amirali ()","Abdollahi, Meisam ()","Abdollahi, Meisam (); Yeganli, S. Faegheh (); Baharloo, Mohammad (); Baniasadi, Amirali ()",0,0,,,https://www.preprints.org/manuscript/202411.0156/v1/download,https://app.dimensions.ai/details/publication/pub.1182074105,40 Engineering; 46 Information and Computing Sciences; 4612 Software Engineering,
1224,pub.1175353034,10.1145/3670474.3685940,,,Learning to Compare Hardware Designs for High-Level Synthesis,"High-level synthesis (HLS) is an automated design process that transforms high-level code into optimized hardware designs, enabling rapid development of efficient hardware accelerators for various applications such as image processing, machine learning, and signal processing. To achieve optimal performance, HLS tools rely on pragmas, which are directives inserted into the source code to guide the synthesis process, and these pragmas can have various settings and values that significantly impact the resulting hardware design. State-of-the-art ML-based HLS methods, such as harp, first train a deep learning model, typically based on graph neural networks (GNNs) applied to graph-based representations of the source code and its pragmas. They then perform design space exploration (DSE) to explore the pragma design space, rank candidate designs using the trained model, and return the top designs as the final designs. However, traditional DSE methods face challenges due to the highly nonlinear relationship between pragma settings and performance metrics, along with complex interactions between pragmas that affect performance in non-obvious ways. To address these challenges, we propose compareXplore, a novel approach that learns to compare hardware designs for effective HLS optimization. compareXplore introduces a hybrid loss function that combines pairwise preference learning with point-wise performance prediction, enabling the model to capture both relative preferences and absolute performance values. Moreover, we introduce a novel Node Difference Attention module that focuses on the most informative differences between designs, enhancing the model's ability to identify critical pragmas impacting performance. compareXplore adopts a two-stage DSE approach, where a pointwise prediction model is used for the initial design pruning, followed by a pairwise comparison stage for precise performance verification. Experimental results demonstrate that compareXplore achieves significant improvements in ranking metrics and generates high-quality HLS results for the selected designs, outperforming the existing state-of-the-art method.",,,,Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD,,,2024-09-09,2024,2024-09-09,2024-09-09,,,1-7,All OA; Green,Proceeding,"Bai, Yunsheng; Sohrabizadeh, Atefeh; Ding, Zijian; Liang, Rongjian; Li, Weikai; Wang, Ding; Ren, Haoxing; Sun, Yizhou; Cong, Jason","Bai, Yunsheng (NVIDIA Corporation and University of California, Los Angeles); Sohrabizadeh, Atefeh (University of California, Los Angeles); Ding, Zijian (University of California, Los Angeles); Liang, Rongjian (NVIDIA Corporation); Li, Weikai (University of California, Los Angeles); Wang, Ding (University of California, Los Angeles); Ren, Haoxing (NVIDIA Corporation); Sun, Yizhou (University of California, Los Angeles); Cong, Jason (University of California, Los Angeles)",,"Bai, Yunsheng (University of California, Los Angeles); Sohrabizadeh, Atefeh (University of California, Los Angeles); Ding, Zijian (University of California, Los Angeles); Liang, Rongjian (Nvidia (United States)); Li, Weikai (University of California, Los Angeles); Wang, Ding (University of California, Los Angeles); Ren, Haoxing (Nvidia (United States)); Sun, Yizhou (University of California, Los Angeles); Cong, Jason (University of California, Los Angeles)",0,0,,,http://arxiv.org/pdf/2409.13138,https://app.dimensions.ai/details/publication/pub.1175353034,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",
1222,pub.1175879363,10.48550/arxiv.2409.13138,,,Learning to Compare Hardware Designs for High-Level Synthesis,"High-level synthesis (HLS) is an automated design process that transforms
high-level code into hardware designs, enabling the rapid development of
hardware accelerators. HLS relies on pragmas, which are directives inserted
into the source code to guide the synthesis process, and pragmas have various
settings and values that significantly impact the resulting hardware design.
State-of-the-art ML-based HLS methods, such as HARP, first train a deep
learning model, typically based on graph neural networks (GNNs) applied to
graph-based representations of the source code and pragmas. They then perform
design space exploration (DSE) to explore the pragma design space, rank
candidate designs using the model, and return the top designs. However,
traditional DSE methods face challenges due to the highly nonlinear
relationship between pragma settings and performance metrics, along with
complex interactions between pragmas that affect performance in non-obvious
ways.
  To address these challenges, we propose compareXplore, a novel approach that
learns to compare hardware designs for effective HLS optimization.
CompareXplore introduces a hybrid loss function that combines pairwise
preference learning with pointwise performance prediction, enabling the model
to capture both relative preferences and absolute performance. Moreover, we
introduce a novel node difference attention module that focuses on the most
informative differences between designs, enabling the model to identify
critical pragmas impacting performance. CompareXplore adopts a two-stage DSE,
where a pointwise prediction model is used for the initial design pruning,
followed by a pairwise comparison stage for precise performance verification.
In extensive experiments, compareXplore achieves significant improvements in
ranking metrics and generates high-quality HLS results for the selected
designs, outperforming the existing SOTA method.",,,arXiv,,,,2024-09-19,2024,,,,,,All OA; Green,Preprint,"Bai, Yunsheng; Sohrabizadeh, Atefeh; Ding, Zijian; Liang, Rongjian; Li, Weikai; Wang, Ding; Ren, Haoxing; Sun, Yizhou; Cong, Jason","Bai, Yunsheng (); Sohrabizadeh, Atefeh (); Ding, Zijian (); Liang, Rongjian (); Li, Weikai (); Wang, Ding (); Ren, Haoxing (); Sun, Yizhou (); Cong, Jason ()",,"Bai, Yunsheng (); Sohrabizadeh, Atefeh (); Ding, Zijian (); Liang, Rongjian (); Li, Weikai (); Wang, Ding (); Ren, Haoxing (); Sun, Yizhou (); Cong, Jason ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1175879363,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences; 4611 Machine Learning",
1212,pub.1171573956,10.48550/arxiv.2405.06067,,,HMT: Hierarchical Memory Transformer for Long Context Language Processing,"Transformer-based large language models (LLM) have been widely used in
language processing applications. However, most of them restrict the context
window that permits the model to attend to every token in the inputs. Previous
works in recurrent models can memorize past tokens to enable unlimited context
and maintain effectiveness. However, they have ""flat"" memory architectures,
which have limitations in selecting and filtering information. Since humans are
good at learning and self-adjustment, we speculate that imitating brain memory
hierarchy is beneficial for model memorization. We propose the Hierarchical
Memory Transformer (HMT), a novel framework that enables and improves models'
long-context processing ability by imitating human memorization behavior.
Leveraging memory-augmented segment-level recurrence, we organize the memory
hierarchy by preserving tokens from early input token segments, passing memory
embeddings along the sequence, and recalling relevant information from history.
Evaluating general language modeling (Wikitext-103, PG-19) and
question-answering tasks (PubMedQA), we show that HMT steadily improves the
long-context processing ability of context-constrained and long-context models.
With an additional 0.5% - 2% of parameters, HMT can easily plug in and augment
future LLMs to handle long context effectively. Our code is open-sourced on
Github: https://github.com/OswaldHe/HMT-pytorch.",,,arXiv,,,,2024-05-09,2024,,,,,,All OA; Green,Preprint,"He, Zifan; Qin, Zongyue; Prakriya, Neha; Sun, Yizhou; Cong, Jason","He, Zifan (); Qin, Zongyue (); Prakriya, Neha (); Sun, Yizhou (); Cong, Jason ()",,"He, Zifan (); Qin, Zongyue (); Prakriya, Neha (); Sun, Yizhou (); Cong, Jason ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1171573956,46 Information and Computing Sciences; 52 Psychology; 5204 Cognitive and Computational Psychology,
1211,pub.1162965402,10.1109/ijcnn54540.2023.10191067,,,A Scalable GPT-2 Inference Hardware Architecture on FPGA,"Transformer-based architectures using attention mechanisms are a class of learning architectures for sequence processing tasks. These include architectures such as the generative pretrained transformer (GPT) and the bidirectional encoder representations from transformers (BERT). GPT-2 is a popular sequence learning architecture that uses transformer architecture. GPT-2 is trained on text prediction, and the network parameters obtained during this training process can be used in various other tasks like text classification and premise-hypothesis testing. Edge computing is an recent trend in which training is done on cloud or server with multiple GPUs, but inference is done on edge devices like mobile phones to reduce latency and improve privacy. This necessitates a study of GPT-2 performance and complexity to distill hardware-based architectures for their usability on edge devices. In this paper, a single layer of GPT-2 based inference architecture is implemented on Virtex-7 xc7vx485tffg1761-2 FPGA board. The inference engine has model dimensionality of 128 and latency of 1.637 ms while operating at 142.44 MHz, consuming 85.6K flip-flops and 96.8K lookup tables, achieving 1.73x speedup compared to previously reported work on transformer-based architecture. The approach proposed in this paper is scalable to models of higher dimensionality.",,"Y. Anil was financially supported by the Ministry of Education, Govt. of India for his Masters.",,2023 International Joint Conference on Neural Networks (IJCNN),,,2023-06-23,2023,,2023-06-23,00,,1-8,Closed,Proceeding,"Yemme, Anil; Garani, Shayan Srinivasa","Yemme, Anil (Dept. of Electronics Systems Engineering, Indian Institute of Science, Bengaluru, India); Garani, Shayan Srinivasa (Dept. of Electronics Systems Engineering, Indian Institute of Science, Bengaluru, India)",,"Yemme, Anil (Indian Institute of Science Bangalore); Garani, Shayan Srinivasa (Indian Institute of Science Bangalore)",1,1,,0.89,,https://app.dimensions.ai/details/publication/pub.1162965402,46 Information and Computing Sciences; 4611 Machine Learning,
1207,pub.1169657731,10.48550/arxiv.2403.07257,,,The Dawn of AI-Native EDA: Opportunities and Challenges of Large Circuit Models,"Within the Electronic Design Automation (EDA) domain, AI-driven solutions
have emerged as formidable tools, yet they typically augment rather than
redefine existing methodologies. These solutions often repurpose deep learning
models from other domains, such as vision, text, and graph analytics, applying
them to circuit design without tailoring to the unique complexities of
electronic circuits. Such an AI4EDA approach falls short of achieving a
holistic design synthesis and understanding, overlooking the intricate
interplay of electrical, logical, and physical facets of circuit data. This
paper argues for a paradigm shift from AI4EDA towards AI-native EDA,
integrating AI at the core of the design process. Pivotal to this vision is the
development of a multimodal circuit representation learning technique, poised
to provide a comprehensive understanding by harmonizing and extracting insights
from varied data sources, such as functional specifications, RTL designs,
circuit netlists, and physical layouts. We champion the creation of large
circuit models (LCMs) that are inherently multimodal, crafted to decode and
express the rich semantics and structures of circuit data, thus fostering more
resilient, efficient, and inventive design methodologies. Embracing this
AI-native philosophy, we foresee a trajectory that transcends the current
innovation plateau in EDA, igniting a profound shift-left in electronic design
methodology. The envisioned advancements herald not just an evolution of
existing EDA tools but a revolution, giving rise to novel instruments of design
tools that promise to radically enhance design productivity and inaugurate a
new epoch where the optimization of circuit performance, power, and area (PPA)
is achieved not incrementally, but through leaps that redefine the benchmarks
of electronic systems' capabilities.",,,arXiv,,,,2024-03-11,2024,,,,,,All OA; Green,Preprint,"Chen, Lei; Chen, Yiqi; Chu, Zhufei; Fang, Wenji; Ho, Tsung-Yi; Huang, Ru; Huang, Yu; Khan, Sadaf; Li, Min; Li, Xingquan; Li, Yu; Liang, Yun; Liu, Jinwei; Liu, Yi; Lin, Yibo; Luo, Guojie; Shi, Zhengyuan; Sun, Guangyu; Tsaras, Dimitrios; Wang, Runsheng; Wang, Ziyi; Wei, Xinming; Xie, Zhiyao; Xu, Qiang; Xue, Chenhao; Yan, Junchi; Yang, Jun; Yu, Bei; Yuan, Mingxuan; Young, Evangeline F. Y.; Zeng, Xuan; Zhang, Haoyi; Zhang, Zuodong; Zhao, Yuxiang; Zhen, Hui-Ling; Zheng, Ziyang; Zhu, Binwu; Zhu, Keren; Zou, Sunan","Chen, Lei (Huawei Noah's Ark Lab); Chen, Yiqi (Peking University); Chu, Zhufei (Ningbo University); Fang, Wenji (Hong Kong University of Science and Technology); Ho, Tsung-Yi (The Chinese University of Hong Kong); Huang, Ru (Peking University; Southeast University); Huang, Yu (Huawei HiSilicon); Khan, Sadaf (The Chinese University of Hong Kong); Li, Min (Huawei Noah's Ark Lab); Li, Xingquan (Peng Cheng Laboratory); Li, Yu (The Chinese University of Hong Kong); Liang, Yun (Peking University); Liu, Jinwei (The Chinese University of Hong Kong); Liu, Yi (The Chinese University of Hong Kong); Lin, Yibo (Peking University); Luo, Guojie (Peking University); Shi, Zhengyuan (The Chinese University of Hong Kong); Sun, Guangyu (Peking University); Tsaras, Dimitrios (Huawei Noah's Ark Lab); Wang, Runsheng (Peking University); Wang, Ziyi (The Chinese University of Hong Kong); Wei, Xinming (Peking University); Xie, Zhiyao (Hong Kong University of Science and Technology); Xu, Qiang (The Chinese University of Hong Kong); Xue, Chenhao (Peking University); Yan, Junchi (Shanghai Jiao Tong University); Yang, Jun (Southeast University); Yu, Bei (The Chinese University of Hong Kong); Yuan, Mingxuan (Huawei Noah's Ark Lab); Young, Evangeline F. Y. (The Chinese University of Hong Kong); Zeng, Xuan (Fudan University); Zhang, Haoyi (Peking University); Zhang, Zuodong (Peking University); Zhao, Yuxiang (Peking University); Zhen, Hui-Ling (Huawei Noah's Ark Lab); Zheng, Ziyang (The Chinese University of Hong Kong); Zhu, Binwu (The Chinese University of Hong Kong); Zhu, Keren (The Chinese University of Hong Kong); Zou, Sunan (Peking University)",,"Chen, Lei (Huawei Noah's Ark Lab); Chen, Yiqi (Peking University); Chu, Zhufei (Ningbo University); Fang, Wenji (Hong Kong University of Science and Technology); Ho, Tsung-Yi (Chinese University of Hong Kong); Huang, Ru (Peking University; Southeast University); Huang, Yu (Huawei HiSilicon); Khan, Sadaf (Chinese University of Hong Kong); Li, Min (Huawei Noah's Ark Lab); Li, Xingquan (Peng Cheng Laboratory); Li, Yu (Chinese University of Hong Kong); Liang, Yun (Peking University); Liu, Jinwei (Chinese University of Hong Kong); Liu, Yi (Chinese University of Hong Kong); Lin, Yibo (Peking University); Luo, Guojie (Peking University); Shi, Zhengyuan (Chinese University of Hong Kong); Sun, Guangyu (Peking University); Tsaras, Dimitrios (Huawei Noah's Ark Lab); Wang, Runsheng (Peking University); Wang, Ziyi (Chinese University of Hong Kong); Wei, Xinming (Peking University); Xie, Zhiyao (Hong Kong University of Science and Technology); Xu, Qiang (Chinese University of Hong Kong); Xue, Chenhao (Peking University); Yan, Junchi (Shanghai Jiao Tong University); Yang, Jun (Southeast University); Yu, Bei (Chinese University of Hong Kong); Yuan, Mingxuan (Huawei Noah's Ark Lab); Young, Evangeline F. Y. (Chinese University of Hong Kong); Zeng, Xuan (Fudan University); Zhang, Haoyi (Peking University); Zhang, Zuodong (Peking University); Zhao, Yuxiang (Peking University); Zhen, Hui-Ling (Huawei Noah's Ark Lab); Zheng, Ziyang (Chinese University of Hong Kong); Zhu, Binwu (Chinese University of Hong Kong); Zhu, Keren (Chinese University of Hong Kong); Zou, Sunan (Peking University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1169657731,"33 Built Environment and Design; 3303 Design; 40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware",
1206,pub.1182136620,10.1109/mlcad62225.2024.10740257,,,Learning to Compare Hardware Designs for High-Level Synthesis,"High-level synthesis (HLS) is an automated design process that transforms high-level code into optimized hardware designs, enabling rapid development of efficient hardware accelerators for various applications such as image processing, machine learning, and signal processing. To achieve optimal performance, HLS tools rely on pragmas, which are directives inserted into the source code to guide the synthesis process, and these pragmas can have various settings and values that significantly impact the resulting hardware design. State-of-the-art ML-based HLS methods, such as HARP, first train a deep learning model, typically based on graph neural networks (GNNs) applied to graph-based representations of the source code and its pragmas. They then perform design space exploration (DSE) to explore the pragma design space, rank candidate designs using the trained model, and return the top designs as the final designs. However, traditional DSE methods face challenges due to the highly nonlinear relationship between pragma settings and performance metrics, along with complex interactions between pragmas that affect performance in non-obvious ways. To address these challenges, we propose compareXplore, a novel approach that learns to compare hardware designs for effective HLS optimization. COMPAREXPLORE introduces a hybrid loss function that combines pairwise preference learning with pointwise performance prediction, enabling the model to capture both relative preferences and absolute performance values. Moreover, we introduce a novel Node Difference Attention module that focuses on the most informative differences between designs, enhancing the model’s ability to identify critical pragmas impacting performance. compareXPLORE adopts a two-stage DSE approach, where a pointwise prediction model is used for the initial design pruning, followed by a pairwise comparison stage for precise performance verification. Experimental results demonstrate that comPAREXPLORE achieves significant improvements in ranking metrics and generates high-quality HLS results for the selected designs, outperforming the existing state-of-the-art method. CCS CONCEPTS • Hardware $\rightarrow$ High-level and register-transfer level synthesis; • Computing methodologies $\rightarrow$ Neural networks.",,,,2024 ACM/IEEE 6th Symposium on Machine Learning for CAD (MLCAD),,,2024-01-11,2024,,2024-01-11,00,,1-7,All OA; Green,Proceeding,"Bai, Yunsheng; Sohrabizadeh, Atefeh; Ding, Zijian; Liang, Rongjian; Li, Weikai; Wang, Ding; Ren, Haoxing; Sun, Yizhou; Cong, Jason","Bai, Yunsheng (NVIDIA Corporation; University of California, Los Angeles); Sohrabizadeh, Atefeh (University of California, Los Angeles); Ding, Zijian (University of California, Los Angeles); Liang, Rongjian (NVIDIA Corporation); Li, Weikai (University of California, Los Angeles); Wang, Ding (University of California, Los Angeles); Ren, Haoxing (NVIDIA Corporation); Sun, Yizhou (University of California, Los Angeles); Cong, Jason (University of California, Los Angeles)","Bai, Yunsheng (Nvidia (United States); University of California, Los Angeles)","Bai, Yunsheng (Nvidia (United States); University of California, Los Angeles); Sohrabizadeh, Atefeh (University of California, Los Angeles); Ding, Zijian (University of California, Los Angeles); Liang, Rongjian (Nvidia (United States)); Li, Weikai (University of California, Los Angeles); Wang, Ding (University of California, Los Angeles); Ren, Haoxing (Nvidia (United States)); Sun, Yizhou (University of California, Los Angeles); Cong, Jason (University of California, Los Angeles)",0,0,,,http://arxiv.org/pdf/2409.13138,https://app.dimensions.ai/details/publication/pub.1182136620,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",
1194,pub.1173894518,10.48550/arxiv.2407.12037,,,A Novel HDL Code Generator for Effectively Testing FPGA Logic Synthesis Compilers,"Field Programmable Gate Array (FPGA) logic synthesis compilers (e.g., Vivado,
Iverilog, Yosys, and Quartus) are widely applied in Electronic Design
Automation (EDA), such as the development of FPGA programs.However, defects
(i.e., incorrect synthesis) in logic synthesis compilers may lead to unexpected
behaviors in target applications, posing security risks. Therefore, it is
crucial to thoroughly test logic synthesis compilers to eliminate such
defects.Despite several Hardware Design Language (HDL) code generators (e.g.,
Verismith) have been proposed to find defects in logic synthesis compilers, the
effectiveness of these generators is still limited by the simple code
generation strategy and the monogeneity of the generated HDL code.This paper
proposes LegoHDL, a novel method to generate syntax valid HDL code for
comprehensively testing FPGA logic synthesis compilers.LegoHDL can generate
more complex and diverse defect-trigger HDL code (e.g., Verilog, VHDL, and
SystemVerilog) by leveraging the guidance of abstract syntax tree and the
extensive function block libraries of cyber-physical systems. Extensive
experiments show that the diversity and defect-trigger capability of HDL code
generated by LegoHDL are significantly better than the state-of-the-art method
(i.e., Verismith).In three months, LegoHDL has reported 20 new defects--many of
which are deep and important; 16 of them have been confirmed.",,,arXiv,,,,2024-07-01,2024,,,,,,All OA; Green,Preprint,"Xu, Zhihao; Guo, Shikai; Zhao, Guilin; Zou, Peiyu; Li, Xiaochen; Jiang, He","Xu, Zhihao (); Guo, Shikai (); Zhao, Guilin (); Zou, Peiyu (); Li, Xiaochen (); Jiang, He ()",,"Xu, Zhihao (); Guo, Shikai (); Zhao, Guilin (); Zou, Peiyu (); Li, Xiaochen (); Jiang, He ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1173894518,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences; 4612 Software Engineering",
1191,pub.1165333200,10.1145/3581784.3607045,,,Co-design Hardware and Algorithm for Vector Search,"Vector search has emerged as the foundation for large-scale information retrieval and machine learning systems, with search engines like Google and Bing processing tens of thousands of queries per second on petabyte-scale document datasets by evaluating vector similarities between encoded query texts and web documents. As performance demands for vector search systems surge, accelerated hardware offers a promising solution in the post-Moore's Law era. We introduce FANNS, an end-to-end and scalable vector search framework on FPGAs. Given a user-provided recall requirement on a dataset and a hardware resource budget, FANNS automatically co-designs hardware and algorithm, subsequently generating the corresponding accelerator. The framework also supports scale-out by incorporating a hardware TCP/IP stack in the accelerator. FANNS attains up to 23.0× and 37.2× speedup compared to FPGA and CPU baselines, respectively, and demonstrates superior scalability to GPUs, achieving 5.5× and 7.6× speedup in median and 95th percentile (P95) latency within an eight-accelerator configuration. The remarkable performance of FANNS lays a robust groundwork for future FPGA integration in data centers and AI supercomputers.",,,,"Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",,,2023-11-11,2023,2023-11-11,2023-11-12,,,1-15,All OA; Green,Proceeding,"Jiang, Wenqi; Li, Shigang; Zhu, Yu; De Fine Licht, Johannes; He, Zhenhao; Shi, Runbin; Renggli, Cedric; Zhang, Shuai; Rekatsinas, Theodoros; Hoefler, Torsten; Alonso, Gustavo","Jiang, Wenqi (ETH Zurich, Zurich, Switzerland); Li, Shigang (Beijing University of Posts and Telecommunications, Bejing, China); Zhu, Yu (ETH Zurich, Zurich, Switzerland); De Fine Licht, Johannes (ETH Zurich, Zurich, Switzerland); He, Zhenhao (ETH Zurich, Zurich, Switzerland); Shi, Runbin (ETH Zurich, Zurich, Switzerland); Renggli, Cedric (Apple, Zurich, Switzerland); Zhang, Shuai (ETH Zurich, Zurich, Switzerland); Rekatsinas, Theodoros (Apple, Zurich, Switzerland); Hoefler, Torsten (ETH Zurich, Zurich, Switzerland); Alonso, Gustavo (ETH Zurich, Zurich, Switzerland)","Jiang, Wenqi (ETH Zurich)","Jiang, Wenqi (ETH Zurich); Li, Shigang (Beijing University of Posts and Telecommunications); Zhu, Yu (ETH Zurich); De Fine Licht, Johannes (ETH Zurich); He, Zhenhao (ETH Zurich); Shi, Runbin (ETH Zurich); Renggli, Cedric (Apple, Zurich, Switzerland); Zhang, Shuai (ETH Zurich); Rekatsinas, Theodoros (Apple, Zurich, Switzerland); Hoefler, Torsten (ETH Zurich); Alonso, Gustavo (ETH Zurich)",3,3,,2.59,https://arxiv.org/pdf/2306.11182,https://app.dimensions.ai/details/publication/pub.1165333200,46 Information and Computing Sciences; 4605 Data Management and Data Science,
1183,pub.1159991545,10.48550/arxiv.2306.11182,,,Co-design Hardware and Algorithm for Vector Search,"Vector search has emerged as the foundation for large-scale information
retrieval and machine learning systems, with search engines like Google and
Bing processing tens of thousands of queries per second on petabyte-scale
document datasets by evaluating vector similarities between encoded query texts
and web documents. As performance demands for vector search systems surge,
accelerated hardware offers a promising solution in the post-Moore's Law era.
We introduce \textit{FANNS}, an end-to-end and scalable vector search framework
on FPGAs. Given a user-provided recall requirement on a dataset and a hardware
resource budget, \textit{FANNS} automatically co-designs hardware and
algorithm, subsequently generating the corresponding accelerator. The framework
also supports scale-out by incorporating a hardware TCP/IP stack in the
accelerator. \textit{FANNS} attains up to 23.0$\times$ and 37.2$\times$ speedup
compared to FPGA and CPU baselines, respectively, and demonstrates superior
scalability to GPUs, achieving 5.5$\times$ and 7.6$\times$ speedup in median
and 95\textsuperscript{th} percentile (P95) latency within an eight-accelerator
configuration. The remarkable performance of \textit{FANNS} lays a robust
groundwork for future FPGA integration in data centers and AI supercomputers.",,,arXiv,,,,2023-06-19,2023,,,,,,All OA; Green,Preprint,"Jiang, Wenqi; Li, Shigang; Zhu, Yu; Licht, Johannes de Fine; He, Zhenhao; Shi, Runbin; Renggli, Cedric; Zhang, Shuai; Rekatsinas, Theodoros; Hoefler, Torsten; Alonso, Gustavo","Jiang, Wenqi (); Li, Shigang (); Zhu, Yu (); Licht, Johannes de Fine (); He, Zhenhao (); Shi, Runbin (); Renggli, Cedric (); Zhang, Shuai (); Rekatsinas, Theodoros (); Hoefler, Torsten (); Alonso, Gustavo ()",,"Jiang, Wenqi (); Li, Shigang (); Zhu, Yu (); Licht, Johannes de Fine (); He, Zhenhao (); Shi, Runbin (); Renggli, Cedric (); Zhang, Shuai (); Rekatsinas, Theodoros (); Hoefler, Torsten (); Alonso, Gustavo ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1159991545,46 Information and Computing Sciences; 4605 Data Management and Data Science,
1182,pub.1172981524,10.48550/arxiv.2406.12385,,,Accelerating Graph-based Vector Search via Delayed-Synchronization Traversal,"Vector search systems are indispensable in large language model (LLM)
serving, search engines, and recommender systems, where minimizing online
search latency is essential. Among various algorithms, graph-based vector
search (GVS) is particularly popular due to its high search performance and
quality. To efficiently serve low-latency GVS, we propose a hardware-algorithm
co-design solution including Falcon, a GVS accelerator, and
Delayed-Synchronization Traversal (DST), an accelerator-optimized graph
traversal algorithm. Falcon implements high-performance GVS operators and
reduces memory accesses with an on-chip Bloom filter to track search states.
DST improves search performance and quality by relaxing the graph traversal
order to maximize accelerator utilization. Evaluation across various graphs and
datasets shows that our Falcon prototype on FPGAs, coupled with DST, achieves
up to 4.3$\times$ and 19.5$\times$ speedups in latency and up to 8.0$\times$
and 26.9$\times$ improvements in energy efficiency over CPU and GPU-based GVS
systems. The remarkable efficiency of Falcon and DST demonstrates their
potential to become the standard solutions for future GVS acceleration.",,,arXiv,,,,2024-06-18,2024,,,,,,All OA; Green,Preprint,"Jiang, Wenqi; Hu, Hang; Hoefler, Torsten; Alonso, Gustavo","Jiang, Wenqi (); Hu, Hang (); Hoefler, Torsten (); Alonso, Gustavo ()",,"Jiang, Wenqi (); Hu, Hang (); Hoefler, Torsten (); Alonso, Gustavo ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1172981524,46 Information and Computing Sciences; 4605 Data Management and Data Science,7 Affordable and Clean Energy
1178,pub.1162883961,10.48550/arxiv.2307.15517,,,A Dataflow Compiler for Efficient LLM Inference using Custom Microscaling Formats,"Model quantization represents both parameters (weights) and intermediate
values (activations) in a more compact format, thereby directly reducing both
computational and memory cost in hardware. The quantization of recent large
language models (LLMs) faces challenges to achieve competitive memory density
compared to other models such as convolutional neural networks, since values in
LLMs require larger dynamic ranges.
  Current hardware can expedite computation for LLMs using compact numerical
formats such as low-bitwidth integers or floating-point numbers. Each has
advantages: integer operations simplify circuit design, whereas floating-point
calculations can enhance accuracy when a wider dynamic range is required. In
this work, we seek an efficient data format that combines the best of both
worlds: Microscaling (MX) formats. MX formats are efficient data formats that
achieve both large dynamic ranges and high memory density.
  In this paper, we propose a compiler named MASE for exploring mixed-precision
MX formats on dataflow hardware accelerators for LLM inference. Our main
contributions are twofold. First, we propose a novel orchestration abstraction
to explore both software and hardware optimizations with new data formats.
Second, MASE achieves LLM inference at an average precision of 4-bits, with
minimal to no accuracy degradation. To our knowledge, MASE represents the first
effort to harness fine-grain multi-precision MX formats in the design of LLM
hardware accelerators. Over a range of LLMs and datasets, MASE achieves an
average improvement of 24% in $\Delta$ accuracy with an overhead of only 3% in
energy efficiency compared to designs using 8-bit fixed-point numbers.",,,arXiv,,,,2023-07-28,2023,,,,,,All OA; Green,Preprint,"Cheng, Jianyi; Zhang, Cheng; Yu, Zhewen; Bouganis, Christos-Savvas; Constantinides, George A.; Zhao, Yiren","Cheng, Jianyi (); Zhang, Cheng (); Yu, Zhewen (); Bouganis, Christos-Savvas (); Constantinides, George A. (); Zhao, Yiren ()",,"Cheng, Jianyi (); Zhang, Cheng (); Yu, Zhewen (); Bouganis, Christos-Savvas (); Constantinides, George A. (); Zhao, Yiren ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1162883961,40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,7 Affordable and Clean Energy
1173,pub.1182090767,10.48550/arxiv.2411.04825,,,VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and Benchmark Models,"Existing text simplification or paraphrase datasets mainly focus on
sentence-level text generation in a general domain. These datasets are
typically developed without using domain knowledge. In this paper, we release a
novel dataset, VTechAGP, which is the first academic-to-general-audience text
paraphrase dataset consisting of 4,938 document-level these and dissertation
academic and general-audience abstract pairs from 8 colleges authored over 25
years. We also propose a novel dynamic soft prompt generative language model,
DSPT5. For training, we leverage a contrastive-generative loss function to
learn the keyword vectors in the dynamic prompt. For inference, we adopt a
crowd-sampling decoding strategy at both semantic and structural levels to
further select the best output candidate. We evaluate DSPT5 and various
state-of-the-art large language models (LLMs) from multiple perspectives.
Results demonstrate that the SOTA LLMs does not provide satisfactory outcomes,
while the lightweight DSPT5 can achieve competitive results. To the best of our
knowledge, we are the first to build a benchmark dataset and solutions for
academic-to-general-audience text paraphrase dataset.",,,arXiv,,,,2024-11-07,2024,,,,,,All OA; Green,Preprint,"Cheng, Ming; Gong, Jiaying; Yuan, Chenhan; Ingram, William A.; Fox, Edward; Eldardiry, Hoda","Cheng, Ming (); Gong, Jiaying (); Yuan, Chenhan (); Ingram, William A. (); Fox, Edward (); Eldardiry, Hoda ()",,"Cheng, Ming (); Gong, Jiaying (); Yuan, Chenhan (); Ingram, William A. (); Fox, Edward (); Eldardiry, Hoda ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1182090767,46 Information and Computing Sciences; 4602 Artificial Intelligence,
1150,pub.1170113517,10.1145/3640115.3640160,,,A Systematic Analysis of Data Diversity in Machine Learning for EDA,"With the exponential growth of Very Large-Scale Integration (VLSI) scale and complexity, traditional algorithms of Electronic Design Automation (EDA) are gradually encountering bottlenecks. The successful application of Machine Learning (ML) in various fields has opened a new path for the development of EDA. The ML model has strong adaptability and generalization ability, which can discover patterns in complex circuit designs, make fast and accurate predictions, and effectively optimize with high quality. Data serves as an important link between ML algorithms and circuit design, and effective data features can improve the assistance of ML models to EDA. This article provides a summary of the front-end of circuit design and selects an ML based EDA application framework with typical data characteristics. It analyzes the data and features in each ML model, lists their predictions or optimizations based on algorithms, and proposes suggestions and considerations for the future development trend of this field. Academic research has demonstrated the enormous potential of ML in EDA applications. Future development requires the integration of stakeholders in the entire design and EDA ecosystem to bring future expansion benefits to the entire industry and society.",,,,Proceedings of the 6th International Conference on Information Technologies and Electrical Engineering,,,2023-11-03,2023,2024-03-26,2023-11-03,,,283-287,Closed,Proceeding,"Ren, Jiachen","Ren, Jiachen (Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, China)",,"Ren, Jiachen (Hong Kong University of Science and Technology)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1170113517,"40 Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",
1148,pub.1174962407,10.48550/arxiv.2408.12173,,,Hardware Acceleration for Knowledge Graph Processing: Challenges & Recent Developments,"Knowledge graphs (KGs) have achieved significant attention in recent years,
particularly in the area of the Semantic Web as well as gaining popularity in
other application domains such as data mining and search engines.
Simultaneously, there has been enormous progress in the development of
different types of heterogeneous hardware, impacting the way KGs are processed.
The aim of this paper is to provide a systematic literature review of knowledge
graph hardware acceleration. For this, we present a classification of the
primary areas in knowledge graph technology that harnesses different hardware
units for accelerating certain knowledge graph functionalities. We then
extensively describe respective works, focusing on how KG related schemes
harness modern hardware accelerators. Based on our review, we identify various
research gaps and future exploratory directions that are anticipated to be of
significant value both for academics and industry practitioners.",,,arXiv,,,,2024-08-22,2024,,,,,,All OA; Green,Preprint,"Besta, Maciej; Gerstenberger, Robert; Iff, Patrick; Sonawane, Pournima; Luna, Juan Gómez; Kanakagiri, Raghavendra; Min, Rui; Kwaśniewski, Grzegorz; Mutlu, Onur; Hoefler, Torsten; Appuswamy, Raja; Mahony, Aidan O","Besta, Maciej (); Gerstenberger, Robert (); Iff, Patrick (); Sonawane, Pournima (); Luna, Juan Gómez (); Kanakagiri, Raghavendra (); Min, Rui (); Kwaśniewski, Grzegorz (); Mutlu, Onur (); Hoefler, Torsten (); Appuswamy, Raja (); Mahony, Aidan O ()",,"Besta, Maciej (); Gerstenberger, Robert (); Iff, Patrick (); Sonawane, Pournima (); Luna, Juan Gómez (); Kanakagiri, Raghavendra (); Min, Rui (); Kwaśniewski, Grzegorz (); Mutlu, Onur (); Hoefler, Torsten (); Appuswamy, Raja (); Mahony, Aidan O ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1174962407,46 Information and Computing Sciences; 4602 Artificial Intelligence; 4605 Data Management and Data Science,
1135,pub.1182550830,10.48550/arxiv.2411.13239,,,Transforming the Hybrid Cloud for Emerging AI Workloads,"This white paper, developed through close collaboration between IBM Research
and UIUC researchers within the IIDAI Institute, envisions transforming hybrid
cloud systems to meet the growing complexity of AI workloads through
innovative, full-stack co-design approaches, emphasizing usability,
manageability, affordability, adaptability, efficiency, and scalability. By
integrating cutting-edge technologies such as generative and agentic AI,
cross-layer automation and optimization, unified control plane, and composable
and adaptive system architecture, the proposed framework addresses critical
challenges in energy efficiency, performance, and cost-effectiveness.
Incorporating quantum computing as it matures will enable quantum-accelerated
simulations for materials science, climate modeling, and other high-impact
domains. Collaborative efforts between academia and industry are central to
this vision, driving advancements in foundation models for material design and
climate solutions, scalable multimodal data processing, and enhanced
physics-based AI emulators for applications like weather forecasting and carbon
sequestration. Research priorities include advancing AI agentic systems, LLM as
an Abstraction (LLMaaA), AI model optimization and unified abstractions across
heterogeneous infrastructure, end-to-end edge-cloud transformation, efficient
programming model, middleware and platform, secure infrastructure,
application-adaptive cloud systems, and new quantum-classical collaborative
workflows. These ideas and solutions encompass both theoretical and practical
research questions, requiring coordinated input and support from the research
community. This joint initiative aims to establish hybrid clouds as secure,
efficient, and sustainable platforms, fostering breakthroughs in AI-driven
applications and scientific discovery across academia, industry, and society.",,,arXiv,,,,2024-11-20,2024,,,,,,All OA; Green,Preprint,"Chen, Deming; Youssef, Alaa; Pendse, Ruchi; Schleife, André; Clark, Bryan K.; Hamann, Hendrik; He, Jingrui; Laino, Teodoro; Varshney, Lav; Wang, Yuxiong; Sil, Avirup; Jabbarvand, Reyhaneh; Xu, Tianyin; Kindratenko, Volodymyr; Costa, Carlos; Adve, Sarita; Mendis, Charith; Zhang, Minjia; Núñez-Corrales, Santiago; Ganti, Raghu; Srivatsa, Mudhakar; Kim, Nam Sung; Torrellas, Josep; Huang, Jian; Seelam, Seetharami; Nahrstedt, Klara; Abdelzaher, Tarek; Eilam, Tamar; Zhao, Huimin; Manica, Matteo; Iyer, Ravishankar; Hirzel, Martin; Adve, Vikram; Marinov, Darko; Franke, Hubertus; Tong, Hanghang; Ainsworth, Elizabeth; Zhao, Han; Vasisht, Deepak; Do, Minh; Oliveira, Fabio; Pacifici, Giovanni; Puri, Ruchir; Nagpurkar, Priya","Chen, Deming (); Youssef, Alaa (); Pendse, Ruchi (); Schleife, André (); Clark, Bryan K. (); Hamann, Hendrik (); He, Jingrui (); Laino, Teodoro (); Varshney, Lav (); Wang, Yuxiong (); Sil, Avirup (); Jabbarvand, Reyhaneh (); Xu, Tianyin (); Kindratenko, Volodymyr (); Costa, Carlos (); Adve, Sarita (); Mendis, Charith (); Zhang, Minjia (); Núñez-Corrales, Santiago (); Ganti, Raghu (); Srivatsa, Mudhakar (); Kim, Nam Sung (); Torrellas, Josep (); Huang, Jian (); Seelam, Seetharami (); Nahrstedt, Klara (); Abdelzaher, Tarek (); Eilam, Tamar (); Zhao, Huimin (); Manica, Matteo (); Iyer, Ravishankar (); Hirzel, Martin (); Adve, Vikram (); Marinov, Darko (); Franke, Hubertus (); Tong, Hanghang (); Ainsworth, Elizabeth (); Zhao, Han (); Vasisht, Deepak (); Do, Minh (); Oliveira, Fabio (); Pacifici, Giovanni (); Puri, Ruchir (); Nagpurkar, Priya ()",,"Chen, Deming (); Youssef, Alaa (); Pendse, Ruchi (); Schleife, André (); Clark, Bryan K. (); Hamann, Hendrik (); He, Jingrui (); Laino, Teodoro (); Varshney, Lav (); Wang, Yuxiong (); Sil, Avirup (); Jabbarvand, Reyhaneh (); Xu, Tianyin (); Kindratenko, Volodymyr (); Costa, Carlos (); Adve, Sarita (); Mendis, Charith (); Zhang, Minjia (); Núñez-Corrales, Santiago (); Ganti, Raghu (); Srivatsa, Mudhakar (); Kim, Nam Sung (); Torrellas, Josep (); Huang, Jian (); Seelam, Seetharami (); Nahrstedt, Klara (); Abdelzaher, Tarek (); Eilam, Tamar (); Zhao, Huimin (); Manica, Matteo (); Iyer, Ravishankar (); Hirzel, Martin (); Adve, Vikram (); Marinov, Darko (); Franke, Hubertus (); Tong, Hanghang (); Ainsworth, Elizabeth (); Zhao, Han (); Vasisht, Deepak (); Do, Minh (); Oliveira, Fabio (); Pacifici, Giovanni (); Puri, Ruchir (); Nagpurkar, Priya ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1182550830,"40 Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences; 4606 Distributed Computing and Systems Software",
1135,pub.1175513463,10.48550/arxiv.2409.05832,,,The Quest to Build Trust Earlier in Digital Design,"The ever-rising complexity of computer systems presents challenges for
maintaining security and trust throughout their lifetime. As hardware forms the
foundation of a secure system, we need tools and techniques that support
computer hardware engineers to improve trust and help them address security
concerns. This paper highlights a vision for tools and techniques to enhance
the security of digital hardware in earlier stages of the digital design
process, especially during design with hardware description languages. We
discuss the challenges that design teams face and explore some recent
literature on understanding, identifying, and mitigating hardware security
weaknesses as early as possible. We highlight the opportunities that emerge
with open-source hardware development and sketch some open questions that guide
ongoing research in this domain.",,,arXiv,,,,2024-09-09,2024,,,,,,All OA; Green,Preprint,"Tan, Benjamin","Tan, Benjamin ()",,"Tan, Benjamin ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1175513463,33 Built Environment and Design; 3303 Design; 46 Information and Computing Sciences; 4604 Cybersecurity and Privacy,
1120,pub.1173478436,10.1109/iscas58744.2024.10558482,,,FLAG: Formula-LLM-Based Auto-Generator for Baseband Hardware,"Traditional hardware auto-generators efficiently assist designers in circuits generation, but still demand high programming skills. Large Language Models (LLMs) allow natural language programming, however, struggle with complex concepts in hardware design. To address this, we propose a formula-LLM-based auto-generator for baseband hardware, called FLAG. FLAG enhances LLMs’ understanding of complex formulas, and decreases the demands for designers’ skills. We showcase the ability of FLAG by generating a polar encoder with code length 8 and parallelism 4. Additionally, a polar encoder with code length 1024 and parallelism 4 is generated by FLAG, achieving comparable performance to state-of-the-art approaches. To the best of our knowledge, this is the first paper on LLM-assisted baseband hardware generation, which demonstrates the potential for large-scale baseband hardware generation using LLMs.","This work was supported in part by National Key R&amp;D Program of China under Grant 2020YFB2205503, in part by NSFC under Grants 62331009, 62122020, and 62001108, in part by the Jiangsu Provincial NSF under Grant BK20211512, in part by the Major Key Project of PCL under Grant PCL2021A01-2, and in part by the Fundamental Research Funds for the Central Universities.","This work was supported in part by National Key R&D Program of China under Grant 2020YFB2205503, in part by NSFC under Grants 62331009, 62122020, and 62001108, in part by the Jiangsu Provincial NSF under Grant BK20211512, in part by the Major Key Project of PCL under Grant PCL2021A01-2, and in part by the Fundamental Research Funds for the Central Universities.",,2024 IEEE International Symposium on Circuits and Systems (ISCAS),,,2024-05-22,2024,,2024-05-22,00,,1-5,Closed,Proceeding,"Mao, Yunwei; You, You; Tan, Xiaosi; Huang, Yongming; You, Xiaohu; Zhang, Chuan","Mao, Yunwei (Lab of Efficient Architectures for Digital-Communication and Signal-Processing (LEADS), National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; Purple Mountain Laboratories, Nanjing, China); You, You (Lab of Efficient Architectures for Digital-Communication and Signal-Processing (LEADS), National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; Purple Mountain Laboratories, Nanjing, China); Tan, Xiaosi (Lab of Efficient Architectures for Digital-Communication and Signal-Processing (LEADS), National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; Purple Mountain Laboratories, Nanjing, China); Huang, Yongming (Lab of Efficient Architectures for Digital-Communication and Signal-Processing (LEADS), National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; Purple Mountain Laboratories, Nanjing, China); You, Xiaohu (Lab of Efficient Architectures for Digital-Communication and Signal-Processing (LEADS), National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; Purple Mountain Laboratories, Nanjing, China); Zhang, Chuan (Lab of Efficient Architectures for Digital-Communication and Signal-Processing (LEADS), National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; Purple Mountain Laboratories, Nanjing, China)","Zhang, Chuan (Southeast University; Purple Mountain Laboratories)","Mao, Yunwei (Southeast University; Purple Mountain Laboratories); You, You (Southeast University; Purple Mountain Laboratories); Tan, Xiaosi (Southeast University; Purple Mountain Laboratories); Huang, Yongming (Southeast University; Purple Mountain Laboratories); You, Xiaohu (Southeast University; Purple Mountain Laboratories); Zhang, Chuan (Southeast University; Purple Mountain Laboratories)",2,2,,,,https://app.dimensions.ai/details/publication/pub.1173478436,40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,
1116,pub.1174065517,10.48550/arxiv.2407.16237,,,OriGen:Enhancing RTL Code Generation with Code-to-Code Augmentation and Self-Reflection,"Recent studies have demonstrated the significant potential of Large Language
Models (LLMs) in generating Register Transfer Level (RTL) code, with notable
advancements showcased by commercial models such as GPT-4 and Claude3-Opus.
However, these proprietary LLMs often raise concerns regarding privacy and
security. While open-source LLMs offer solutions to these concerns, they
typically underperform commercial models in RTL code generation tasks,
primarily due to the scarcity of high-quality open-source RTL datasets. To
address this challenge, we introduce OriGen , a fully open-source framework
that incorporates self-reflection capabilities and a novel dataset augmentation
methodology for generating high-quality, large-scale RTL code. Our approach
employs a code-tocode augmentation technique to enhance the quality of
open-source RTL code datasets. Furthermore, OriGen can rectify syntactic errors
through a self-reflection process that leverages compiler feedback.
Experimental results demonstrate that OriGen significantly outperforms other
open-source alternatives in RTL code generation. It surpasses the previous
best-performing open-source LLM by 12.8% and even exceeds GPT-4 Turbo in the
pass@1 metric on the VerilogEval-Human benchmark. Moreover, OriGen exhibits
superior capabilities in self-reflection and error correction, outperforming
GPT-4 by 19.9% on a benchmark designed to evaluate self-reflection
capabilities.",,,arXiv,,,,2024-07-23,2024,,,,,,All OA; Green,Preprint,"Cui, Fan; Yin, Chenyang; Zhou, Kexing; Xiao, Youwei; Sun, Guangyu; Xu, Qiang; Guo, Qipeng; Song, Demin; Lin, Dahua; Zhang, Xingcheng; Yun; Liang","Cui, Fan (Eric); Yin, Chenyang (Eric); Zhou, Kexing (Eric); Xiao, Youwei (Eric); Sun, Guangyu (Eric); Xu, Qiang (Eric); Guo, Qipeng (Eric); Song, Demin (Eric); Lin, Dahua (Eric); Zhang, Xingcheng (Eric); Yun (Eric); Liang ()",,"Cui, Fan (Eric); Yin, Chenyang (Eric); Zhou, Kexing (Eric); Xiao, Youwei (Eric); Sun, Guangyu (Eric); Xu, Qiang (Eric); Guo, Qipeng (Eric); Song, Demin (Eric); Lin, Dahua (Eric); Zhang, Xingcheng (Eric); Yun (Eric); Liang ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1174065517,"40 Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",
1115,pub.1181194589,10.1109/fpl64840.2024.00044,,,SDA: Low-Bit Stable Diffusion Acceleration on Edge FPGAs,"This paper introduces SDA, the first effort to adapt the expensive stable diffusion (SD) model for edge FPGA deployment. First, we apply quantization-aware training to quantize its weights to 4 -bit and activations to 8 -bit ($W 4 A 8$) with a negligible accuracy loss. Based on that, we propose a high-performance hybrid systolic array (hybridSA) architecture that natively executes convolution and attention operators across varying quantization bit-widths (e.g., $W 4 A 8$ and all 8 -bit $Q K^{T} V$ in attention). To improve computational efficiency, hybridSA integrates diverse DSP packing techniques into hybrid weightstationary and output-stationary dataflows that are optimized for convolution and attention. It also supports flexible dataflow transitions to address the distinct demands of its output sequence by subsequent nonlinear operators. Moreover, we observe that nonlinear operators become the new performance bottleneck after the acceleration of convolution and attention, and offload them onto the FPGA as well. To reduce the latency of each nonlinear operator, we pipeline its own execution at a fine granularity. To minimize the resource utilization of nonlinear operators, we carefully balance their execution with hybridSA in a coarse-grained pipeline. Experimental results demonstrate that our low-bit ($W 4 A 8$) SDA accelerator on the embedded AMDXilinx ZCU102 FPGA achieves a speedup of $97.3 \times$ (which takes about $\mathrm{2 . 1}$ minutes for one SD inference), compared to the original SD-v1.5 model on the ARM Cortex-A53 CPU (which takes about 3.5 hours for one SD inference). Our SDA project is open sourced here: https://github.com/Michaela1224/SDA_code.","This work was supported in part by NSERC Discovery Grant RGPIN-2019-04613, DGECR-2019-00120, Alliance Grant ALLRP-552042-2020; CFI John R. Evans Leaders Fund and BC Knowledge Development Fund; NSF Award CNS1909172 and IIS-2310254.","This work was supported in part by NSERC Discovery Grant RGPIN-2019-04613, DGECR-2019-00120, Alliance Grant ALLRP-552042-2020; CFI John R. Evans Leaders Fund and BC Knowledge Development Fund; NSF Award CNS1909172 and IIS-2310254.",,2024 34th International Conference on Field-Programmable Logic and Applications (FPL),,,2024-01-06,2024,,2024-01-06,00,,264-273,Closed,Proceeding,"Yang, Geng; Xie, Yanyue; Xue, Zhong Jia; Chang, Sung-En; Li, Yanyu; Dong, Peiyan; Lei, Jie; Xie, Weiying; Wang, Yanzhi; Lin, Xue; Fang, Zhenman","Yang, Geng (Xidian University; Simon Fraser University); Xie, Yanyue (Northeastern University); Xue, Zhong Jia (Simon Fraser University); Chang, Sung-En (Northeastern University); Li, Yanyu (Northeastern University); Dong, Peiyan (Northeastern University); Lei, Jie (University of Technology, Sydney); Xie, Weiying (Xidian University); Wang, Yanzhi (Northeastern University); Lin, Xue (Northeastern University); Fang, Zhenman (Simon Fraser University)","Yang, Geng (Xidian University; Simon Fraser University)","Yang, Geng (Xidian University; Simon Fraser University); Xie, Yanyue (Northeastern University); Xue, Zhong Jia (Simon Fraser University); Chang, Sung-En (Northeastern University); Li, Yanyu (Northeastern University); Dong, Peiyan (Northeastern University); Lei, Jie (University of Technology Sydney); Xie, Weiying (Xidian University); Wang, Yanzhi (Northeastern University); Lin, Xue (Northeastern University); Fang, Zhenman (Simon Fraser University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1181194589,40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,7 Affordable and Clean Energy
1112,pub.1172414771,10.1145/3650200.3656622,,,Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision Transformers,"Vision transformers (ViTs) have demonstrated their superior accuracy for computer vision tasks compared to convolutional neural networks (CNNs). However, ViT models are often computation-intensive for efficient deployment on resource-limited edge devices. This work proposes Quasar-ViT, a hardware-oriented quantization-aware architecture search framework for ViTs, to design efficient ViT models for hardware implementation while preserving the accuracy. First, Quasar-ViT trains a supernet using our row-wise flexible mixed-precision quantization scheme, mixed-precision weight entanglement, and supernet layer scaling techniques. Then, it applies an efficient hardware-oriented search algorithm, integrated with hardware latency and resource modeling, to determine a series of optimal subnets from supernet under different inference latency targets. Finally, we propose a series of model-adaptive designs on the FPGA platform to support the architecture search and mitigate the gap between the theoretical computation reduction and the practical inference speedup. Our searched models achieve 101.5, 159.6, and 251.6 frames-per-second (FPS) inference speed on the AMD/Xilinx ZCU102 FPGA with 80.4%, 78.6%, and 74.9% top-1 accuracy, respectively, for the ImageNet dataset, consistently outperforming prior works.",,,,Proceedings of the 38th ACM International Conference on Supercomputing,,,2024-05-30,2024,2024-06-03,2024-05-30,,,324-337,All OA; Hybrid,Proceeding,"Li, Zhengang; Lu, Alec; Xie, Yanyue; Kong, Zhenglun; Sun, Mengshu; Tang, Hao; Xue, Zhong Jia; Dong, Peiyan; Ding, Caiwen; Wang, Yanzhi; Lin, Xue; Fang, Zhenman","Li, Zhengang (Northeastern University, USA); Lu, Alec (Simon Fraser University, Canada); Xie, Yanyue (Northeastern University, United States of America); Kong, Zhenglun (Northeastern University, USA); Sun, Mengshu (Beijing University of Technology, China); Tang, Hao (ETH Zurich, Switzerland); Xue, Zhong Jia (Simon Fraser University, Canada); Dong, Peiyan (Northeastern University, United States of America); Ding, Caiwen (University of Connecticut, United States of America); Wang, Yanzhi (Northeastern University, United States of America); Lin, Xue (Northeastern University, United States of America); Fang, Zhenman (Simon Fraser University, Canada)",,"Li, Zhengang (Northeastern University); Lu, Alec (Simon Fraser University); Xie, Yanyue (Northeastern University); Kong, Zhenglun (Northeastern University); Sun, Mengshu (Beijing University of Technology); Tang, Hao (ETH Zurich); Xue, Zhong Jia (Simon Fraser University); Dong, Peiyan (Northeastern University); Ding, Caiwen (University of Connecticut); Wang, Yanzhi (Northeastern University); Lin, Xue (Northeastern University); Fang, Zhenman (Simon Fraser University)",2,2,,,https://dl.acm.org/doi/pdf/10.1145/3650200.3656622,https://app.dimensions.ai/details/publication/pub.1172414771,46 Information and Computing Sciences; 4611 Machine Learning,
1109,pub.1174137555,10.48550/arxiv.2407.18175,,,Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision Transformers,"Vision transformers (ViTs) have demonstrated their superior accuracy for
computer vision tasks compared to convolutional neural networks (CNNs).
However, ViT models are often computation-intensive for efficient deployment on
resource-limited edge devices. This work proposes Quasar-ViT, a
hardware-oriented quantization-aware architecture search framework for ViTs, to
design efficient ViT models for hardware implementation while preserving the
accuracy. First, Quasar-ViT trains a supernet using our row-wise flexible
mixed-precision quantization scheme, mixed-precision weight entanglement, and
supernet layer scaling techniques. Then, it applies an efficient
hardware-oriented search algorithm, integrated with hardware latency and
resource modeling, to determine a series of optimal subnets from supernet under
different inference latency targets. Finally, we propose a series of
model-adaptive designs on the FPGA platform to support the architecture search
and mitigate the gap between the theoretical computation reduction and the
practical inference speedup. Our searched models achieve 101.5, 159.6, and
251.6 frames-per-second (FPS) inference speed on the AMD/Xilinx ZCU102 FPGA
with 80.4%, 78.6%, and 74.9% top-1 accuracy, respectively, for the ImageNet
dataset, consistently outperforming prior works.",,,arXiv,,,,2024-07-25,2024,,,,,,All OA; Green,Preprint,"Li, Zhengang; Lu, Alec; Xie, Yanyue; Kong, Zhenglun; Sun, Mengshu; Tang, Hao; Xue, Zhong Jia; Dong, Peiyan; Ding, Caiwen; Wang, Yanzhi; Lin, Xue; Fang, Zhenman","Li, Zhengang (); Lu, Alec (); Xie, Yanyue (); Kong, Zhenglun (); Sun, Mengshu (); Tang, Hao (); Xue, Zhong Jia (); Dong, Peiyan (); Ding, Caiwen (); Wang, Yanzhi (); Lin, Xue (); Fang, Zhenman ()",,"Li, Zhengang (); Lu, Alec (); Xie, Yanyue (); Kong, Zhenglun (); Sun, Mengshu (); Tang, Hao (); Xue, Zhong Jia (); Dong, Peiyan (); Ding, Caiwen (); Wang, Yanzhi (); Lin, Xue (); Fang, Zhenman ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1174137555,46 Information and Computing Sciences; 4611 Machine Learning,
1084,pub.1173658037,10.48550/arxiv.2407.06245,,,ORAN-Bench-13K: An Open Source Benchmark for Assessing LLMs in Open Radio Access Networks,"Large Language Models (LLMs) can revolutionize how we deploy and operate Open
Radio Access Networks (O-RAN) by enhancing network analytics, anomaly
detection, and code generation and significantly increasing the efficiency and
reliability of a plethora of O-RAN tasks. In this paper, we present
ORAN-Bench-13K, the first comprehensive benchmark designed to evaluate the
performance of Large Language Models (LLMs) within the context of O-RAN. Our
benchmark consists of 13,952 meticulously curated multiple-choice questions
generated from 116 O-RAN specification documents. We leverage a novel
three-stage LLM framework, and the questions are categorized into three
distinct difficulties to cover a wide spectrum of ORAN-related knowledge. We
thoroughly evaluate the performance of several state-of-the-art LLMs, including
Gemini, Chat-GPT, and Mistral. Additionally, we propose ORANSight, a
Retrieval-Augmented Generation (RAG)-based pipeline that demonstrates superior
performance on ORAN-Bench-13K compared to other tested closed-source models.
Our findings indicate that current popular LLM models are not proficient in
O-RAN, highlighting the need for specialized models. We observed a noticeable
performance improvement when incorporating the RAG-based ORANSight pipeline,
with a Macro Accuracy of 0.784 and a Weighted Accuracy of 0.776, which was on
average 21.55% and 22.59% better than the other tested LLMs.",,,arXiv,,,,2024-07-08,2024,,,,,,All OA; Green,Preprint,"Gajjar, Pranshav; Shah, Vijay K.","Gajjar, Pranshav (); Shah, Vijay K. ()",,"Gajjar, Pranshav (); Shah, Vijay K. ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1173658037,46 Information and Computing Sciences; 4605 Data Management and Data Science,
1069,pub.1146559719,10.48550/arxiv.2203.12852,,,"Graph Neural Networks in Particle Physics: Implementations, Innovations, and Challenges","Many physical systems can be best understood as sets of discrete data with
associated relationships. Where previously these sets of data have been
formulated as series or image data to match the available machine learning
architectures, with the advent of graph neural networks (GNNs), these systems
can be learned natively as graphs. This allows a wide variety of high- and
low-level physical features to be attached to measurements and, by the same
token, a wide variety of HEP tasks to be accomplished by the same GNN
architectures. GNNs have found powerful use-cases in reconstruction, tagging,
generation and end-to-end analysis. With the wide-spread adoption of GNNs in
industry, the HEP community is well-placed to benefit from rapid improvements
in GNN latency and memory usage. However, industry use-cases are not perfectly
aligned with HEP and much work needs to be done to best match unique GNN
capabilities to unique HEP obstacles. We present here a range of these
capabilities, predictions of which are currently being well-adopted in HEP
communities, and which are still immature. We hope to capture the landscape of
graph techniques in machine learning as well as point out the most significant
gaps that are inhibiting potentially large leaps in research.",,,arXiv,,,,2022-03-23,2022,,,,,,All OA; Green,Preprint,"Thais, Savannah; Calafiura, Paolo; Chachamis, Grigorios; DeZoort, Gage; Duarte, Javier; Ganguly, Sanmay; Kagan, Michael; Murnane, Daniel; Neubauer, Mark S.; Terao, Kazuhiro","Thais, Savannah (); Calafiura, Paolo (); Chachamis, Grigorios (); DeZoort, Gage (); Duarte, Javier (); Ganguly, Sanmay (); Kagan, Michael (); Murnane, Daniel (); Neubauer, Mark S. (); Terao, Kazuhiro ()",,"Thais, Savannah (); Calafiura, Paolo (); Chachamis, Grigorios (); DeZoort, Gage (); Duarte, Javier (); Ganguly, Sanmay (); Kagan, Michael (); Murnane, Daniel (); Neubauer, Mark S. (); Terao, Kazuhiro ()",3,3,,1.46,,https://app.dimensions.ai/details/publication/pub.1146559719,46 Information and Computing Sciences; 4611 Machine Learning,
1067,pub.1166427811,10.48550/arxiv.2311.16417,,,Challenges and Opportunities to Enable Large-Scale Computing via Heterogeneous Chiplets,"Fast-evolving artificial intelligence (AI) algorithms such as large language
models have been driving the ever-increasing computing demands in today's data
centers. Heterogeneous computing with domain-specific architectures (DSAs)
brings many opportunities when scaling up and scaling out the computing system.
In particular, heterogeneous chiplet architecture is favored to keep scaling up
and scaling out the system as well as to reduce the design complexity and the
cost stemming from the traditional monolithic chip design. However, how to
interconnect computing resources and orchestrate heterogeneous chiplets is the
key to success. In this paper, we first discuss the diversity and evolving
demands of different AI workloads. We discuss how chiplet brings better cost
efficiency and shorter time to market. Then we discuss the challenges in
establishing chiplet interface standards, packaging, and security issues. We
further discuss the software programming challenges in chiplet systems.",,,arXiv,,,,2023-11-27,2023,,,,,,All OA; Green,Preprint,"Yang, Zhuoping; Ji, Shixin; Chen, Xingzhen; Zhuang, Jinming; Zhang, Weifeng; Jani, Dharmesh; Zhou, Peipei","Yang, Zhuoping (); Ji, Shixin (); Chen, Xingzhen (); Zhuang, Jinming (); Zhang, Weifeng (); Jani, Dharmesh (); Zhou, Peipei ()",,"Yang, Zhuoping (); Ji, Shixin (); Chen, Xingzhen (); Zhuang, Jinming (); Zhang, Weifeng (); Jani, Dharmesh (); Zhou, Peipei ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1166427811,46 Information and Computing Sciences; 4608 Human-Centred Computing,
1066,pub.1170130400,10.1109/asp-dac58780.2024.10473961,,,Challenges and Opportunities to Enable Large-Scale Computing via Heterogeneous Chiplets,"Fast-evolving artificial intelligence (AI) algorithms such as large language models have been driving the ever-increasing computing demands in today's data centers. Heterogeneous computing with domain-specific architectures (DSAs) brings many opportunities when scaling up and scaling out the computing system. In particular, heterogeneous chiplet architecture is favored to keep scaling up and scaling out the system as well as to reduce the design complexity and the cost stemming from the traditional monolithic chip design. However, how to interconnect computing resources and orchestrate heterogeneous chiplets is the key to success. In this paper, we first discuss the diversity and evolving demands of different AI workloads. We discuss how chiplet brings better cost efficiency and shorter time to market. Then we discuss the challenges in establishing chiplet interface standards, packaging, and security issues. We further discuss the software programming challenges in chiplet systems.",,,,2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC),,,2024-01-22,2024,,,,,765-770,All OA; Green,Proceeding,"Yang, Zhuoping; Ji, Shixin; Chen, Xingzhen; Zhuang, Jinming; Zhang, Weifeng; Jani, Dharmesh; Zhou, Peipei","Yang, Zhuoping (University of Pittsburgh); Ji, Shixin (University of Pittsburgh); Chen, Xingzhen (University of Pittsburgh); Zhuang, Jinming (University of Pittsburgh); Zhang, Weifeng (Lightelligence); Jani, Dharmesh (Meta); Zhou, Peipei (University of Pittsburgh)",,"Yang, Zhuoping (University of Pittsburgh); Ji, Shixin (University of Pittsburgh); Chen, Xingzhen (University of Pittsburgh); Zhuang, Jinming (University of Pittsburgh); Zhang, Weifeng (Lightelligence); Jani, Dharmesh (Meta); Zhou, Peipei (University of Pittsburgh)",2,2,,,https://arxiv.org/pdf/2311.16417,https://app.dimensions.ai/details/publication/pub.1170130400,46 Information and Computing Sciences; 4608 Human-Centred Computing,
1028,pub.1175372533,10.48550/arxiv.2409.00492,,,Accurate Compression of Text-to-Image Diffusion Models via Vector Quantization,"Text-to-image diffusion models have emerged as a powerful framework for
high-quality image generation given textual prompts. Their success has driven
the rapid development of production-grade diffusion models that consistently
increase in size and already contain billions of parameters. As a result,
state-of-the-art text-to-image models are becoming less accessible in practice,
especially in resource-limited environments. Post-training quantization (PTQ)
tackles this issue by compressing the pretrained model weights into lower-bit
representations. Recent diffusion quantization techniques primarily rely on
uniform scalar quantization, providing decent performance for the models
compressed to 4 bits. This work demonstrates that more versatile vector
quantization (VQ) may achieve higher compression rates for large-scale
text-to-image diffusion models. Specifically, we tailor vector-based PTQ
methods to recent billion-scale text-to-image models (SDXL and SDXL-Turbo), and
show that the diffusion models of 2B+ parameters compressed to around 3 bits
using VQ exhibit the similar image quality and textual alignment as previous
4-bit compression techniques.",,,arXiv,,,,2024-08-31,2024,,,,,,All OA; Green,Preprint,"Egiazarian, Vage; Kuznedelev, Denis; Voronov, Anton; Svirschevski, Ruslan; Goin, Michael; Pavlov, Daniil; Alistarh, Dan; Baranchuk, Dmitry","Egiazarian, Vage (); Kuznedelev, Denis (); Voronov, Anton (); Svirschevski, Ruslan (); Goin, Michael (); Pavlov, Daniil (); Alistarh, Dan (); Baranchuk, Dmitry ()",,"Egiazarian, Vage (); Kuznedelev, Denis (); Voronov, Anton (); Svirschevski, Ruslan (); Goin, Michael (); Pavlov, Daniil (); Alistarh, Dan (); Baranchuk, Dmitry ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1175372533,40 Engineering; 4006 Communications Engineering; 46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
1022,pub.1181866392,10.48550/arxiv.2410.22561,,,Fuzzerfly Effect: Hardware Fuzzing for Memory Safety,"Hardware-level memory vulnerabilities severely threaten computing systems.
However, hardware patching is inefficient or difficult postfabrication. We
investigate the effectiveness of hardware fuzzing in detecting hardware memory
vulnerabilities and highlight challenges and potential future research
directions to enhance hardware fuzzing for memory safety.",,,arXiv,,,,2024-10-29,2024,,,,,,All OA; Green,Preprint,"Rostami, Mohamadreza; Chen, Chen; Kande, Rahul; Li, Huimin; Rajendran, Jeyavijayan; Sadeghi, Ahmad-Reza","Rostami, Mohamadreza (); Chen, Chen (); Kande, Rahul (); Li, Huimin (); Rajendran, Jeyavijayan (); Sadeghi, Ahmad-Reza ()",,"Rostami, Mohamadreza (); Chen, Chen (); Kande, Rahul (); Li, Huimin (); Rajendran, Jeyavijayan (); Sadeghi, Ahmad-Reza ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1181866392,33 Built Environment and Design; 3301 Architecture,
1017,pub.1168088422,10.1145/3635035.3637285,,,Flexible Systolic Array Platform on Virtual 2-D Multi-FPGA Plane,"Systolic arrays are a promising approach to achieving high-performance processing based on highly parallelized designs in various fields, such as AI and bioinformatics. Many previous studies have devoted considerable effort to exploring efficient circuit designs for specific processing. However, the increasing size of systolic arrays forces us to process increasingly large workloads by dividing them into smaller pieces. Therefore, we propose a systolic array platform based on a two-dimensional FPGA plane in which multiple FPGAs are connected by a virtual network. The systolic array realized by this system can be freely customized in shape and size according to the target. Distributed memory access through off-chip memory on each FPGA board and simple stream processing enable scalable performance. This paper presents a preliminary implementation based on the proposed systolic array platform and its performance evaluation. The evaluation results show that the proposed method improves the processing performance in proportion to the number of FPGAs. The results also show that the proposed platform is highly scalable due to the small circuit area required, and that the processing performance depends on the network bandwidth, which means that recent high-bandwidth FPGA boards can be expected to significantly improve the performance.",,,,Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region,,,2024-01-18,2024,2024-01-19,2024-01-18,,,84-94,All OA; Hybrid,Proceeding,"Ueno, Tomohiro; Del Sozzo, Emanuele; Sano, Kentaro","Ueno, Tomohiro (Center for Computational Science, RIKEN, Japan); Del Sozzo, Emanuele (Center for Computational Science, RIKEN, Japan); Sano, Kentaro (Center for Computational Science, RIKEN, Japan)",,"Ueno, Tomohiro (RIKEN); Del Sozzo, Emanuele (RIKEN); Sano, Kentaro (RIKEN)",0,0,,,https://dl.acm.org/doi/pdf/10.1145/3635035.3637285,https://app.dimensions.ai/details/publication/pub.1168088422,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware",
1015,pub.1171016809,10.48550/arxiv.2404.14294,,,A Survey on Efficient Inference for Large Language Models,"Large Language Models (LLMs) have attracted extensive attention due to their
remarkable performance across various tasks. However, the substantial
computational and memory requirements of LLM inference pose challenges for
deployment in resource-constrained scenarios. Efforts within the field have
been directed towards developing techniques aimed at enhancing the efficiency
of LLM inference. This paper presents a comprehensive survey of the existing
literature on efficient LLM inference. We start by analyzing the primary causes
of the inefficient LLM inference, i.e., the large model size, the
quadratic-complexity attention operation, and the auto-regressive decoding
approach. Then, we introduce a comprehensive taxonomy that organizes the
current literature into data-level, model-level, and system-level optimization.
Moreover, the paper includes comparative experiments on representative methods
within critical sub-fields to provide quantitative insights. Last but not
least, we provide some knowledge summary and discuss future research
directions.",,,arXiv,,,,2024-04-22,2024,,,,,,All OA; Green,Preprint,"Zhou, Zixuan; Ning, Xuefei; Hong, Ke; Fu, Tianyu; Xu, Jiaming; Li, Shiyao; Lou, Yuming; Wang, Luning; Yuan, Zhihang; Li, Xiuhong; Yan, Shengen; Dai, Guohao; Zhang, Xiao-Ping; Dong, Yuhan; Wang, Yu","Zhou, Zixuan (); Ning, Xuefei (); Hong, Ke (); Fu, Tianyu (); Xu, Jiaming (); Li, Shiyao (); Lou, Yuming (); Wang, Luning (); Yuan, Zhihang (); Li, Xiuhong (); Yan, Shengen (); Dai, Guohao (); Zhang, Xiao-Ping (); Dong, Yuhan (); Wang, Yu ()",,"Zhou, Zixuan (); Ning, Xuefei (); Hong, Ke (); Fu, Tianyu (); Xu, Jiaming (); Li, Shiyao (); Lou, Yuming (); Wang, Luning (); Yuan, Zhihang (); Li, Xiuhong (); Yan, Shengen (); Dai, Guohao (); Zhang, Xiao-Ping (); Dong, Yuhan (); Wang, Yu ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1171016809,46 Information and Computing Sciences; 4602 Artificial Intelligence,
1011,pub.1171273532,10.48550/arxiv.2405.00645,,,Gradient-based Automatic Mixed Precision Quantization for Neural Networks On-Chip,"Model size and inference speed at deployment time, are major challenges in
many deep learning applications. A promising strategy to overcome these
challenges is quantization. However, a straightforward uniform quantization to
very low precision can result in significant accuracy loss. Mixed-precision
quantization, based on the idea that certain parts of the network can
accommodate lower precision without compromising performance compared to other
parts, offers a potential solution. In this work, we present High Granularity
Quantization (HGQ), an innovative quantization-aware training method that could
fine-tune the per-weight and per-activation precision by making them
optimizable through gradient descent. This approach enables ultra-low latency
and low power neural networks on hardware capable of performing arithmetic
operations with an arbitrary number of bits, such as FPGAs and ASICs. We
demonstrate that HGQ can outperform existing methods by a substantial margin,
achieving resource reduction by up to a factor of 20 and latency improvement by
a factor of 5 while preserving accuracy.",,,arXiv,,,,2024-05-01,2024,,,,,,All OA; Green,Preprint,"Sun, Chang; Årrestad, Thea K.; Loncar, Vladimir; Ngadiuba, Jennifer; Spiropulu, Maria","Sun, Chang (); Årrestad, Thea K. (); Loncar, Vladimir (); Ngadiuba, Jennifer (); Spiropulu, Maria ()",,"Sun, Chang (); Årrestad, Thea K. (); Loncar, Vladimir (); Ngadiuba, Jennifer (); Spiropulu, Maria ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1171273532,46 Information and Computing Sciences; 4611 Machine Learning,
985,pub.1170324959,10.48550/arxiv.2404.00057,,,PerOS: Personalized Self-Adapting Operating Systems in the Cloud,"Operating systems (OSes) are foundational to computer systems, managing
hardware resources and ensuring secure environments for diverse applications.
However, despite their enduring importance, the fundamental design objectives
of OSes have seen minimal evolution over decades. Traditionally prioritizing
aspects like speed, memory efficiency, security, and scalability, these
objectives often overlook the crucial aspect of intelligence as well as
personalized user experience. The lack of intelligence becomes increasingly
critical amid technological revolutions, such as the remarkable advancements in
machine learning (ML).
  Today's personal devices, evolving into intimate companions for users, pose
unique challenges for traditional OSes like Linux and iOS, especially with the
emergence of specialized hardware featuring heterogeneous components.
Furthermore, the rise of large language models (LLMs) in ML has introduced
transformative capabilities, reshaping user interactions and software
development paradigms.
  While existing literature predominantly focuses on leveraging ML methods for
system optimization or accelerating ML workloads, there is a significant gap in
addressing personalized user experiences at the OS level. To tackle this
challenge, this work proposes PerOS, a personalized OS ingrained with LLM
capabilities. PerOS aims to provide tailored user experiences while
safeguarding privacy and personal data through declarative interfaces,
self-adaptive kernels, and secure data management in a scalable cloud-centric
architecture; therein lies the main research question of this work: How can we
develop intelligent, secure, and scalable OSes that deliver personalized
experiences to thousands of users?",,,arXiv,,,,2024-03-26,2024,,,,,,All OA; Green,Preprint,"Hè, Hongyu","Hè, Hongyu ()",,"Hè, Hongyu ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1170324959,46 Information and Computing Sciences; 4604 Cybersecurity and Privacy; 4608 Human-Centred Computing,3 Good Health and Well Being
975,pub.1173861789,10.48550/arxiv.2407.10424,,,CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization,"The increasing complexity and high costs associated with modern processor
design have led to a surge in demand for processor design automation.
Instruction-tuned large language models (LLMs) have demonstrated remarkable
performance in automatically generating code for general-purpose programming
languages like Python. However, these methods fail on hardware description
languages (HDLs) like Verilog due to the scarcity of high-quality instruction
tuning data, as even advanced LLMs like GPT-3.5 exhibit limited performance on
Verilog generation. Regarding this issue, we observe that (1) Verilog code
collected from the real world has higher quality than those generated by LLMs.
(2) LLMs like GPT-3.5 excel in summarizing Verilog code rather than generating
it. Based on these observations, this paper introduces CodeV, a series of
open-source instruction-tuned Verilog generation LLMs. Instead of generating
descriptions first and then getting the corresponding code from advanced LLMs,
we prompt the LLM with Verilog code and let the LLM generate the corresponding
natural language description by multi-level summarization. Experimental results
show that CodeV relatively surpasses the previous open-source SOTA by 14.4%
(BetterV in VerilogEval) and 11.3% (RTLCoder in RTLLM) respectively, and also
relatively outperforms previous commercial SOTA GPT-4 by 22.1% in VerilogEval.",,,arXiv,,,,2024-07-14,2024,,,,,,All OA; Green,Preprint,"Zhao, Yang; Huang, Di; Li, Chongxiao; Jin, Pengwei; Nan, Ziyuan; Ma, Tianyun; Qi, Lei; Pan, Yansong; Zhang, Zhenxing; Zhang, Rui; Zhang, Xishan; Du, Zidong; Guo, Qi; Hu, Xing; Chen, Yunji","Zhao, Yang (); Huang, Di (); Li, Chongxiao (); Jin, Pengwei (); Nan, Ziyuan (); Ma, Tianyun (); Qi, Lei (); Pan, Yansong (); Zhang, Zhenxing (); Zhang, Rui (); Zhang, Xishan (); Du, Zidong (); Guo, Qi (); Hu, Xing (); Chen, Yunji ()",,"Zhao, Yang (); Huang, Di (); Li, Chongxiao (); Jin, Pengwei (); Nan, Ziyuan (); Ma, Tianyun (); Qi, Lei (); Pan, Yansong (); Zhang, Zhenxing (); Zhang, Rui (); Zhang, Xishan (); Du, Zidong (); Guo, Qi (); Hu, Xing (); Chen, Yunji ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1173861789,40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,
967,pub.1164827596,10.48550/arxiv.2310.06257,,,SCAR: Power Side-Channel Analysis at RTL-Level,"Power side-channel attacks exploit the dynamic power consumption of
cryptographic operations to leak sensitive information of encryption hardware.
Therefore, it is necessary to conduct power side-channel analysis for assessing
the susceptibility of cryptographic systems and mitigating potential risks.
Existing power side-channel analysis primarily focuses on post-silicon
implementations, which are inflexible in addressing design flaws, leading to
costly and time-consuming post-fabrication design re-spins. Hence, pre-silicon
power side-channel analysis is required for early detection of vulnerabilities
to improve design robustness. In this paper, we introduce SCAR, a novel
pre-silicon power side-channel analysis framework based on Graph Neural
Networks (GNN). SCAR converts register-transfer level (RTL) designs of
encryption hardware into control-data flow graphs and use that to detect the
design modules susceptible to side-channel leakage. Furthermore, we incorporate
a deep learning-based explainer in SCAR to generate quantifiable and
human-accessible explanation of our detection and localization decisions. We
have also developed a fortification component as a part of SCAR that uses
large-language models (LLM) to automatically generate and insert additional
design code at the localized zone to shore up the side-channel leakage. When
evaluated on popular encryption algorithms like AES, RSA, and PRESENT, and
postquantum cryptography algorithms like Saber and CRYSTALS-Kyber, SCAR,
achieves up to 94.49% localization accuracy, 100% precision, and 90.48% recall.
Additionally, through explainability analysis, SCAR reduces features for GNN
model training by 57% while maintaining comparable accuracy. We believe that
SCAR will transform the security-critical hardware design cycle, resulting in
faster design closure at a reduced design cost.",,,arXiv,,,,2023-10-09,2023,,,,,,All OA; Green,Preprint,"Srivastava, Amisha; Das, Sanjay; Choudhury, Navnil; Psiakis, Rafail; Silva, Pedro Henrique; Pal, Debjit; Basu, Kanad","Srivastava, Amisha (); Das, Sanjay (); Choudhury, Navnil (); Psiakis, Rafail (); Silva, Pedro Henrique (); Pal, Debjit (); Basu, Kanad ()",,"Srivastava, Amisha (); Das, Sanjay (); Choudhury, Navnil (); Psiakis, Rafail (); Silva, Pedro Henrique (); Pal, Debjit (); Basu, Kanad ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1164827596,"40 Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences; 4604 Cybersecurity and Privacy",
965,pub.1167861231,10.48550/arxiv.2401.05459,,,"Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security","Since the advent of personal computing devices, intelligent personal
assistants (IPAs) have been one of the key technologies that researchers and
engineers have focused on, aiming to help users efficiently obtain information
and execute tasks, and provide users with more intelligent, convenient, and
rich interaction experiences. With the development of smartphones and IoT,
computing and sensing devices have become ubiquitous, greatly expanding the
boundaries of IPAs. However, due to the lack of capabilities such as user
intent understanding, task planning, tool using, and personal data management
etc., existing IPAs still have limited practicality and scalability. Recently,
the emergence of foundation models, represented by large language models
(LLMs), brings new opportunities for the development of IPAs. With the powerful
semantic understanding and reasoning capabilities, LLM can enable intelligent
agents to solve complex problems autonomously. In this paper, we focus on
Personal LLM Agents, which are LLM-based agents that are deeply integrated with
personal data and personal devices and used for personal assistance. We
envision that Personal LLM Agents will become a major software paradigm for
end-users in the upcoming era. To realize this vision, we take the first step
to discuss several important questions about Personal LLM Agents, including
their architecture, capability, efficiency and security. We start by
summarizing the key components and design choices in the architecture of
Personal LLM Agents, followed by an in-depth analysis of the opinions collected
from domain experts. Next, we discuss several key challenges to achieve
intelligent, efficient and secure Personal LLM Agents, followed by a
comprehensive survey of representative solutions to address these challenges.",,,arXiv,,,,2024-01-10,2024,,,,,,All OA; Green,Preprint,"Li, Yuanchun; Wen, Hao; Wang, Weijun; Li, Xiangyu; Yuan, Yizhen; Liu, Guohong; Liu, Jiacheng; Xu, Wenxing; Wang, Xiang; Sun, Yi; Kong, Rui; Wang, Yile; Geng, Hanfei; Luan, Jian; Jin, Xuefeng; Ye, Zilong; Xiong, Guanjing; Zhang, Fan; Li, Xiang; Xu, Mengwei; Li, Zhijun; Li, Peng; Liu, Yang; Zhang, Ya-Qin; Liu, Yunxin","Li, Yuanchun (); Wen, Hao (); Wang, Weijun (); Li, Xiangyu (); Yuan, Yizhen (); Liu, Guohong (); Liu, Jiacheng (); Xu, Wenxing (); Wang, Xiang (); Sun, Yi (); Kong, Rui (); Wang, Yile (); Geng, Hanfei (); Luan, Jian (); Jin, Xuefeng (); Ye, Zilong (); Xiong, Guanjing (); Zhang, Fan (); Li, Xiang (); Xu, Mengwei (); Li, Zhijun (); Li, Peng (); Liu, Yang (); Zhang, Ya-Qin (); Liu, Yunxin ()",,"Li, Yuanchun (); Wen, Hao (); Wang, Weijun (); Li, Xiangyu (); Yuan, Yizhen (); Liu, Guohong (); Liu, Jiacheng (); Xu, Wenxing (); Wang, Xiang (); Sun, Yi (); Kong, Rui (); Wang, Yile (); Geng, Hanfei (); Luan, Jian (); Jin, Xuefeng (); Ye, Zilong (); Xiong, Guanjing (); Zhang, Fan (); Li, Xiang (); Xu, Mengwei (); Li, Zhijun (); Li, Peng (); Liu, Yang (); Zhang, Ya-Qin (); Liu, Yunxin ()",1,1,,,,https://app.dimensions.ai/details/publication/pub.1167861231,46 Information and Computing Sciences; 4602 Artificial Intelligence; 4605 Data Management and Data Science; 4608 Human-Centred Computing,
954,pub.1168273281,10.48550/arxiv.2401.14117,,,Evaluation of POSIT Arithmetic with Accelerators,"We present an evaluation of 32-bit POSIT arithmetic through its
implementation as accelerators on FPGAs and GPUs. POSIT, a floating-point
number format, adaptively changes the size of its fractional part. We developed
hardware designs for FPGAs and software for GPUs to accelerate linear algebra
operations using Posit(32,2) arithmetic. Our FPGA- and GPU-based accelerators
in Posit(32,2) arithmetic significantly accelerated the Cholesky and LU
decomposition algorithms for dense matrices. In terms of numerical accuracy,
Posit(32,2) arithmetic is approximately 0.5 - 1.0 digits more accurate than the
standard 32-bit format, especially when the norm of the elements of the input
matrix is close to 1. Evaluating power consumption, we observed that the power
efficiency of the accelerators ranged between 0.043 - 0.076 Gflops/watts for
the LU decomposition in Posit(32,2) arithmetic. The power efficiency of the
latest GPUs as accelerators of Posit(32,2) arithmetic is better than that of
the evaluated FPGA chip.",,,arXiv,,,,2024-01-25,2024,,,,,,All OA; Green,Preprint,"Nakasato, Naohito; Murakami, Yuki; Kono, Fumiya; Nakata, Maho","Nakasato, Naohito (); Murakami, Yuki (); Kono, Fumiya (); Nakata, Maho ()",,"Nakasato, Naohito (); Murakami, Yuki (); Kono, Fumiya (); Nakata, Maho ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1168273281,40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,7 Affordable and Clean Energy
933,pub.1163293420,10.48550/arxiv.2308.06932,,,DIVAS: An LLM-based End-to-End Framework for SoC Security Analysis and Policy-based Protection,"Securing critical assets in a bus-based System-On-Chip (SoC) is imperative to
mitigate potential vulnerabilities and prevent unauthorized access, ensuring
the integrity, availability, and confidentiality of the system. Ensuring
security throughout the SoC design process is a formidable task owing to the
inherent intricacies in SoC designs and the dispersion of assets across diverse
IPs. Large Language Models (LLMs), exemplified by ChatGPT (OpenAI) and BARD
(Google), have showcased remarkable proficiency across various domains,
including security vulnerability detection and prevention in SoC designs. In
this work, we propose DIVAS, a novel framework that leverages the knowledge
base of LLMs to identify security vulnerabilities from user-defined SoC
specifications, map them to the relevant Common Weakness Enumerations (CWEs),
followed by the generation of equivalent assertions, and employ security
measures through enforcement of security policies. The proposed framework is
implemented using multiple ChatGPT and BARD models, and their performance was
analyzed while generating relevant CWEs from the SoC specifications provided.
The experimental results obtained from open-source SoC benchmarks demonstrate
the efficacy of our proposed framework.",,,arXiv,,,,2023-08-14,2023,,,,,,All OA; Green,Preprint,"Paria, Sudipta; Dasgupta, Aritra; Bhunia, Swarup","Paria, Sudipta (); Dasgupta, Aritra (); Bhunia, Swarup ()",,"Paria, Sudipta (); Dasgupta, Aritra (); Bhunia, Swarup ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1163293420,46 Information and Computing Sciences; 4604 Cybersecurity and Privacy,
927,pub.1168091809,10.1145/3635035.3635046,,,Evaluation of POSIT Arithmetic with Accelerators,"We present an evaluation of 32-bit POSIT arithmetic through its implementation as accelerators on FPGAs and GPUs. POSIT, a floating-point number format, adaptively changes the size of its fractional part. We developed hardware designs for FPGAs and software for GPUs to accelerate linear algebra operations using Posit(32,2) arithmetic. Our FPGA- and GPU-based accelerators in Posit(32,2) arithmetic significantly accelerated the Cholesky and LU decomposition algorithms for dense matrices. In terms of numerical accuracy, Posit(32,2) arithmetic is approximately 0.5 - 1.0 digits more accurate than the standard 32-bit format, especially when the norm of the elements of the input matrix is close to 1. Evaluating power consumption, we observed that the power efficiency of the accelerators ranged between 0.043 - 0.076 Gflops/watts for the LU decomposition in Posit(32,2) arithmetic. The power efficiency of the latest GPUs as accelerators of Posit(32,2) arithmetic is better than that of the evaluated FPGA chip.",,,,Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region,,,2024-01-18,2024,2024-01-19,2024-01-18,,,62-72,All OA; Green,Proceeding,"Nakasato, Naohito; Murakami, Yuki; Kono, Fumiya; Nakata, Maho","Nakasato, Naohito (University of Aizu, Japan); Murakami, Yuki (National Institute of Information and Communications Technology, Japan); Kono, Fumiya (Shizuoka Institute of Science and Technology, Japan); Nakata, Maho (Institute of Physical and Chemical Research (Riken), Japan)",,"Nakasato, Naohito (University of Aizu); Murakami, Yuki (National Institute of Information and Communications Technology); Kono, Fumiya (Shizuoka Institute of Science and Technology); Nakata, Maho (RIKEN)",1,1,,,https://arxiv.org/pdf/2401.14117,https://app.dimensions.ai/details/publication/pub.1168091809,40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,7 Affordable and Clean Energy
925,pub.1144866149,10.1109/icemi52946.2021.9679613,,,Connection Resource Testing Technology on FPGA Based on SOC core,"For FPGA testing, usually hundreds of code streams need to be loaded. Because SoC FPGA is embedded with processor system, its code stream loading method is very different from traditional FPGA. This article provides a configuration code selection interface for ATE by designing a startup program, and makes PS reconfigure PL according to the user's choice, and load a different configuration code for each test. The deterministic wiring test method can fully cover the FPGA wiring resources. This paper proposes a deterministic wiring method based on Zynq-7000 series chips to traverse the wiring resources of the SoC FPGA and generate configuration codes. Finally, the wiring resource test of SoC FPGA is realized through ATE and the above code stream loading scheme.","This work was supported in part by the Fundamental Research Funds for the Central Universities under Grant Z YGX2019J055, and the National Key R&amp;D Program of China under Grant SQ2017YFF0106801. This work was supported in part by the Fundamental Research Funds for the Central Universities under Grant Z YGX2019J055, and the National Key R&amp;D Program of China under Grant SQ2017YFF0106801.",,,2021 IEEE 15th International Conference on Electronic Measurement & Instruments (ICEMI),,,2021-10-31,2021,,2021-10-31,00,,383-390,Closed,Proceeding,"Xie, Weikun; Nan, Ziyuan; Gao, Shi","Xie, Weikun (School of Automation engineering, University of Electronic Science and Technology of China, Chengdu, China); Nan, Ziyuan (School of Automation engineering, University of Electronic Science and Technology of China, Chengdu, China); Gao, Shi (School of Automation engineering, University of Electronic Science and Technology of China, Chengdu, China)",,"Xie, Weikun (University of Electronic Science and Technology of China); Nan, Ziyuan (University of Electronic Science and Technology of China); Gao, Shi (University of Electronic Science and Technology of China)",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1144866149,46 Information and Computing Sciences; 4601 Applied Computing,
923,pub.1168184509,10.48550/arxiv.2401.12230,,,Computing in the Era of Large Generative Models: From Cloud-Native to AI-Native,"In this paper, we investigate the intersection of large generative AI models
and cloud-native computing architectures. Recent large models such as ChatGPT,
while revolutionary in their capabilities, face challenges like escalating
costs and demand for high-end GPUs. Drawing analogies between
large-model-as-a-service (LMaaS) and cloud database-as-a-service (DBaaS), we
describe an AI-native computing paradigm that harnesses the power of both
cloud-native technologies (e.g., multi-tenancy and serverless computing) and
advanced machine learning runtime (e.g., batched LoRA inference). These joint
efforts aim to optimize costs-of-goods-sold (COGS) and improve resource
accessibility. The journey of merging these two domains is just at the
beginning and we hope to stimulate future research and development in this
area.",,,arXiv,,,,2024-01-17,2024,,,,,,All OA; Green,Preprint,"Lu, Yao; Bian, Song; Chen, Lequn; He, Yongjun; Hui, Yulong; Lentz, Matthew; Li, Beibin; Liu, Fei; Li, Jialin; Liu, Qi; Liu, Rui; Liu, Xiaoxuan; Ma, Lin; Rong, Kexin; Wang, Jianguo; Wu, Yingjun; Wu, Yongji; Zhang, Huanchen; Zhang, Minjia; Zhang, Qizhen; Zhou, Tianyi; Zhuo, Danyang","Lu, Yao (); Bian, Song (); Chen, Lequn (); He, Yongjun (); Hui, Yulong (); Lentz, Matthew (); Li, Beibin (); Liu, Fei (); Li, Jialin (); Liu, Qi (); Liu, Rui (); Liu, Xiaoxuan (); Ma, Lin (); Rong, Kexin (); Wang, Jianguo (); Wu, Yingjun (); Wu, Yongji (); Zhang, Huanchen (); Zhang, Minjia (); Zhang, Qizhen (); Zhou, Tianyi (); Zhuo, Danyang ()",,"Lu, Yao (); Bian, Song (); Chen, Lequn (); He, Yongjun (); Hui, Yulong (); Lentz, Matthew (); Li, Beibin (); Liu, Fei (); Li, Jialin (); Liu, Qi (); Liu, Rui (); Liu, Xiaoxuan (); Ma, Lin (); Rong, Kexin (); Wang, Jianguo (); Wu, Yingjun (); Wu, Yongji (); Zhang, Huanchen (); Zhang, Minjia (); Zhang, Qizhen (); Zhou, Tianyi (); Zhuo, Danyang ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1168184509,46 Information and Computing Sciences; 4606 Distributed Computing and Systems Software,
907,pub.1122786030,10.23919/eusipco.2019.8902999,,,Low Complexity Robust Adaptive Beamformer Based On Parallel RLMS and Kalman RLMS,"To ease spectral congestion and enhance frequency reuse, researchers are targeting smart antenna systems using spatial multiplexing and adaptive signal processing techniques. Moreover, the accuracy and efficiency of such systems is highly dependent on the adaptive algorithms they employ. A popular, adaptive beamforming algorithm, widely used in smart antennas, is the Recursive Least Square (RLS) algorithm. While, the classical RLS implementation achieves high convergence, it still suffers from its inability to track the target of interest. Recently, a new adaptive algorithm called Recursive Least Square - Least Mean Square (RLMS) which employs a RLS stage followed by a Least Mean Square (LMS) algorithm stage and separated by an estimate of the array image vector, i.e. steering vector, has been proposed. RLMS outperforms previous RLS and LMS variants, with superior convergence and tracking capabilities, at the cost of a moderate increase in computational complexity. In this paper, an enhanced, low complexity parallel version of the cascade RLMS is presented by eliminating the need for computing the array image vector cascading stage. Hence, For an antenna of N elements our strategy can reduce the complexity of the system by 20N multiplications, 6N additions and 2N divisions. Moreover, a new Kalman based parallel RLMS (RKLMS) method is also proposed, where the LMS stage is replaced by a Kalman implementation of the classical LMS, and compared under low Signal to Interference plus Noise ratios (SINR). Simulation results show identical performance for the parallel RLMS, cascaded RLMS at 10dB and superior performance and robustness for the RKLMS on low SINR cases up to -10dB.",,,,2019 27th European Signal Processing Conference (EUSIPCO),,,2019-01-06,2019,,2019-01-06,00,,1-5,All OA; Green,Proceeding,"Akkad, Ghattas; Mansour, Ali; ElHassan, Bachar A.; Srar, Jalal; Najem, Mohamad; Le Roy, Frédéric","Akkad, Ghattas (Lab-STICC, UMR 6285, ENSTA Bretagne, Brest, France); Mansour, Ali (Lab-STICC, UMR 6285, ENSTA Bretagne, Brest, France); ElHassan, Bachar A. (Faculty of Engineering, Lebanese University, Tripoli, Lebanon); Srar, Jalal (Electrical and Electronic Department, Misratah University, Misratah, Libya); Najem, Mohamad (CCE, Lebanese International University, Mount Lebanon, Lebanon); Le Roy, Frédéric (Lab-STICC, UMR 6285, ENSTA Bretagne, Brest, France)","Akkad, Ghattas (Lab-STICC, UMR 6285, ENSTA Bretagne, Brest, France)","Akkad, Ghattas (Lab-STICC, UMR 6285, ENSTA Bretagne, Brest, France); Mansour, Ali (Lab-STICC, UMR 6285, ENSTA Bretagne, Brest, France); ElHassan, Bachar A. (Lebanese University); Srar, Jalal (Electrical and Electronic Department, Misratah University, Misratah, Libya); Najem, Mohamad (Lebanese International University); Le Roy, Frédéric (Lab-STICC, UMR 6285, ENSTA Bretagne, Brest, France)",13,4,,3.7,https://hal.archives-ouvertes.fr/hal-02434614/file/Akkad2019.pdf,https://app.dimensions.ai/details/publication/pub.1122786030,40 Engineering; 4006 Communications Engineering; 46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
896,pub.1174625683,10.1109/iseda62518.2024.10617475,,,SATGL: An Open-Source Graph Learning Toolkit for Boolean Satisfiability,"As the first proven NP-complete problem, the Boolean Satisfiability (SAT) problem holds significant theoretical value and has wide-ranging practical applications. It has also led to the development of numerous SAT-related tasks, such as MaxSAT and UNSAT core prediction. Due to the high complexity of handling these SAT-related tasks and the natural conversion of SAT formulas into graph structures, researchers have recently developed various graph learning methods to assist in prediction. However, these methods are often experimented on different datasets, with different approaches and different tasks, making it challenging to conduct unified evaluations and develop new algorithms. In this paper, we introduce the SATGL toolkit, the first open-source graph learning toolkit for the SAT problem. We expect SATGL to contribute to the advancement of artificial intelligence (AI) for SAT, facilitating SAT solving and new algorithm design.","This work is supported in part by the National Natural Science Foundation of China (No. U20B2045, U1936220, 62192784, 62172052, 62002029, 61772082) and Beijing Natural Science Foundation (No. 4244107).","This work is supported in part by the National Natural Science Foundation of China (No. U20B2045, U1936220, 62192784, 62172052, 62002029, 61772082) and Beijing Natural Science Foundation (No. 4244107).",,2024 2nd International Symposium of Electronics Design Automation (ISEDA),,,2024-05-13,2024,,2024-05-13,00,,746-751,Closed,Proceeding,"Cheng, HongTao; Liu, Jiawei; Zhai, Jianwang; Zhao, Mingyu; Yang, Cheng; Shi, Chuan","Cheng, HongTao (Beijing University of Posts and Telecommunications, Beijing, China); Liu, Jiawei (Beijing University of Posts and Telecommunications, Beijing, China); Zhai, Jianwang (Beijing University of Posts and Telecommunications, Beijing, China); Zhao, Mingyu (Beijing University of Posts and Telecommunications, Beijing, China); Yang, Cheng (Beijing University of Posts and Telecommunications, Beijing, China); Shi, Chuan (Beijing University of Posts and Telecommunications, Beijing, China)","Cheng, HongTao (Beijing University of Posts and Telecommunications); Liu, Jiawei (Beijing University of Posts and Telecommunications); Shi, Chuan (Beijing University of Posts and Telecommunications)","Cheng, HongTao (Beijing University of Posts and Telecommunications); Liu, Jiawei (Beijing University of Posts and Telecommunications); Zhai, Jianwang (Beijing University of Posts and Telecommunications); Zhao, Mingyu (Beijing University of Posts and Telecommunications); Yang, Cheng (Beijing University of Posts and Telecommunications); Shi, Chuan (Beijing University of Posts and Telecommunications)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1174625683,46 Information and Computing Sciences; 4602 Artificial Intelligence,
881,pub.1171027317,10.1145/3620666.3651336,,,FEASTA: A Flexible and Efficient Accelerator for Sparse Tensor Algebra in Machine Learning,"Recently, sparse tensor algebra (SpTA) plays an increasingly important role in machine learning. However, due to the unstructured sparsity of SpTA, the general-purpose processors (e.g., GPU and CPU) are inefficient because of the underutilized hardware resources. Sparse kernel accelerators are optimized for specific tasks. However, their dedicated processing units and data paths cannot effectively support other SpTA tasks with different dataflow and various sparsity, resulting in performance degradation. This paper proposes FEASTA, a Flexible and Efficient Accelerator for Sparse Tensor Algebra. To process general SpTA tasks with various sparsity efficiently, we design FEASTA meticulously from three levels. At the dataflow abstraction level, we apply the Einstein Summation on the sparse fiber tree data structure to model the unified execution flow of general SpTA as joining and merging the fiber tree. At the instruction set architecture (ISA) level, a general SpTA ISA is proposed based on the execution flow. It includes different types of instructions for dense and sparse data, achieving flexibility and efficiency at the instruction level. At the architecture level, an instruction-driven architecture consisting of configurable and high-performance function units is designed, supporting the flexible and efficient ISA. Evaluations show that FEASTA has 5.40× geomean energy efficiency improvements compared to GPU among various workloads. FEASTA delivers 1.47× and 3.19× higher performance on sparse matrix multiplication kernels compared to state-of-the-art sparse matrix accelerator and CPU extension. Across diverse kernels, FEASTA achieves 1.69-12.70× energy efficiency over existing architectures.",,,,"Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3",,,2024-04-27,2024,2024-04-27,2024-04-27,,,349-366,Closed,Proceeding,"Zhong, Kai; Zhu, Zhenhua; Dai, Guohao; Wang, Hongyi; Yang, Xinhao; Zhang, Haoyu; Si, Jin; Mao, Qiuli; Zeng, Shulin; Hong, Ke; Zhang, Genghan; Yang, Huazhong; Wang, Yu","Zhong, Kai (Tsinghua University, Beijing, China); Zhu, Zhenhua (Tsinghua University, Beijing, China); Dai, Guohao (Shanghai Jiao Tong University, Shanghai, China; Infinigence-AI, Beijing, China); Wang, Hongyi (Tsinghua University, Beijing, China); Yang, Xinhao (Tsinghua University, Beijing, China); Zhang, Haoyu (Tsinghua University, Beijing, China); Si, Jin (Beijing University of Posts and Telecommunications, Beijing, China); Mao, Qiuli (Tsinghua University, Beijing, China); Zeng, Shulin (Tsinghua University, Beijing, China; Infinigence-AI, Beijing, China); Hong, Ke (Tsinghua University, Beijing, China); Zhang, Genghan (Stanford University, Stanford, USA); Yang, Huazhong (Tsinghua University, Beijing, China); Wang, Yu (Tsinghua University, Beijing, China)","Wang, Yu (Tsinghua University)","Zhong, Kai (Tsinghua University); Zhu, Zhenhua (Tsinghua University); Dai, Guohao (Shanghai Jiao Tong University; Infinigence-AI, Beijing, China); Wang, Hongyi (Tsinghua University); Yang, Xinhao (Tsinghua University); Zhang, Haoyu (Tsinghua University); Si, Jin (Beijing University of Posts and Telecommunications); Mao, Qiuli (Tsinghua University); Zeng, Shulin (Tsinghua University; Infinigence-AI, Beijing, China); Hong, Ke (Tsinghua University); Zhang, Genghan (Stanford University); Yang, Huazhong (Tsinghua University); Wang, Yu (Tsinghua University)",2,2,,,,https://app.dimensions.ai/details/publication/pub.1171027317,33 Built Environment and Design; 3301 Architecture; 46 Information and Computing Sciences,7 Affordable and Clean Energy
874,pub.1160228461,10.48550/arxiv.2306.15749,,,To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration,"As deep learning models scale, they become increasingly competitive from
domains spanning from computer vision to natural language processing; however,
this happens at the expense of efficiency since they require increasingly more
memory and computing power. The power efficiency of the biological brain
outperforms any large-scale deep learning ( DL ) model; thus, neuromorphic
computing tries to mimic the brain operations, such as spike-based information
processing, to improve the efficiency of DL models. Despite the benefits of the
brain, such as efficient information transmission, dense neuronal
interconnects, and the co-location of computation and memory, the available
biological substrate has severely constrained the evolution of biological
brains. Electronic hardware does not have the same constraints; therefore,
while modeling spiking neural networks ( SNNs) might uncover one piece of the
puzzle, the design of efficient hardware backends for SNN s needs further
investigation, potentially taking inspiration from the available work done on
the artificial neural networks ( ANNs) side. As such, when is it wise to look
at the brain while designing new hardware, and when should it be ignored? To
answer this question, we quantitatively compare the digital hardware
acceleration techniques and platforms of ANNs and SNN s. As a result, we
provide the following insights: (i) ANNs currently process static data more
efficiently, (ii) applications targeting data produced by neuromorphic sensors,
such as event-based cameras and silicon cochleas, need more investigation since
the behavior of these sensors might naturally fit the SNN paradigm, and (iii)
hybrid approaches combining SNN s and ANNs might lead to the best solutions and
should be investigated further at the hardware level, accounting for both
efficiency and loss optimization.",,,arXiv,,,,2023-06-27,2023,,,,,,All OA; Green,Preprint,"Ottati, Fabrizio; Gao, Chang; Chen, Qinyu; Brignone, Giovanni; Casu, Mario R.; Eshraghian, Jason K.; Lavagno, Luciano","Ottati, Fabrizio (); Gao, Chang (); Chen, Qinyu (); Brignone, Giovanni (); Casu, Mario R. (); Eshraghian, Jason K. (); Lavagno, Luciano ()",,"Ottati, Fabrizio (); Gao, Chang (); Chen, Qinyu (); Brignone, Giovanni (); Casu, Mario R. (); Eshraghian, Jason K. (); Lavagno, Luciano ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1160228461,46 Information and Computing Sciences; 4611 Machine Learning,7 Affordable and Clean Energy
867,pub.1170751509,10.48550/arxiv.2404.10076,,,Field-Programmable Gate Array Architecture for Deep Learning: Survey & Future Directions,"Deep learning (DL) is becoming the cornerstone of numerous applications both
in datacenters and at the edge. Specialized hardware is often necessary to meet
the performance requirements of state-of-the-art DL models, but the rapid pace
of change in DL models and the wide variety of systems integrating DL make it
impossible to create custom computer chips for all but the largest markets.
Field-programmable gate arrays (FPGAs) present a unique blend of
reprogrammability and direct hardware execution that make them suitable for
accelerating DL inference. They offer the ability to customize processing
pipelines and memory hierarchies to achieve lower latency and higher energy
efficiency compared to general-purpose CPUs and GPUs, at a fraction of the
development time and cost of custom chips. Their diverse high-speed IOs also
enable directly interfacing the FPGA to the network and/or a variety of
external sensors, making them suitable for both datacenter and edge use cases.
As DL has become an ever more important workload, FPGA architectures are
evolving to enable higher DL performance. In this article, we survey both
academic and industrial FPGA architecture enhancements for DL. First, we give a
brief introduction on the basics of FPGA architecture and how its components
lead to strengths and weaknesses for DL applications. Next, we discuss
different styles of DL inference accelerators on FPGA, ranging from
model-specific dataflow styles to software-programmable overlay styles. We
survey DL-specific enhancements to traditional FPGA building blocks such as
logic blocks, arithmetic circuitry, and on-chip memories, as well as new
in-fabric DL-specialized blocks for accelerating tensor computations. Finally,
we discuss hybrid devices that combine processors and coarse-grained
accelerator blocks with FPGA-like interconnect and networks-on-chip, and
highlight promising future research directions.",,,arXiv,,,,2024-04-15,2024,,,,,,All OA; Green,Preprint,"Boutros, Andrew; Arora, Aman; Betz, Vaughn","Boutros, Andrew (); Arora, Aman (); Betz, Vaughn ()",,"Boutros, Andrew (); Arora, Aman (); Betz, Vaughn ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1170751509,"33 Built Environment and Design; 3301 Architecture; 40 Engineering; 4009 Electronics, Sensors and Digital Hardware",7 Affordable and Clean Energy
859,pub.1176108495,10.48550/arxiv.2409.18553,,,Efficient Noise Mitigation for Enhancing Inference Accuracy in DNNs on Mixed-Signal Accelerators,"In this paper, we propose a framework to enhance the robustness of the neural
models by mitigating the effects of process-induced and aging-related
variations of analog computing components on the accuracy of the analog neural
networks. We model these variations as the noise affecting the precision of the
activations and introduce a denoising block inserted between selected layers of
a pre-trained model. We demonstrate that training the denoising block
significantly increases the model's robustness against various noise levels. To
minimize the overhead associated with adding these blocks, we present an
exploration algorithm to identify optimal insertion points for the denoising
blocks. Additionally, we propose a specialized architecture to efficiently
execute the denoising blocks, which can be integrated into mixed-signal
accelerators. We evaluate the effectiveness of our approach using Deep Neural
Network (DNN) models trained on the ImageNet and CIFAR-10 datasets. The results
show that on average, by accepting 2.03% parameter count overhead, the accuracy
drop due to the variations reduces from 31.7% to 1.15%.",,,arXiv,,,,2024-09-27,2024,,,,,,All OA; Green,Preprint,"Azizi, Seyedarmin; Sadeghi, Mohammad Erfan; Kamal, Mehdi; Pedram, Massoud","Azizi, Seyedarmin (); Sadeghi, Mohammad Erfan (); Kamal, Mehdi (); Pedram, Massoud ()",,"Azizi, Seyedarmin (); Sadeghi, Mohammad Erfan (); Kamal, Mehdi (); Pedram, Massoud ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1176108495,46 Information and Computing Sciences; 4611 Machine Learning,
859,pub.1174983029,10.1109/asap61560.2024.00049,,,xTern: Energy-Efficient Ternary Neural Network Inference on RISC-V-Based Edge Systems,"Ternary neural networks (TNNs) offer a superior accuracy-energy tradeoff compared to binary neural networks. However, until now, they have required specialized accelerators to realize their efficiency potential, which has hindered widespread adoption. To address this, we present xTern, a lightweight extension of the RISC-V instruction set architecture (ISA) targeted at accelerating TNN inference on general-purpose cores. To complement the ISA extension, we developed a set of optimized kernels leveraging xTern, achieving 67 % higher throughput than their 2-bit equivalents. Power consumption is only marginally increased by 5.2 %, resulting in an energy efficiency improvement by 57.1 %. We demonstrate that the proposed xTern extension, integrated into an octa-core compute cluster, incurs a minimal silicon area overhead of 0.9 % with no impact on timing. In end-to-end benchmarks, we demonstrate that xTern enables the deployment of TNNs achieving up to 1.6 percentage points higher CIFAR-10 classification accuracy than 2-bit networks at equal inference latency. Our results show that xTern enables RISC-V-based ultra-low-power edge AI platforms to benefit from the efficiency potential of TNNs.","This work is supported in part by the NeuroSoC project, funded under Horizon Europe Grant Agreement n° 101070634.","This work is supported in part by the NeuroSoC project, funded under Horizon Europe Grant Agreement n° 101070634.",,"2024 IEEE 35th International Conference on Application-specific Systems, Architectures and Processors (ASAP)",,,2024-07-26,2024,,2024-07-26,00,,206-213,All OA; Green,Proceeding,"Rutishauser, Georg; Mihali, Joan; Scherer, Moritz; Bonini, Luca","Rutishauser, Georg (Departement Informationstechnologie und Elektrotechnik, ETH Zürich, Zürich, Switzerland); Mihali, Joan (Dipartimento di Ingegneria dell'Energia Elettrica e dell'Informazione, Università di Bologna, Bologna, Italy); Scherer, Moritz (Departement Informationstechnologie und Elektrotechnik, ETH Zürich, Zürich, Switzerland); Bonini, Luca (Departement Informationstechnologie und Elektrotechnik, ETH Zürich, Zürich, Switzerland; Dipartimento di Ingegneria dell'Energia Elettrica e dell'Informazione, Università di Bologna, Bologna, Italy)",,"Rutishauser, Georg (ETH Zurich); Mihali, Joan (University of Bologna); Scherer, Moritz (ETH Zurich); Bonini, Luca (ETH Zurich; University of Bologna)",0,0,,,https://arxiv.org/pdf/2405.19065,https://app.dimensions.ai/details/publication/pub.1174983029,33 Built Environment and Design; 3301 Architecture; 46 Information and Computing Sciences,7 Affordable and Clean Energy
857,pub.1172184942,10.48550/arxiv.2405.19065,,,xTern: Energy-Efficient Ternary Neural Network Inference on RISC-V-Based Edge Systems,"Ternary neural networks (TNNs) offer a superior accuracy-energy trade-off
compared to binary neural networks. However, until now, they have required
specialized accelerators to realize their efficiency potential, which has
hindered widespread adoption. To address this, we present xTern, a lightweight
extension of the RISC-V instruction set architecture (ISA) targeted at
accelerating TNN inference on general-purpose cores. To complement the ISA
extension, we developed a set of optimized kernels leveraging xTern, achieving
67% higher throughput than their 2-bit equivalents. Power consumption is only
marginally increased by 5.2%, resulting in an energy efficiency improvement by
57.1%. We demonstrate that the proposed xTern extension, integrated into an
octa-core compute cluster, incurs a minimal silicon area overhead of 0.9% with
no impact on timing. In end-to-end benchmarks, we demonstrate that xTern
enables the deployment of TNNs achieving up to 1.6 percentage points higher
CIFAR-10 classification accuracy than 2-bit networks at equal inference
latency. Our results show that xTern enables RISC-V-based ultra-low-power edge
AI platforms to benefit from the efficiency potential of TNNs.",,,arXiv,,,,2024-05-29,2024,,,,,,All OA; Green,Preprint,"Rutishauser, Georg; Mihali, Joan; Scherer, Moritz; Benini, Luca","Rutishauser, Georg (); Mihali, Joan (); Scherer, Moritz (); Benini, Luca ()",,"Rutishauser, Georg (); Mihali, Joan (); Scherer, Moritz (); Benini, Luca ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1172184942,33 Built Environment and Design; 3301 Architecture; 46 Information and Computing Sciences,7 Affordable and Clean Energy
854,pub.1184138737,10.2139/ssrn.5036518,,,The Future of Software Engineering and IT Operations,"From managing finances via mobile apps and scheduling appointments to controlling home lighting with smart devices, software is now embedded in nearly every aspect of our lives. Given its undeniable importance, it is essential to explore how the underlying systems of software development and maintenance meet the demands of a digital world.Just as Moore’s Law describes the trend of shrinking transistor sizes, a similar phenomenon can be observed in software engineering. Emerging technologies in both software engineering and underlying hardware performance drive down the time and cost of software development but at the same time, increase its complexity and capabilities. This raises two critical questions: How will software engineering evolve to meet the arising complexities, and how will we effectively monitor and manage the expanding IT systems? These questions shape the focus of this report: The future of software engineering and IT operations.Besides technological dimensions, broader societal, environmental, legal, and economic factors also play a critical role in this regard. For instance, how will increasing concerns about sustainability affect software development practices? What legal frameworks need to be updated to regulate AI and data privacy? And how will economic shifts and global labor shortages impact the talent pool for software engineers?The transformation of software engineering and IT operations brings new challenges that demand innovative business models. For example, improving the accessibility of digital products will require solutions that bridge technology and regulation. Furthermore, post-breach response strategies will be redefined and assessed in novel ways. Additionally, the education of computer science students must evolve to align with the future needs of the workforce.This report organizes these considerations into three sections: trends, exploration, and ideation. First, current trends across technical, societal, environmental, regulatory, economic, and legal domains are examined, analyzing their potential future impact on the IT industry. Building on this, key problems and opportunities are identified and thoroughly analyzed. The final section translates these insights into five innovative business ideas, addressing critical areas such as accessibility compliance, education, AI inference, product management, and post-breach response.",,,SSRN Electronic Journal,,,,2025,2025,2025,,,,,All OA; Green,Preprint,"Eger, Vera; Sartor, Sebastian; Hollmann, Agustin Coppari; Milsztein, Aliosha; Borsutzky, Antonia; Wieser, Benedikt; Harjes, Federico; Preuss-Neudorf, Felicia; Tscherniak, Isabel; Skupien, Jakub; Seidou, Jonas; Fall, Khadim; Driese, Leon; Zimmer, Linus; Stein, Lizzy; Knoll, Max; Uludoğan, Mehmet; Serbinova, Milena; Sindemann, Niklas; Reichardt, Nils; Böllhoff, Paul; Burkhardt, Paul; Hugenroth, Philipp; Wahler, Philipp; Samdhani, Rudraksha; Yildiz, Selin; Tollmann, Sophie; Maurer, Teresa","Eger, Vera (Center for Digital Technology and Management (CDTM)); Sartor, Sebastian (Center for Digital Technology and Management (CDTM); MIT FutureTech; Technical University Munich); Hollmann, Agustin Coppari (Center for Digital Technology and Management (CDTM)); Milsztein, Aliosha (Center for Digital Technology and Management (CDTM)); Borsutzky, Antonia (Center for Digital Technology and Management (CDTM)); Wieser, Benedikt (Center for Digital Technology and Management (CDTM)); Harjes, Federico (Center for Digital Technology and Management (CDTM)); Preuss-Neudorf, Felicia (Center for Digital Technology and Management (CDTM)); Tscherniak, Isabel (Center for Digital Technology and Management (CDTM)); Skupien, Jakub (Center for Digital Technology and Management (CDTM)); Seidou, Jonas (Center for Digital Technology and Management (CDTM)); Fall, Khadim (Center for Digital Technology and Management (CDTM)); Driese, Leon (Center for Digital Technology and Management (CDTM)); Zimmer, Linus (Center for Digital Technology and Management (CDTM)); Stein, Lizzy (Center for Digital Technology and Management (CDTM)); Knoll, Max (Center for Digital Technology and Management (CDTM)); Uludoğan, Mehmet (Center for Digital Technology and Management (CDTM)); Serbinova, Milena (Center for Digital Technology and Management (CDTM)); Sindemann, Niklas (Center for Digital Technology and Management (CDTM)); Reichardt, Nils (Center for Digital Technology and Management (CDTM)); Böllhoff, Paul (Center for Digital Technology and Management (CDTM)); Burkhardt, Paul (Center for Digital Technology and Management (CDTM)); Hugenroth, Philipp (Center for Digital Technology and Management (CDTM)); Wahler, Philipp (Center for Digital Technology and Management (CDTM)); Samdhani, Rudraksha (Center for Digital Technology and Management (CDTM)); Yildiz, Selin (Center for Digital Technology and Management (CDTM)); Tollmann, Sophie (Center for Digital Technology and Management (CDTM)); Maurer, Teresa (Center for Digital Technology and Management (CDTM))",,"Eger, Vera (Center for Digital Technology and Management (CDTM)); Sartor, Sebastian (Technical University of Munich); Hollmann, Agustin Coppari (Center for Digital Technology and Management (CDTM)); Milsztein, Aliosha (Center for Digital Technology and Management (CDTM)); Borsutzky, Antonia (Center for Digital Technology and Management (CDTM)); Wieser, Benedikt (Center for Digital Technology and Management (CDTM)); Harjes, Federico (Center for Digital Technology and Management (CDTM)); Preuss-Neudorf, Felicia (Center for Digital Technology and Management (CDTM)); Tscherniak, Isabel (Center for Digital Technology and Management (CDTM)); Skupien, Jakub (Center for Digital Technology and Management (CDTM)); Seidou, Jonas (Center for Digital Technology and Management (CDTM)); Fall, Khadim (Center for Digital Technology and Management (CDTM)); Driese, Leon (Center for Digital Technology and Management (CDTM)); Zimmer, Linus (Center for Digital Technology and Management (CDTM)); Stein, Lizzy (Center for Digital Technology and Management (CDTM)); Knoll, Max (Center for Digital Technology and Management (CDTM)); Uludoğan, Mehmet (Center for Digital Technology and Management (CDTM)); Serbinova, Milena (Center for Digital Technology and Management (CDTM)); Sindemann, Niklas (Center for Digital Technology and Management (CDTM)); Reichardt, Nils (Center for Digital Technology and Management (CDTM)); Böllhoff, Paul (Center for Digital Technology and Management (CDTM)); Burkhardt, Paul (Center for Digital Technology and Management (CDTM)); Hugenroth, Philipp (Center for Digital Technology and Management (CDTM)); Wahler, Philipp (Center for Digital Technology and Management (CDTM)); Samdhani, Rudraksha (Center for Digital Technology and Management (CDTM)); Yildiz, Selin (Center for Digital Technology and Management (CDTM)); Tollmann, Sophie (Center for Digital Technology and Management (CDTM)); Maurer, Teresa (Center for Digital Technology and Management (CDTM))",0,0,,,,https://app.dimensions.ai/details/publication/pub.1184138737,46 Information and Computing Sciences; 4612 Software Engineering,4 Quality Education
836,pub.1172481991,10.48550/arxiv.2406.02302,,,Towards AI-Assisted Sustainable Adaptive Video Streaming Systems: Tutorial and Survey,"Improvements in networking technologies and the steadily increasing numbers
of users, as well as the shift from traditional broadcasting to streaming
content over the Internet, have made video applications (e.g., live and
Video-on-Demand (VoD)) predominant sources of traffic. Recent advances in
Artificial Intelligence (AI) and its widespread application in various academic
and industrial fields have focused on designing and implementing a variety of
video compression and content delivery techniques to improve user Quality of
Experience (QoE). However, providing high QoE services results in more energy
consumption and carbon footprint across the service delivery path, extending
from the end user's device through the network and service infrastructure
(e.g., cloud providers). Despite the importance of energy efficiency in video
streaming, there is a lack of comprehensive surveys covering state-of-the-art
AI techniques and their applications throughout the video streaming lifecycle.
Existing surveys typically focus on specific parts, such as video encoding,
delivery networks, playback, or quality assessment, without providing a
holistic view of the entire lifecycle and its impact on energy consumption and
QoE. Motivated by this research gap, this survey provides a comprehensive
overview of the video streaming lifecycle, content delivery, energy and Video
Quality Assessment (VQA) metrics and models, and AI techniques employed in
video streaming. In addition, it conducts an in-depth state-of-the-art analysis
focused on AI-driven approaches to enhance the energy efficiency of end-to-end
aspects of video streaming systems (i.e., encoding, delivery network, playback,
and VQA approaches). Finally, it discusses prospective research directions for
developing AI-assisted energy-aware video streaming systems.",,,arXiv,,,,2024-06-04,2024,,,,,,All OA; Green,Preprint,"Farahani, Reza; Azimi, Zoha; Timmerer, Christian; Prodan, Radu","Farahani, Reza (); Azimi, Zoha (); Timmerer, Christian (); Prodan, Radu ()",,"Farahani, Reza (); Azimi, Zoha (); Timmerer, Christian (); Prodan, Radu ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1172481991,40 Engineering; 4006 Communications Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation; 4606 Distributed Computing and Systems Software,7 Affordable and Clean Energy
828,pub.1169253711,10.1109/icce59016.2024.10444351,,,ICCE 2024 TOC,,,,2011 IEEE International Conference on Consumer Electronics (ICCE),2024 IEEE International Conference on Consumer Electronics (ICCE),,,2024-02-28,2024,2024-02-28,,,,i-cl,All OA; Bronze,Proceeding,,,,,0,0,,,https://ieeexplore.ieee.org/ielx7/10444098/10444131/10444351.pdf,https://app.dimensions.ai/details/publication/pub.1169253711,,
825,pub.1094067624,10.1109/imcec.2016.7867532,,,Analysis of Sampling Rate Conversion Technology in Software Radio,"As the mobile communication systems develop from 2G to 3G, people require a uniform-spectrum, uniform-architecture. Software radio provides a key solution to the interconnection of various communication architectures and radio standards with different spectrum, poor cooperation capability and so on. Sample rate conversion based software radio is a key technology to implement the interconnection. According to different requirements for the bandwidths and rates of various standards in 3G mobile communication systems, we designed the corresponding sample rate conversion schemes for CDMA2000, TD-SCDMA and compared the sample rate conversion schemes for five standards in 3G with each other. Experimental results show that our designs can effectively reduce the computing cost and memory cost in filtering.",,,,"2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",,,2016-10-01,2016,,2016-10-01,,,1812-1817,Closed,Proceeding,"Zhang, Yamei; Hao, Yunfang","Zhang, Yamei (ZTE Telecommunication College, Xi'an Peihua Universty, Xi'an, 710125, China); Hao, Yunfang (ZTE Telecommunication College, Xi'an Peihua Universty, Xi'an, 710125, China)",,"Zhang, Yamei (Xi'an Peihua University); Hao, Yunfang (Xi'an Peihua University)",1,0,,0.32,,https://app.dimensions.ai/details/publication/pub.1094067624,40 Engineering; 4006 Communications Engineering; 46 Information and Computing Sciences,
824,pub.1095501439,10.1109/pesc.2004.1354839,,,A High Frequency Link Direct DC-AC Converter for Residential Fuel Cell Power Systems,"This paper describes a boost converter cascaded high frequency link direct dc-ac converter suitable for fuel cell power sources. A new multi-loop control for a boost converter to reduce the low frequency input current harmonics drawn from the fuel cell is proposed. A new PWM technique for the cycloconverter at the secondary to reject the low order harmonics in the output voltages is presented in detail. The front-end boost converter regulates the de bus voltage effectively where the fuel cell voltage varies widely depending on the power demand, and provides a constant de voltage to the high frequency link dc-ac converter. The proposed multi-loop control reduces the low frequency (120Hz) input current harmonic of the boost converter without additional filter requirement in order to improve the performance of fuel cell system. The proposed PWM technique for the cycloconverter cancels the low order harmonics in the output voltage caused by the low frequency voltage ripple in the de bus and guarantees the high quality of the output voltage. The trade-offs of proposed scheme are described in detail with mathematical evaluation approach.",,,,2004 IEEE 35th Annual Power Electronics Specialists Conference (IEEE Cat. No.04CH37551),,,2004-01-01,2004,,2004-01-01,6,,4755-4761,Closed,Proceeding,"Song, Yu Jin; Enjeti, Prasad N.","Song, Yu Jin (Power Electronics and Fuel Cell Power Systems Laboratory, Department of Electrical Engineering, Texas A&M University, College Station, TX, 77843-3218, USA); Enjeti, Prasad N. (Power Electronics and Fuel Cell Power Systems Laboratory, Department of Electrical Engineering, Texas A&M University, College Station, TX, 77843-3218, USA)",,"Song, Yu Jin (Texas A&M University); Enjeti, Prasad N. (Texas A&M University)",25,0,,6.11,,https://app.dimensions.ai/details/publication/pub.1095501439,"40 Engineering; 4007 Control Engineering, Mechatronics and Robotics; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 4010 Engineering Practice and Education",
796,pub.1166949243,10.1109/mlccim60412.2023.00016,,,A STDP Rules-Based Spiking Neural Network Implementation for Image Recognition,"With the progress and development of artificial intelligence, the concept of neuromorphic computing has been widely observed since its proposed. The biological neural system can use sparse electrical spikes for information transfer, has powerful memory-learning functions, and perform complex tasks with extremely low power consumption. As the third generation neural network, spiking neural network has higher level of bionic characteristics than the traditional neural network. It can simulate the information processing mechanism of human brain to a greater extent, which is the focus of the research of brain-like computing. In this work, a two-layer SNN composed of Leaky-Integrate-Fire neurons was implemented in the BindsNET simulation environment. The network was learned and trained by the time-dependent plasticity of spike rule, and achieves 92% recognition accuracy on the MNIST handwritten digit set.",The work was supported financially by the National Natural Science Foundation of China (No. 62174150) and the Natural Science Foundation of Jang Su Province (No. BK20211040).,The work was supported financially by the National Natural Science Foundation of China (No. 62174150) and the Natural Science Foundation of Jang Su Province (No. BK20211040).,,"2023 2nd International Conference on Machine Learning, Cloud Computing and Intelligent Mining (MLCCIM)",,,2023-07-29,2023,,2023-07-29,00,,71-76,Closed,Proceeding,"Teng, Haoran; Yu, Yin; Wei, Jinghe; Liu, Guozhu","Teng, Haoran (The 58th Research Institute of China Electronics, Technology Group Corporation, Wuxi, China; School of Electronic Science, National University of Defense Technology, Changsha, China); Yu, Yin (No. 38 Research Institute of CETC, Hefei, China; School of Electronic Science, National University of Defense Technology, Changsha, China); Wei, Jinghe (The 58th Research Institute of China Electronics, Technology Group Corporation, Wuxi, China); Liu, Guozhu (The 58th Research Institute of China Electronics, Technology Group Corporation, Wuxi, China)","Teng, Haoran (China Electronics Technology Group Corporation (China); National University of Defense Technology)","Teng, Haoran (China Electronics Technology Group Corporation (China); National University of Defense Technology); Yu, Yin (China Electronics Technology Group Corporation (China); National University of Defense Technology); Wei, Jinghe (China Electronics Technology Group Corporation (China)); Liu, Guozhu (China Electronics Technology Group Corporation (China))",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1166949243,32 Biomedical and Clinical Sciences; 3209 Neurosciences; 46 Information and Computing Sciences; 4611 Machine Learning,
777,pub.1182285550,10.1109/idsta62194.2024.10746990,,,Enhancing Safety in Autonomous Vehicles through Advanced AI-Driven Perception and Decision-Making Systems,"Accidents, congested roads, consumption of energy, and emissions may all decrease significantly with the rise of self-driving cars, providing a promising response to social and environmental issues. In this study, researchers investigate the way the integration of innovative AI-driven perception and decision-making systems into AVs impacts safety. AVs have the potential to change transportation by reducing accidents caused mainly by human error. They may operate on their own or in conjunction with human drivers. The primary goal is to investigate and enhance AV safety by creating highly sophisticated perception and decision-making technologies driven by machine learning. Pragmatism research philosophy, experimental research design, and inductive approach serve as the selected methods of the study. In addition, secondary qualitative data analysis methods help evaluate the entire study. The outcomes of the study demonstrated that the advancement of autonomous vehicles depends significantly on the creation of AI-driven sensors. “Vehicle-to-vehicle (V2X) networks” for communication and safety features increase security, while integrated camera systems with acoustic and thermal sensors improve sensing capacities. Deep learning techniques, especially “convolutional neural networks (CNN)” and “fully convolutional networks (FCN)”, facilitate accurate object recognition and segmentation. Vehicles’ ability to handle difficult road conditions is further improved by a real-time risk evaluation and trajectory planning based on human-like behavioral modeling.",,,,2024 Fifth International Conference on Intelligent Data Science Technologies and Applications (IDSTA),,,2024-01-27,2024,,2024-01-27,00,,208-217,Closed,Proceeding,"Alahmed, Yazan; Abadla, Reema; Al Ansari, Mohammed Jassim","Alahmed, Yazan (Faculty of Engineering, Al Ain University, Abu Dhabi, U.A.E.); Abadla, Reema (Faculty of Engineering, Al Ain University, Abu Dhabi, U.A.E.); Al Ansari, Mohammed Jassim (School of Business, University of Central Lancashire, preston, UK)","Alahmed, Yazan (Al Ain University)","Alahmed, Yazan (Al Ain University); Abadla, Reema (Al Ain University); Al Ansari, Mohammed Jassim (University of Central Lancashire)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1182285550,"35 Commerce, Management, Tourism and Services; 3509 Transportation, Logistics and Supply Chains; 46 Information and Computing Sciences; 4605 Data Management and Data Science",3 Good Health and Well Being; 7 Affordable and Clean Energy
772,pub.1174031735,10.48550/arxiv.2407.15320,,,Edge Graph Intelligence: Reciprocally Empowering Edge Networks with Graph Intelligence,"Recent years have witnessed a thriving growth of computing facilities
connected at the network edge, cultivating edge computing networks as a
fundamental infrastructure for supporting miscellaneous intelligent services.
Meanwhile, Artificial Intelligence frontiers have extrapolated Machine Learning
to the graph domain and promoted Graph Intelligence (GI), which unlocks
unprecedented ability in learning from massive data in graph structures. Given
the inherent relation between graphs and networks, the interdiscipline of graph
representation learning and edge networks, i.e., Edge GI or EGI, has revealed a
novel interplay between them -- GI models principally open a new door for
modeling, understanding, and optimizing edge networks, and conversely, edge
networks serve as physical support for training, deploying, and accelerating GI
models. Driven by this delicate closed-loop, EGI can be widely recognized as a
promising solution to fully unleash the potential of edge computing power and
is garnering significant attention. Nevertheless, research on EGI yet remains
nascent, and there is a soaring demand within both the communications and AI
communities for a dedicated venue to share recent advancements. To this end,
this paper promotes the concept of EGI, explores its scope and core principles,
and conducts a comprehensive survey concerning recent research efforts on this
emerging field and specifically, introduces and discusses: 1) fundamentals of
edge computing and graph representation learning, 2) emerging techniques
centering on the closed loop between graph intelligence and edge networks, and
3) open challenges and research opportunities of future EGI. By bridging the
gap across communication, networking, and graph learning areas, we believe that
this survey can garner increased attention, foster meaningful discussions, and
inspire further research ideas in EGI.",,,arXiv,,,,2024-07-07,2024,,,,,,All OA; Green,Preprint,"Zeng, Liekang; Ye, Shengyuan; Chen, Xu; Zhang, Xiaoxi; Ren, Ju; Tang, Jian; Yang, Yang; Xuemin; Shen","Zeng, Liekang (Sherman); Ye, Shengyuan (Sherman); Chen, Xu (Sherman); Zhang, Xiaoxi (Sherman); Ren, Ju (Sherman); Tang, Jian (Sherman); Yang, Yang (Sherman); Xuemin (Sherman); Shen ()",,"Zeng, Liekang (Sherman); Ye, Shengyuan (Sherman); Chen, Xu (Sherman); Zhang, Xiaoxi (Sherman); Ren, Ju (Sherman); Tang, Jian (Sherman); Yang, Yang (Sherman); Xuemin (Sherman); Shen ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1174031735,46 Information and Computing Sciences; 4602 Artificial Intelligence,
725,pub.1183022696,10.1109/micro61859.2024.00106,,,VGA: Hardware Accelerator for Scalable Long Sequence Model Inference,"Effectively modeling relationships between distant elements in a long input sequence is an important task that remains challenging to this day. The state-of-the-art models for processing sequential data are self-attention-based transformer models. However, the computational complexity of self-attention is quadratic to the input sequence length, which often becomes the limiting factor in scaling the sequence length. Recently, state space model (SSM)-based global convolution models, which replace attention with convolution, have been found to be effective for modeling long sequences, with a sub-quadratic complexity using Fast Fourier Transform (FFT). However, they show sub-optimal performance on data-parallel accelerators like GPU, due to the regions of extremely low compute utilization with memory bandwidth-bound operations. To address this inefficiency, this paper proposes the Vandermonde matrix Generating Accelerator (VGA), a custom accelerator that performs FFT-based convolution in an area/power-efficient manner. VGA introduces Complex number Compute Units (CCUs) to fully utilize the high on-chip SRAM bandwidth, and parameters are generated on the fly to drastically reduce the required SRAM capacity. VGA achieves 76×(48×) higher area (power) efficiency than NVIDIA A100 GPU when executing the global convolution operator of H3, a state-of-the-art SSM-based model.","This work was supported by a research grant from Sam-sung Advanced Institute of Technology (SAIT), the National Research Foundation of Korea (NRF) grants funded by the Korea Government (MSIT) (RS-2024-00340008 and RS-2024- 00405857). The EDA tool was supported by the IC Design Ed-ucation Center (IDEC), Korea. Jae W. Lee is the corresponding author.","This work was supported by a research grant from Samsung Advanced Institute of Technology (SAIT), the National Research Foundation of Korea (NRF) grants funded by the Korea Government (MSIT) (RS-2024-00340008 and RS-2024- 00405857). The EDA tool was supported by the IC Design Education Center (IDEC), Korea.",,2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO),,,2024-11-06,2024,,2024-11-06,00,,1444-1457,Closed,Proceeding,"Lee, Seung Yul; Lee, Hyunseung; Hong, Jihoon; Cho, SangLyul; Lee, Jae W.","Lee, Seung Yul (Seoul National University, Seoul, Republic of Korea); Lee, Hyunseung (Seoul National University, Seoul, Republic of Korea); Hong, Jihoon (Seoul National University, Seoul, Republic of Korea); Cho, SangLyul (Seoul National University, Seoul, Republic of Korea); Lee, Jae W. (Seoul National University, Seoul, Republic of Korea)",,"Lee, Seung Yul (Seoul National University); Lee, Hyunseung (Seoul National University); Hong, Jihoon (Seoul National University); Cho, SangLyul (Seoul National University); Lee, Jae W. (Seoul National University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1183022696,40 Engineering; 4008 Electrical Engineering,
681,pub.1183917307,10.1109/apccas62602.2024.10808902,,,APCCAS 2024 Cover Page,,,,2023 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS),2024 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS),,,2024-12-27,2024,2024-12-27,,,,c1-c107,Closed,Proceeding,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1183917307,,
653,pub.1182280415,10.1109/sc41406.2024.00021,,,CUSZP2: A GPU Lossy Compressor with Extreme Throughput and Optimized Compression Ratio,"Existing GPU lossy compressors suffer from expensive data movement overheads, inefficient memory access patterns, and high synchronization latency, resulting in limited throughput. This work proposes cuSZP2, a generic single-kernel error-bounded lossy compressor purely on GPUs designed for applications that require high speed, such as large-scale GPU simulation and large language model training. In particular, CUSZP2 proposes a novel lossless encoding method, optimizes memory access patterns, and hides synchronization latency, achieving extreme end-to-end throughput and optimized compression ratio. Experiments on NVIDIA A100 GPU with 9 real-world HPC datasets demonstrate that, even with higher compression ratios and data quality, CUSZP2 can deliver on average 332.42 and $513.04 \mathrm{~GB} / \mathrm{s}$ end-to-end throughput for compression and decompression, respectively, which is around $2 \times$ of existing pure-GPU compressors and $200 \times$ of CPU-GPU hybrid compressors.","This material was based upon work supported by the U.S. Department of Energy, Office of Science, Advanced Scientific Computing Research (ASCR), under contract DE-AC0206CH11357 and DE-SC0024559. The material was also supported by the National Science Foundation under Grant OAC2003709, OAC-2104023, OAC-2311875, and OAC-2211538. The experimental resource for this paper was provided by the Laboratory Computing Resource Center on the Swing cluster at Argonne National Laboratory.",,,"SC24: International Conference for High Performance Computing, Networking, Storage and Analysis",,,2024-11-22,2024,,2024-11-22,00,,1-18,Closed,Proceeding,"Huang, Yafan; Di, Sheng; Li, Guanpeng; Cappello, Franck","Huang, Yafan (Computer Science Department, University of Iowa, Iowa City, IA, USA); Di, Sheng (Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA); Li, Guanpeng (Computer Science Department, University of Iowa, Iowa City, IA, USA); Cappello, Franck (Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA)","Di, Sheng (Argonne National Laboratory)","Huang, Yafan (University of Iowa); Di, Sheng (Argonne National Laboratory); Li, Guanpeng (University of Iowa); Cappello, Franck (Argonne National Laboratory)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1182280415,40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences,7 Affordable and Clean Energy
647,pub.1160112010,10.48550/arxiv.2306.13474,,,Efficient Online Processing with Deep Neural Networks,"The capabilities and adoption of deep neural networks (DNNs) grow at an
exhilarating pace: Vision models accurately classify human actions in videos
and identify cancerous tissue in medical scans as precisely than human experts;
large language models answer wide-ranging questions, generate code, and write
prose, becoming the topic of everyday dinner-table conversations. Even though
their uses are exhilarating, the continually increasing model sizes and
computational complexities have a dark side. The economic cost and negative
environmental externalities of training and serving models is in evident
disharmony with financial viability and climate action goals.
  Instead of pursuing yet another increase in predictive performance, this
dissertation is dedicated to the improvement of neural network efficiency.
Specifically, a core contribution addresses the efficiency aspects during
online inference. Here, the concept of Continual Inference Networks (CINs) is
proposed and explored across four publications. CINs extend prior
state-of-the-art methods developed for offline processing of spatio-temporal
data and reuse their pre-trained weights, improving their online processing
efficiency by an order of magnitude. These advances are attained through a
bottom-up computational reorganization and judicious architectural
modifications. The benefit to online inference is demonstrated by reformulating
several widely used network architectures into CINs, including 3D CNNs,
ST-GCNs, and Transformer Encoders. An orthogonal contribution tackles the
concurrent adaptation and computational acceleration of a large source model
into multiple lightweight derived models. Drawing on fusible adapter networks
and structured pruning, Structured Pruning Adapters achieve superior predictive
accuracy under aggressive pruning using significantly fewer learned weights
compared to fine-tuning with pruning.",,,arXiv,,,,2023-06-23,2023,,,,,,All OA; Green,Preprint,"Hedegaard, Lukas","Hedegaard, Lukas ()",,"Hedegaard, Lukas ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1160112010,46 Information and Computing Sciences; 4611 Machine Learning,
641,pub.1166542035,10.1109/iccad57390.2023.10323951,,,MasterRTL: A Pre-Synthesis PPA Estimation Framework for Any RTL Design,"In modern VLSI design flow, the register-transfer level (RTL) stage is a critical point, where designers define precise design behavior with hardware description languages (HDLs) like Verilog. Since the RTL design is in the format of HDL code, the standard way to evaluate its quality requires time-consuming subsequent synthesis steps with EDA tools. This time-consuming process significantly impedes design optimization at the early RTL stage. Despite the emergence of some recent ML-based solutions, they fail to maintain high accuracy for any given RTL design. In this work, we propose an innovative pre-synthesis PPA estimation framework named MasterRTL. It first converts the HDL code to a new bit-level design representation named the simple operator graph (SOG). By only adopting single-bit simple operators, this SOG proves to be a general representation that unifies different design types and styles. The SOG is also more similar to the target gate-level netlist, reducing the gap between RTL representation and netlist. In addition to the new SOG representation, MasterRTL proposes new ML methods for the RTL-stage modeling of timing, power, and area separately. Compared with state-of-the-art solutions, the experiment on a comprehensive dataset with 90 different designs shows accuracy improvement by 0.33, 0.22, and 0.15 in correlation for total negative slack (TNS), worst negative slack (WNS), and power, respectively.","This work is partially funded by the Hong Kong Research Grants Council (RGC) ECS Grant 26208723, Guangdong Basic and Ap-plied Basic Research Foundation no. 2022A1515110178, Guangzhou-HKUST(GZ) Joint Funding Scheme no. SL2022A03J01288, Guangzhou Basic Research Project no. SL2022A04J00615, and AC-CESS - AI Chip Center for Emerging Smart Systems, sponsored by InnoHK funding, Hong Kong SAR. Also, the authors thank the help from Prof Jiang Hu and Prianka Sengupta at Texas A&amp;M University.","This work is partially funded by the Hong Kong Research Grants Council (RGC) ECS Grant 26208723, Guangdong Basic and Ap-plied Basic Research Foundation no. 2022A1515110178, Guangzhou-HKUST(GZ) Joint Funding Scheme no. SL2022A03J01288, Guangzhou Basic Research Project no. SL2022A04J00615, and AC-CESS - AI Chip Center for Emerging Smart Systems, sponsored by InnoHK funding, Hong Kong SAR. Also, the authors thank the help from Prof Jiang Hu and Prianka Sengupta at Texas A&M University.",,2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD),,,2023-11-02,2023,,2023-11-02,00,,1-9,All OA; Green,Proceeding,"Fang, Wenji; Lu, Yao; Liu, Shang; Zhang, Qijun; Xu, Ceyu; Wills, Lisa Wu; Zhang, Hongce; Xie, Zhiyao","Fang, Wenji (Hong Kong University of Science and Technology (Guangzhou); Hong Kong University of Science and Technology); Lu, Yao (Hong Kong University of Science and Technology); Liu, Shang (Hong Kong University of Science and Technology); Zhang, Qijun (Hong Kong University of Science and Technology); Xu, Ceyu (Duke University); Wills, Lisa Wu (Duke University); Zhang, Hongce (Hong Kong University of Science and Technology (Guangzhou); Hong Kong University of Science and Technology); Xie, Zhiyao (Hong Kong University of Science and Technology)","Zhang, Hongce (Hong Kong University of Science and Technology; Hong Kong University of Science and Technology); Xie, Zhiyao (Hong Kong University of Science and Technology)","Fang, Wenji (Hong Kong University of Science and Technology; Hong Kong University of Science and Technology); Lu, Yao (Hong Kong University of Science and Technology); Liu, Shang (Hong Kong University of Science and Technology); Zhang, Qijun (Hong Kong University of Science and Technology); Xu, Ceyu (Duke University); Wills, Lisa Wu (Duke University); Zhang, Hongce (Hong Kong University of Science and Technology; Hong Kong University of Science and Technology); Xie, Zhiyao (Hong Kong University of Science and Technology)",4,4,,3.25,http://arxiv.org/pdf/2311.08441,https://app.dimensions.ai/details/publication/pub.1166542035,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware",
637,pub.1166062089,10.48550/arxiv.2311.08441,,,MasterRTL: A Pre-Synthesis PPA Estimation Framework for Any RTL Design,"In modern VLSI design flow, the register-transfer level (RTL) stage is a
critical point, where designers define precise design behavior with hardware
description languages (HDLs) like Verilog. Since the RTL design is in the
format of HDL code, the standard way to evaluate its quality requires
time-consuming subsequent synthesis steps with EDA tools. This time-consuming
process significantly impedes design optimization at the early RTL stage.
Despite the emergence of some recent ML-based solutions, they fail to maintain
high accuracy for any given RTL design. In this work, we propose an innovative
pre-synthesis PPA estimation framework named MasterRTL. It first converts the
HDL code to a new bit-level design representation named the simple operator
graph (SOG). By only adopting single-bit simple operators, this SOG proves to
be a general representation that unifies different design types and styles. The
SOG is also more similar to the target gate-level netlist, reducing the gap
between RTL representation and netlist. In addition to the new SOG
representation, MasterRTL proposes new ML methods for the RTL-stage modeling of
timing, power, and area separately. Compared with state-of-the-art solutions,
the experiment on a comprehensive dataset with 90 different designs shows
accuracy improvement by 0.33, 0.22, and 0.15 in correlation for total negative
slack (TNS), worst negative slack (WNS), and power, respectively.",,,arXiv,,,,2023-11-14,2023,,,,,,All OA; Green,Preprint,"Fang, Wenji; Lu, Yao; Liu, Shang; Zhang, Qijun; Xu, Ceyu; Wills, Lisa Wu; Zhang, Hongce; Xie, Zhiyao","Fang, Wenji (); Lu, Yao (); Liu, Shang (); Zhang, Qijun (); Xu, Ceyu (); Wills, Lisa Wu (); Zhang, Hongce (); Xie, Zhiyao ()",,"Fang, Wenji (); Lu, Yao (); Liu, Shang (); Zhang, Qijun (); Xu, Ceyu (); Wills, Lisa Wu (); Zhang, Hongce (); Xie, Zhiyao ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1166062089,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware",
634,pub.1181579144,10.48550/arxiv.2410.17073,,,Personalized Playback Technology: How Short Video Services Create Excellent User Experience,"Short-form video content has become increasingly popular and influential in
recent years. Its concise yet engaging format aligns well with todays'
fast-paced and on-the-go lifestyles, making it a dominating trend in the
digital world. As one of the front runners in the short video platform space,
ByteDance has been highly successful in delivering a one-of-a-kind short video
experience and attracting billions of users worldwide. One key contributing
factor is its advanced end-to-end personalized short video playback technology,
where we pioneered and developed the new technical field over the past five
years to optimize user experience. This paper introduces the major concepts and
methodologies of this personalized video playback technology that distinguish
it from traditional multimedia technologies. More details, including goal
setting, iterative process, modeling, experimental methods and required
supporting systems, are also provided to encourage deeper research in this
area.",,,arXiv,,,,2024-10-22,2024,,,,,,All OA; Green,Preprint,"Deng, Weihui; Fan, Zhiwei; Fu, Deliang; Gong, Yun; Huang, Shenglan; Li, Xiaocheng; Li, Zheng; Liao, Yiting; Liu, He; Qiao, Chunyu; Wang, Bin; Wang, Zhen; Xiong, Zhengyu","Deng, Weihui (); Fan, Zhiwei (); Fu, Deliang (); Gong, Yun (); Huang, Shenglan (); Li, Xiaocheng (); Li, Zheng (); Liao, Yiting (); Liu, He (); Qiao, Chunyu (); Wang, Bin (); Wang, Zhen (); Xiong, Zhengyu ()",,"Deng, Weihui (); Fan, Zhiwei (); Fu, Deliang (); Gong, Yun (); Huang, Shenglan (); Li, Xiaocheng (); Li, Zheng (); Liao, Yiting (); Liu, He (); Qiao, Chunyu (); Wang, Bin (); Wang, Zhen (); Xiong, Zhengyu ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1181579144,40 Engineering; 4008 Electrical Engineering; 46 Information and Computing Sciences; 4603 Computer Vision and Multimedia Computation,
627,pub.1170976591,10.1145/3620665.3640422,,,AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference,"The Transformer-based generative model (TbGM), comprising summarization (Sum) and generation (Gen) stages, has demonstrated unprecedented generative performance across a wide range of applications. However, it also demands immense amounts of compute and memory resources. Especially, the Gen stages, consisting of the attention and fully-connected (FC) layers, dominate the overall execution time. Meanwhile, we reveal that the conventional system with GPUs used for TbGM inference cannot efficiently execute the attention layer, even with batching, due to various constraints. To address this inefficiency, we first propose AttAcc, a processing-in-memory (PIM) architecture for efficient execution of the attention layer. Subsequently, for the end-to-end acceleration of TbGM inference, we propose a novel heterogeneous system architecture and optimizations that strategically use xPU and PIM together. It leverages the high memory bandwidth of AttAcc for the attention layer and the powerful compute capability of the conventional system for the FC layer. Lastly, we demonstrate that our GPU-PIM system outperforms the conventional system with the same memory capacity, improving performance and energy efficiency of running a 175B TbGM by up to 2.81× and 2.67×, respectively.",,,,"Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2",,,2024-04-27,2024,2024-04-27,2024-04-27,,,103-119,Closed,Proceeding,"Park, Jaehyun; Choi, Jaewan; Kyung, Kwanhee; Kim, Michael Jaemin; Kwon, Yongsuk; Kim, Nam Sung; Ahn, Jung Ho","Park, Jaehyun (Seoul National University, Seoul, Republic of Korea); Choi, Jaewan (Seoul National University, Seoul, Republic of Korea); Kyung, Kwanhee (Seoul National University, Seoul, Republic of Korea); Kim, Michael Jaemin (Seoul National University, Seoul, Republic of Korea); Kwon, Yongsuk (Seoul National University, Seoul, Republic of Korea); Kim, Nam Sung (University of Illinois Urbana-Champaign, Urbana-Champaign, United States of America); Ahn, Jung Ho (Seoul National University, Seoul, Republic of Korea)","Ahn, Jung Ho (Seoul National University)","Park, Jaehyun (Seoul National University); Choi, Jaewan (Seoul National University); Kyung, Kwanhee (Seoul National University); Kim, Michael Jaemin (Seoul National University); Kwon, Yongsuk (Seoul National University); Kim, Nam Sung (University of Illinois Urbana-Champaign); Ahn, Jung Ho (Seoul National University)",4,4,,,,https://app.dimensions.ai/details/publication/pub.1170976591,33 Built Environment and Design; 3301 Architecture; 46 Information and Computing Sciences,7 Affordable and Clean Energy
584,pub.1171406661,10.1109/inocon60754.2024.10511379,,,INOCON 2024 Conference Schedule,,,,,2024 3rd International Conference for Innovation in Technology (INOCON),,,2024-05-06,2024,2024-05-06,,,,1-26,All OA; Bronze,Proceeding,,,,,0,0,,,https://ieeexplore.ieee.org/ielx7/10511271/10511093/10511379.pdf,https://app.dimensions.ai/details/publication/pub.1171406661,,
581,pub.1164085024,10.48550/arxiv.2309.07438,,,Towards Artificial General Intelligence (AGI) in the Internet of Things (IoT): Opportunities and Challenges,"Artificial General Intelligence (AGI), possessing the capacity to comprehend,
learn, and execute tasks with human cognitive abilities, engenders significant
anticipation and intrigue across scientific, commercial, and societal arenas.
This fascination extends particularly to the Internet of Things (IoT), a
landscape characterized by the interconnection of countless devices, sensors,
and systems, collectively gathering and sharing data to enable intelligent
decision-making and automation. This research embarks on an exploration of the
opportunities and challenges towards achieving AGI in the context of the IoT.
Specifically, it starts by outlining the fundamental principles of IoT and the
critical role of Artificial Intelligence (AI) in IoT systems. Subsequently, it
delves into AGI fundamentals, culminating in the formulation of a conceptual
framework for AGI's seamless integration within IoT. The application spectrum
for AGI-infused IoT is broad, encompassing domains ranging from smart grids,
residential environments, manufacturing, and transportation to environmental
monitoring, agriculture, healthcare, and education. However, adapting AGI to
resource-constrained IoT settings necessitates dedicated research efforts.
Furthermore, the paper addresses constraints imposed by limited computing
resources, intricacies associated with large-scale IoT communication, as well
as the critical concerns pertaining to security and privacy.",,,arXiv,,,,2023-09-14,2023,,,,,,All OA; Green,Preprint,"Dou, Fei; Ye, Jin; Yuan, Geng; Lu, Qin; Niu, Wei; Sun, Haijian; Guan, Le; Lu, Guoyu; Mai, Gengchen; Liu, Ninghao; Lu, Jin; Liu, Zhengliang; Wu, Zihao; Tan, Chenjiao; Xu, Shaochen; Wang, Xianqiao; Li, Guoming; Chai, Lilong; Li, Sheng; Sun, Jin; Sun, Hongyue; Shao, Yunli; Li, Changying; Liu, Tianming; Song, Wenzhan","Dou, Fei (); Ye, Jin (); Yuan, Geng (); Lu, Qin (); Niu, Wei (); Sun, Haijian (); Guan, Le (); Lu, Guoyu (); Mai, Gengchen (); Liu, Ninghao (); Lu, Jin (); Liu, Zhengliang (); Wu, Zihao (); Tan, Chenjiao (); Xu, Shaochen (); Wang, Xianqiao (); Li, Guoming (); Chai, Lilong (); Li, Sheng (); Sun, Jin (); Sun, Hongyue (); Shao, Yunli (); Li, Changying (); Liu, Tianming (); Song, Wenzhan ()",,"Dou, Fei (); Ye, Jin (); Yuan, Geng (); Lu, Qin (); Niu, Wei (); Sun, Haijian (); Guan, Le (); Lu, Guoyu (); Mai, Gengchen (); Liu, Ninghao (); Lu, Jin (); Liu, Zhengliang (); Wu, Zihao (); Tan, Chenjiao (); Xu, Shaochen (); Wang, Xianqiao (); Li, Guoming (); Chai, Lilong (); Li, Sheng (); Sun, Jin (); Sun, Hongyue (); Shao, Yunli (); Li, Changying (); Liu, Tianming (); Song, Wenzhan ()",2,2,,1.72,,https://app.dimensions.ai/details/publication/pub.1164085024,46 Information and Computing Sciences; 4605 Data Management and Data Science; 4606 Distributed Computing and Systems Software,
578,pub.1157850584,10.48550/arxiv.2305.03148,,,CAMEL: Co-Designing AI Models and Embedded DRAMs for Efficient On-Device Learning,"On-device learning allows AI models to adapt to user data, thereby enhancing
service quality on edge platforms. However, training AI on resource-limited
devices poses significant challenges due to the demanding computing workload
and the substantial memory consumption and data access required by deep neural
networks (DNNs). To address these issues, we propose utilizing embedded dynamic
random-access memory (eDRAM) as the primary storage medium for transient
training data. In comparison to static random-access memory (SRAM), eDRAM
provides higher storage density and lower leakage power, resulting in reduced
access cost and power leakage. Nevertheless, to maintain the integrity of the
stored data, periodic power-hungry refresh operations could potentially degrade
system performance.
  To minimize the occurrence of expensive eDRAM refresh operations, it is
beneficial to shorten the lifetime of stored data during the training process.
To achieve this, we adopt the principles of algorithm and hardware co-design,
introducing a family of reversible DNN architectures that effectively decrease
data lifetime and storage costs throughout training. Additionally, we present a
highly efficient on-device training engine named \textit{CAMEL}, which
leverages eDRAM as the primary on-chip memory. This engine enables efficient
on-device training with significantly reduced memory usage and off-chip DRAM
traffic while maintaining superior training accuracy. We evaluate our CAMEL
system on multiple DNNs with different datasets, demonstrating a $2.5\times$
speedup of the training process and $2.8\times$ training energy savings than
the other baseline hardware platforms.",,,arXiv,,,,2023-05-04,2023,,,,,,All OA; Green,Preprint,"Zhang, Sai Qian; Tambe, Thierry; Cuevas, Nestor; Wei, Gu-Yeon; Brooks, David","Zhang, Sai Qian (); Tambe, Thierry (); Cuevas, Nestor (); Wei, Gu-Yeon (); Brooks, David ()",,"Zhang, Sai Qian (); Tambe, Thierry (); Cuevas, Nestor (); Wei, Gu-Yeon (); Brooks, David ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1157850584,"40 Engineering; 4008 Electrical Engineering; 4009 Electronics, Sensors and Digital Hardware; 46 Information and Computing Sciences",7 Affordable and Clean Energy
577,pub.1095571812,10.1109/icdcsyst.2012.6188784,,,"2012 International Conference on Devices, Circuits and Systems (ICDCS)",Provides the entire conference content.,,,,"2012 International Conference on Devices, Circuits and Systems (ICDCS)",,,2012-03,2012,,,,,1-748,Closed,Proceeding,,,,,0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1095571812,46 Information and Computing Sciences; 4608 Human-Centred Computing,
547,pub.1173479081,10.1109/noms59830.2024.10575665,,,NOMS 2024 Cover Page,,,,2014 IEEE Network Operations and Management Symposium (NOMS),NOMS 2024-2024 IEEE Network Operations and Management Symposium,,,2024-07-02,2024,2024-07-02,,,,c1-c44,Closed,Proceeding,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1173479081,,
533,pub.1172245250,10.48550/arxiv.2405.19730,,,Research on the Spatial Data Intelligent Foundation Model,"This report focuses on spatial data intelligent large models, delving into
the principles, methods, and cutting-edge applications of these models. It
provides an in-depth discussion on the definition, development history, current
status, and trends of spatial data intelligent large models, as well as the
challenges they face. The report systematically elucidates the key technologies
of spatial data intelligent large models and their applications in urban
environments, aerospace remote sensing, geography, transportation, and other
scenarios. Additionally, it summarizes the latest application cases of spatial
data intelligent large models in themes such as urban development, multimodal
systems, remote sensing, smart transportation, and resource environments.
Finally, the report concludes with an overview and outlook on the development
prospects of spatial data intelligent large models.",,,arXiv,,,,2024-05-30,2024,,,,,,All OA; Green,Preprint,"Wang, Shaohua; Xie, Xing; Li, Yong; Guo, Danhuai; Cai, Zhi; Liu, Yu; Yue, Yang; Pan, Xiao; Lu, Feng; Wu, Huayi; Gui, Zhipeng; Ding, Zhiming; Zheng, Bolong; Zhang, Fuzheng; Wang, Jingyuan; Chen, Zhengchao; Lu, Hao; Li, Jiayi; Yue, Peng; Yu, Wenhao; Yao, Yao; Sun, Leilei; Zhang, Yong; Chen, Longbiao; Du, Xiaoping; Li, Xiang; Zhang, Xueying; Qin, Kun; Gong, Zhaoya; Dong, Weihua; Meng, Xiaofeng","Wang, Shaohua (State Key Laboratory of Remote Sensing Science, Aerospace Information Research Institute, Chinese Academy of Sciences); Xie, Xing (Microsoft Research Asia); Li, Yong (Tsinghua University); Guo, Danhuai (Beijing University of Chemical Technology); Cai, Zhi (Beijing University of Technology); Liu, Yu (Peking University); Yue, Yang (Shenzhen University); Pan, Xiao (Shijiazhuang Railway University); Lu, Feng (Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences); Wu, Huayi (Wuhan University); Gui, Zhipeng (Wuhan University); Ding, Zhiming (Research Institute of Software, Chinese Academy of Sciences); Zheng, Bolong (Huazhong University of Science and Technology); Zhang, Fuzheng (Fast Natural Language Processing Center and Audio Center); Wang, Jingyuan (Beihang University); Chen, Zhengchao (State Key Laboratory of Remote Sensing Science, Aerospace Information Research Institute, Chinese Academy of Sciences); Lu, Hao (SuperMap Software Co. Ltd); Li, Jiayi (Wuhan University); Yue, Peng (Wuhan University); Yu, Wenhao (China University of Geosciences); Yao, Yao (China University of Geosciences); Sun, Leilei (Beihang University); Zhang, Yong (Beijing University of Technology); Chen, Longbiao (Xiamen University); Du, Xiaoping (Key Laboratory of Digital Geography, Chinese Academy of Sciences); Li, Xiang (East China Normal University); Zhang, Xueying (Nanjing Normal University); Qin, Kun (Wuhan University); Gong, Zhaoya (Peking University); Dong, Weihua (Beijing Normal University); Meng, Xiaofeng (Renmin University of China)",,"Wang, Shaohua (Aerospace Information Research Institute); Xie, Xing (Microsoft Research Asia (China)); Li, Yong (Tsinghua University); Guo, Danhuai (Beijing University of Chemical Technology); Cai, Zhi (Beijing University of Technology); Liu, Yu (Peking University); Yue, Yang (Shenzhen University); Pan, Xiao (Shijiazhuang Railway University); Lu, Feng (Institute of Geographic Sciences and Natural Resources Research); Wu, Huayi (Wuhan University); Gui, Zhipeng (Wuhan University); Ding, Zhiming (Chinese Academy of Sciences); Zheng, Bolong (Huazhong University of Science and Technology); Zhang, Fuzheng (Fast Natural Language Processing Center and Audio Center); Wang, Jingyuan (Beihang University); Chen, Zhengchao (Aerospace Information Research Institute); Lu, Hao (SuperMap Software Co. Ltd); Li, Jiayi (Wuhan University); Yue, Peng (Wuhan University); Yu, Wenhao (China University of Geosciences); Yao, Yao (China University of Geosciences); Sun, Leilei (Beihang University); Zhang, Yong (Beijing University of Technology); Chen, Longbiao (Xiamen University); Du, Xiaoping (Chinese Academy of Sciences); Li, Xiang (East China Normal University); Zhang, Xueying (Nanjing Normal University); Qin, Kun (Wuhan University); Gong, Zhaoya (Peking University); Dong, Weihua (Beijing Normal University); Meng, Xiaofeng (Renmin University of China)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1172245250,37 Earth Sciences; 3704 Geoinformatics; 40 Engineering; 4013 Geomatic Engineering,
465,pub.1141324966,10.48550/arxiv.2109.10476,,,Self-Supervised Learning to Prove Equivalence Between Straight-Line Programs via Rewrite Rules,"We target the problem of automatically synthesizing proofs of semantic
equivalence between two programs made of sequences of statements. We represent
programs using abstract syntax trees (AST), where a given set of
semantics-preserving rewrite rules can be applied on a specific AST pattern to
generate a transformed and semantically equivalent program. In our system, two
programs are equivalent if there exists a sequence of application of these
rewrite rules that leads to rewriting one program into the other. We propose a
neural network architecture based on a transformer model to generate proofs of
equivalence between program pairs. The system outputs a sequence of rewrites,
and the validity of the sequence is simply checked by verifying it can be
applied. If no valid sequence is produced by the neural network, the system
reports the programs as non-equivalent, ensuring by design no programs may be
incorrectly reported as equivalent. Our system is fully implemented for one
single grammar which can represent straight-line programs with function calls
and multiple types. To efficiently train the system to generate such sequences,
we develop an original incremental training technique, named self-supervised
sample selection. We extensively study the effectiveness of this novel training
approach on proofs of increasing complexity and length. Our system, S4Eq,
achieves 97% proof success on a curated dataset of 10,000 pairs of equivalent
programs.",,,arXiv,,,,2021-09-21,2021,,,,,,All OA; Green,Preprint,"Kommrusch, Steve; Monperrus, Martin; Pouchet, Louis-Noël","Kommrusch, Steve (); Monperrus, Martin (); Pouchet, Louis-Noël ()",,"Kommrusch, Steve (); Monperrus, Martin (); Pouchet, Louis-Noël ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1141324966,46 Information and Computing Sciences; 4611 Machine Learning; 4612 Software Engineering,
459,pub.1181196918,10.1109/emcsipi49824.2024.10705444,,,EMC+SIPI 2024 TOC,,,,EMC proceedings,"2024 IEEE International Symposium on Electromagnetic Compatibility, Signal &amp; Power Integrity (EMC+SIPI)",,,2024-10-09,2024,2024-10-09,,,,i-ciii,Closed,Proceeding,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1181196918,,
406,pub.1151716837,10.48550/arxiv.2210.03204,,,Enabling Deep Learning on Edge Devices,"Deep neural networks (DNNs) have succeeded in many different perception
tasks, e.g., computer vision, natural language processing, reinforcement
learning, etc. The high-performed DNNs heavily rely on intensive resource
consumption. For example, training a DNN requires high dynamic memory, a
large-scale dataset, and a large number of computations (a long training time);
even inference with a DNN also demands a large amount of static storage,
computations (a long inference time), and energy. Therefore, state-of-the-art
DNNs are often deployed on a cloud server with a large number of
super-computers, a high-bandwidth communication bus, a shared storage
infrastructure, and a high power supplement.
  Recently, some new emerging intelligent applications, e.g., AR/VR, mobile
assistants, Internet of Things, require us to deploy DNNs on
resource-constrained edge devices. Compare to a cloud server, edge devices
often have a rather small amount of resources. To deploy DNNs on edge devices,
we need to reduce the size of DNNs, i.e., we target a better trade-off between
resource consumption and model accuracy.
  In this dissertation, we studied four edge intelligence scenarios, i.e.,
Inference on Edge Devices, Adaptation on Edge Devices, Learning on Edge
Devices, and Edge-Server Systems, and developed different methodologies to
enable deep learning in each scenario. Since current DNNs are often
over-parameterized, our goal is to find and reduce the redundancy of the DNNs
in each scenario.",,,arXiv,,,,2022-10-06,2022,,,,,,All OA; Green,Preprint,"Qu, Zhongnan","Qu, Zhongnan ()",,"Qu, Zhongnan ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1151716837,46 Information and Computing Sciences; 4606 Distributed Computing and Systems Software; 4611 Machine Learning,7 Affordable and Clean Energy
341,pub.1183856445,10.1109/iros58592.2024.10801700,,,IROS 2024 Subject Index Page,,,,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),,,2024-12-25,2024,2024-12-25,,,,i-cclxxxvii,Closed,Proceeding,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1183856445,,
340,pub.1172921021,10.48550/arxiv.2406.09548,,,Between Randomness and Arbitrariness: Some Lessons for Reliable Machine Learning at Scale,"To develop rigorous knowledge about ML models -- and the systems in which
they are embedded -- we need reliable measurements. But reliable measurement is
fundamentally challenging, and touches on issues of reproducibility,
scalability, uncertainty quantification, epistemology, and more. This
dissertation addresses criteria needed to take reliability seriously: both
criteria for designing meaningful metrics, and for methodologies that ensure
that we can dependably and efficiently measure these metrics at scale and in
practice. In doing so, this dissertation articulates a research vision for a
new field of scholarship at the intersection of machine learning, law, and
policy. Within this frame, we cover topics that fit under three different
themes: (1) quantifying and mitigating sources of arbitrariness in ML, (2)
taming randomness in uncertainty estimation and optimization algorithms, in
order to achieve scalability without sacrificing reliability, and (3) providing
methods for evaluating generative-AI systems, with specific focuses on
quantifying memorization in language models and training latent diffusion
models on open-licensed data. By making contributions in these three themes,
this dissertation serves as an empirical proof by example that research on
reliable measurement for machine learning is intimately and inescapably bound
up with research in law and policy. These different disciplines pose similar
research questions about reliable measurement in machine learning. They are, in
fact, two complementary sides of the same research vision, which, broadly
construed, aims to construct machine-learning systems that cohere with broader
societal values.",,,arXiv,,,,2024-06-13,2024,,,,,,All OA; Green,Preprint,"Cooper, A. Feder","Cooper, A. Feder ()",,"Cooper, A. Feder ()",0,0,,,,https://app.dimensions.ai/details/publication/pub.1172921021,46 Information and Computing Sciences; 4611 Machine Learning,
340,pub.1175691291,10.2139/ssrn.4922875,,,<span>Between Randomness and Arbitrariness:</span><span> Some Lessons for Reliable Machine Learning at Scale</span>,"To develop rigorous knowledge about ML models --- and the systems in which they are embedded --- we need reliable measurements. But reliable measurement is fundamentally challenging, and touches on issues of reproducibility, scalability, uncertainty quantification, epistemology, and more. This dissertation addresses criteria needed to take reliability seriously: both criteria for designing meaningful metrics, and for methodologies that ensure that we can dependably and efficiently measure these metrics at scale and in practice. In doing so, this dissertation articulates a research vision for a new field of scholarship at the intersection of machine learning, law, and policy. Within this frame, we cover topics that fit under three different themes. First, we quantify and mitigate sources of arbitrariness in machine learning, with respect to hyperparameter optimization and social prediction contexts. We clarify important connections between machine-learning arbitrariness, rooted in non-determinism, with legal notions of arbitrariness that implicate legal rules and due process. Second, we tame randomness in uncertainty estimation and optimization algorithms, in order to achieve scalability without sacrificing reliability. We discuss how across computing, and particularly in machine learning, scalability and reliability are typically in trade-off. Analogous trade-offs in law and policy make this type of trade-off a useful abstraction for communicating about machine-learning capabilities and risks to policymakers and other non-expert stakeholders. Third, we provide methods for evaluating generative-AI systems, with specific focuses on quantifying memorization in language models and training latent diffusion models on open-licensed data. These contributions have urgent and significant connections to U.S. copyright law. We provide an abridged discussion of landmark legal scholarship that details the complicated relationships between generative-AI supply chain and copyright. By making contributions in these three themes, this dissertation serves as an empirical proof by example that research on reliable measurement for machine learning is intimately and inescapably bound up with research in law and policy. These different disciplines pose similar research questions about reliable measurement in machine learning. They are, in fact, two complementary sides of the same research vision, which, broadly construed, aims to construct machine-learning systems that cohere with broader societal values.",,,SSRN Electronic Journal,,,,2024,2024,2024,,,,,All OA; Green,Preprint,"Cooper, A. Feder","Cooper, A. Feder (Microsoft Research; Stanford University; Yale University)",,"Cooper, A. Feder (Microsoft Research; Stanford University; Yale University)",0,0,,,,https://app.dimensions.ai/details/publication/pub.1175691291,46 Information and Computing Sciences; 4611 Machine Learning; 48 Law and Legal Studies,
295,pub.1163728144,10.1109/icce-taiwan58799.2023.10226807,,,ICCE-Taiwan 2023 Conference Proceedings,,,,2020 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan),2023 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan),,,2023-08-31,2023,2023-08-31,,,,1-891,All OA; Bronze,Proceeding,,,,,0,0,,,https://ieeexplore.ieee.org/ielx7/10226627/10226615/10226807.pdf,https://app.dimensions.ai/details/publication/pub.1163728144,,
184,pub.1146680350,10.48550/arxiv.2203.14101,,,A Roadmap for Big Model,"With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view.",,,arXiv,,,,2022-03-26,2022,,,,,,All OA; Green,Preprint,"Yuan, Sha; Zhao, Hanyu; Zhao, Shuai; Leng, Jiahong; Liang, Yangxiao; Wang, Xiaozhi; Yu, Jifan; Lv, Xin; Shao, Zhou; He, Jiaao; Lin, Yankai; Han, Xu; Liu, Zhenghao; Ding, Ning; Rao, Yongming; Gao, Yizhao; Zhang, Liang; Ding, Ming; Fang, Cong; Wang, Yisen; Long, Mingsheng; Zhang, Jing; Dong, Yinpeng; Pang, Tianyu; Cui, Peng; Huang, Lingxiao; Liang, Zheng; Shen, Huawei; Zhang, Hui; Zhang, Quanshi; Dong, Qingxiu; Tan, Zhixing; Wang, Mingxuan; Wang, Shuo; Zhou, Long; Li, Haoran; Bao, Junwei; Pan, Yingwei; Zhang, Weinan; Yu, Zhou; Yan, Rui; Shi, Chence; Xu, Minghao; Zhang, Zuobai; Wang, Guoqiang; Pan, Xiang; Li, Mengjie; Chu, Xiaoyu; Yao, Zijun; Zhu, Fangwei; Cao, Shulin; Xue, Weicheng; Ma, Zixuan; Zhang, Zhengyan; Hu, Shengding; Qin, Yujia; Xiao, Chaojun; Zeng, Zheni; Cui, Ganqu; Chen, Weize; Zhao, Weilin; Yao, Yuan; Li, Peng; Zheng, Wenzhao; Zhao, Wenliang; Wang, Ziyi; Zhang, Borui; Fei, Nanyi; Hu, Anwen; Ling, Zenan; Li, Haoyang; Cao, Boxi; Han, Xianpei; Zhan, Weidong; Chang, Baobao; Sun, Hao; Deng, Jiawen; Zheng, Chujie; Li, Juanzi; Hou, Lei; Cao, Xigang; Zhai, Jidong; Liu, Zhiyuan; Sun, Maosong; Lu, Jiwen; Lu, Zhiwu; Jin, Qin; Song, Ruihua; Wen, Ji-Rong; Lin, Zhouchen; Wang, Liwei; Su, Hang; Zhu, Jun; Sui, Zhifang; Zhang, Jiajun; Liu, Yang; He, Xiaodong; Huang, Minlie; Tang, Jian; Tang, Jie","Yuan, Sha (); Zhao, Hanyu (); Zhao, Shuai (); Leng, Jiahong (); Liang, Yangxiao (); Wang, Xiaozhi (); Yu, Jifan (); Lv, Xin (); Shao, Zhou (); He, Jiaao (); Lin, Yankai (); Han, Xu (); Liu, Zhenghao (); Ding, Ning (); Rao, Yongming (); Gao, Yizhao (); Zhang, Liang (); Ding, Ming (); Fang, Cong (); Wang, Yisen (); Long, Mingsheng (); Zhang, Jing (); Dong, Yinpeng (); Pang, Tianyu (); Cui, Peng (); Huang, Lingxiao (); Liang, Zheng (); Shen, Huawei (); Zhang, Hui (); Zhang, Quanshi (); Dong, Qingxiu (); Tan, Zhixing (); Wang, Mingxuan (); Wang, Shuo (); Zhou, Long (); Li, Haoran (); Bao, Junwei (); Pan, Yingwei (); Zhang, Weinan (); Yu, Zhou (); Yan, Rui (); Shi, Chence (); Xu, Minghao (); Zhang, Zuobai (); Wang, Guoqiang (); Pan, Xiang (); Li, Mengjie (); Chu, Xiaoyu (); Yao, Zijun (); Zhu, Fangwei (); Cao, Shulin (); Xue, Weicheng (); Ma, Zixuan (); Zhang, Zhengyan (); Hu, Shengding (); Qin, Yujia (); Xiao, Chaojun (); Zeng, Zheni (); Cui, Ganqu (); Chen, Weize (); Zhao, Weilin (); Yao, Yuan (); Li, Peng (); Zheng, Wenzhao (); Zhao, Wenliang (); Wang, Ziyi (); Zhang, Borui (); Fei, Nanyi (); Hu, Anwen (); Ling, Zenan (); Li, Haoyang (); Cao, Boxi (); Han, Xianpei (); Zhan, Weidong (); Chang, Baobao (); Sun, Hao (); Deng, Jiawen (); Zheng, Chujie (); Li, Juanzi (); Hou, Lei (); Cao, Xigang (); Zhai, Jidong (); Liu, Zhiyuan (); Sun, Maosong (); Lu, Jiwen (); Lu, Zhiwu (); Jin, Qin (); Song, Ruihua (); Wen, Ji-Rong (); Lin, Zhouchen (); Wang, Liwei (); Su, Hang (); Zhu, Jun (); Sui, Zhifang (); Zhang, Jiajun (); Liu, Yang (); He, Xiaodong (); Huang, Minlie (); Tang, Jian (); Tang, Jie ()",,"Yuan, Sha (); Zhao, Hanyu (); Zhao, Shuai (); Leng, Jiahong (); Liang, Yangxiao (); Wang, Xiaozhi (); Yu, Jifan (); Lv, Xin (); Shao, Zhou (); He, Jiaao (); Lin, Yankai (); Han, Xu (); Liu, Zhenghao (); Ding, Ning (); Rao, Yongming (); Gao, Yizhao (); Zhang, Liang (); Ding, Ming (); Fang, Cong (); Wang, Yisen (); Long, Mingsheng (); Zhang, Jing (); Dong, Yinpeng (); Pang, Tianyu (); Cui, Peng (); Huang, Lingxiao (); Liang, Zheng (); Shen, Huawei (); Zhang, Hui (); Zhang, Quanshi (); Dong, Qingxiu (); Tan, Zhixing (); Wang, Mingxuan (); Wang, Shuo (); Zhou, Long (); Li, Haoran (); Bao, Junwei (); Pan, Yingwei (); Zhang, Weinan (); Yu, Zhou (); Yan, Rui (); Shi, Chence (); Xu, Minghao (); Zhang, Zuobai (); Wang, Guoqiang (); Pan, Xiang (); Li, Mengjie (); Chu, Xiaoyu (); Yao, Zijun (); Zhu, Fangwei (); Cao, Shulin (); Xue, Weicheng (); Ma, Zixuan (); Zhang, Zhengyan (); Hu, Shengding (); Qin, Yujia (); Xiao, Chaojun (); Zeng, Zheni (); Cui, Ganqu (); Chen, Weize (); Zhao, Weilin (); Yao, Yuan (); Li, Peng (); Zheng, Wenzhao (); Zhao, Wenliang (); Wang, Ziyi (); Zhang, Borui (); Fei, Nanyi (); Hu, Anwen (); Ling, Zenan (); Li, Haoyang (); Cao, Boxi (); Han, Xianpei (); Zhan, Weidong (); Chang, Baobao (); Sun, Hao (); Deng, Jiawen (); Zheng, Chujie (); Li, Juanzi (); Hou, Lei (); Cao, Xigang (); Zhai, Jidong (); Liu, Zhiyuan (); Sun, Maosong (); Lu, Jiwen (); Lu, Zhiwu (); Jin, Qin (); Song, Ruihua (); Wen, Ji-Rong (); Lin, Zhouchen (); Wang, Liwei (); Su, Hang (); Zhu, Jun (); Sui, Zhifang (); Zhang, Jiajun (); Liu, Yang (); He, Xiaodong (); Huang, Minlie (); Tang, Jian (); Tang, Jie ()",0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1146680350,46 Information and Computing Sciences; 4602 Artificial Intelligence,
65,pub.1094423255,10.1109/pads.1995.404323,,,Proceedings 9th Workshop on Parallel and Distributed Simulation (ACM/IEEE),,,,,Proceedings 9th Workshop on Parallel and Distributed Simulation (ACM/IEEE),,,1995,1995,,,,,0_1,Closed,Proceeding,,,,,0,0,,,,https://app.dimensions.ai/details/publication/pub.1094423255,,
59,pub.1145850958,10.1109/ei252483.2021.9713535,,,The 5th IEEE Conference on Energy Internet and Energy System Integration,The 5th IEEE Conference on Energy Internet and Energy System Integration.,,,,2021 IEEE 5th Conference on Energy Internet and Energy System Integration (EI2),,,2022-02-25,2022,2022-02-25,,,,1-4530,Closed,Proceeding,,,,,0,0,,0.0,,https://app.dimensions.ai/details/publication/pub.1145850958,46 Information and Computing Sciences; 51 Physical Sciences; 5106 Nuclear and Plasma Physics,7 Affordable and Clean Energy
